#!/usr/bin/env python3
"""
æ•°æ®ç®¡ç†æ¨¡å—

æœ¬æ¨¡å—è´Ÿè´£ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆåŸå§‹/åŠ å·¥/è’¸é¦ï¼‰ï¼Œæä¾›æ•°æ®é¢„è§ˆã€å…³é”®è¯æœç´¢ã€å¤‡ä»½/æ¢å¤ã€åˆ é™¤ç­‰åŠŸèƒ½ã€‚
åŠŸèƒ½ç‰¹ç‚¹ï¼š
- æ•°æ®é›†ç»Ÿä¸€ç®¡ç†å’Œåˆ†ç±»
- æ•°æ®é¢„è§ˆå’Œå…³é”®è¯æœç´¢
- æ•°æ®å¤‡ä»½å’Œæ¢å¤
- æ•°æ®åˆ é™¤å’Œæ¸…ç†
- æ•°æ®ç»Ÿè®¡å’Œç›‘æ§

è®¾è®¡åŸåˆ™ï¼š
- ç»Ÿä¸€çš„æ•°æ®ç®¡ç†æ¥å£
- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
- å®‰å…¨çš„æ•°æ®æ“ä½œ
- å¯è¿½æº¯çš„æ•°æ®å†å²

ä½œè€…ï¼šè‡ªåŠ¨æ•°æ®è’¸é¦è½¯ä»¶å›¢é˜Ÿ
ç‰ˆæœ¬ï¼šv1.0
è®¸å¯ï¼šå•†ä¸šè½¯ä»¶
"""

import os
import json
import shutil
import fnmatch
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from .dependencies import pd

# å¯¼å…¥ç»Ÿä¸€å¼‚å¸¸ç±»
try:
    from .exceptions import DataManagerError, DataNotFoundError, DataAccessError
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å®šä¹‰ï¼ˆå‘åå…¼å®¹ï¼‰
    class DataManagerError(Exception):
        """æ•°æ®ç®¡ç†ç›¸å…³å¼‚å¸¸ç±»"""
        pass
    
    class DataNotFoundError(DataManagerError):
        """æ•°æ®ä¸å­˜åœ¨å¼‚å¸¸"""
        pass
    
    class DataAccessError(DataManagerError):
        """æ•°æ®è®¿é—®å¼‚å¸¸"""
        pass

# åŸºç¡€æ”¯æ’‘å±‚å¯¼å…¥
try:
    # ä½œä¸ºæ¨¡å—å¯¼å…¥æ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from .config_manager import config_manager
    from .log_manager import log_manager
    from .utils import FileOperations, DataProcessing
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from config_manager import config_manager
    from log_manager import log_manager
    from utils import FileOperations, DataProcessing


class DataType:
    """æ•°æ®ç±»å‹å¸¸é‡"""
    RAW = "raw"              # åŸå§‹æ•°æ®
    PROCESSED = "processed"  # åŠ å·¥æ•°æ®
    DISTILLED = "distilled"  # è’¸é¦æ•°æ®
    BACKUP = "backup"        # å¤‡ä»½æ•°æ®


class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨
    
    è´Ÿè´£ç®¡ç†æ‰€æœ‰æ•°æ®é›†çš„åˆ†ç±»ã€é¢„è§ˆã€æœç´¢ã€å¤‡ä»½ç­‰åŠŸèƒ½ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
        self.logger = log_manager.get_logger('data_manager')
        
        # è·å–é…ç½®
        self.root_dir = Path(config_manager.get_config('base.root_dir', './data'))
        self.preview_rows = config_manager.get_config('data_manager.preview_rows', 100)
        self.search_limit = config_manager.get_config('data_manager.search_limit', 1000)
        
        # æ•°æ®ç›®å½•æ˜ å°„
        self.data_dirs = {
            DataType.RAW: self.root_dir / 'raw',
            DataType.PROCESSED: self.root_dir / 'processed', 
            DataType.DISTILLED: self.root_dir / 'distilled',
            DataType.BACKUP: self.root_dir / 'backup'
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        self.supported_formats = ['.jsonl', '.json', '.csv', '.xlsx', '.xml', '.md']
        
        self.logger.info('æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ')
    
    def _ensure_directories(self) -> None:
        """ç¡®ä¿æ‰€æœ‰æ•°æ®ç›®å½•å­˜åœ¨"""
        try:
            for data_type, dir_path in self.data_dirs.items():
                dir_path.mkdir(parents=True, exist_ok=True)
            self.logger.debug('æ•°æ®ç›®å½•æ£€æŸ¥å®Œæˆ')
        except Exception as e:
            self.logger.error(f'åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {e}')
    
    def list_datasets(self, data_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿåˆ—å‡ºæ•°æ®é›†ï¼ˆé«˜åº¦ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        """
        try:
            datasets = []
            
            # ç¡®å®šè¦æ‰«æçš„ç›®å½•
            if data_type and data_type in self.data_dirs:
                scan_dirs = {data_type: self.data_dirs[data_type]}
            else:
                scan_dirs = {k: v for k, v in self.data_dirs.items() if k != DataType.BACKUP}
            
            # é’ˆå¯¹ä¸åŒç±»å‹ä½¿ç”¨ä¸åŒçš„æ‰«æç­–ç•¥
            for dtype, dir_path in scan_dirs.items():
                if not dir_path.exists():
                    continue
                
                # é™åˆ¶æœ€å¤§æ‰«ææ–‡ä»¶æ•°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
                max_files = 2000
                
                try:
                    if dtype == 'raw':
                        # åŸå§‹æ•°æ®é€šå¸¸åœ¨æ·±å±‚ç›®å½•ï¼Œä½¿ç”¨é€’å½’ä½†é™åˆ¶æ·±åº¦
                        self._scan_raw_directory(dir_path, datasets, dtype, max_files)
                    else:
                        # å¤„ç†æ•°æ®é€šå¸¸åœ¨æµ…å±‚ç›®å½•ï¼Œä½¿ç”¨ç®€å•æ‰«æ
                        self._scan_processed_directory(dir_path, datasets, dtype)
                        
                except Exception as e:
                    self.logger.warning(f'æ‰«æç›®å½•å¤±è´¥: {dir_path}, é”™è¯¯: {e}')
                    continue
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            datasets.sort(key=lambda x: x['modified_time'], reverse=True)
            
            self.logger.info(f'åˆ—å‡ºæ•°æ®é›†å®Œæˆ: ç±»å‹={data_type or "all"}, æ•°é‡={len(datasets)}')
            return datasets
            
        except Exception as e:
            self.logger.error(f'åˆ—å‡ºæ•°æ®é›†å¤±è´¥: {e}')
            return []
    
    def _scan_raw_directory(self, dir_path: Path, datasets: List, dtype: str, max_files: int = 2000):
        """æ‰«æåŸå§‹æ•°æ®ç›®å½•ï¼ˆä½¿ç”¨os.walkä¼˜åŒ–æ€§èƒ½ï¼‰"""
        import os
        
        scanned_count = 0
        
        for root, dirs, files in os.walk(str(dir_path)):
            # è¿‡æ»¤æ‰ä»¥.å¼€å¤´çš„ç›®å½•ï¼ˆå¦‚.git, ._____tempç­‰ï¼‰
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æ–‡ä»¶é™åˆ¶
                if len(datasets) >= max_files:
                    return

                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                if not file.lower().endswith(('.json', '.jsonl', '.csv', '.parquet')):
                    continue
                    
                # è·³è¿‡å…ƒæ•°æ®å’Œç³»ç»Ÿæ–‡ä»¶
                skip_files = [
                    'meta.json', 'dataset_infos.json', 'dataset_dict.json',
                    'dataset_info.json', 'state.json', 'config.json'
                ]
                if file in skip_files:
                    continue
                
                # è·³è¿‡ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶
                if 'cache' in root.lower() or 'downloads' in root.lower():
                    continue
                
                try:
                    file_path = Path(root) / file
                    stat_info = file_path.stat()
                    file_size_mb = stat_info.st_size / (1024 * 1024)
                    
                    # å°è¯•æ„å»ºæ›´å‹å¥½çš„æ˜¾ç¤ºåç§°
                    # å¦‚æœæ–‡ä»¶åœ¨å­ç›®å½•ä¸­ï¼Œä½¿ç”¨ "å­ç›®å½•/æ–‡ä»¶å" æ ¼å¼
                    # ç‰¹åˆ«é’ˆå¯¹ MegaScience/MegaScience/dataset/data/xxx.parquet è¿™ç§æƒ…å†µ
                    try:
                        rel_path = file_path.relative_to(dir_path)
                        # å¦‚æœè·¯å¾„æ·±åº¦å¤§äº1ï¼Œå°è¯•æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†
                        parts = rel_path.parts
                        if len(parts) > 1:
                            # æŸ¥æ‰¾æ˜¯å¦æœ‰ dataset ç›®å½•
                            if 'dataset' in parts:
                                idx = parts.index('dataset')
                                if idx > 0:
                                    # ä½¿ç”¨ dataset ä¹‹å‰çš„ç›®å½•åä½œä¸ºæ•°æ®é›†åç§°çš„ä¸€éƒ¨åˆ†
                                    # ä¾‹å¦‚ MegaScience/MegaScience/dataset -> MegaScience/MegaScience
                                    prefix = "/".join(parts[:idx])
                                    display_name = f"{prefix}/{file}"
                                else:
                                    display_name = str(rel_path).replace('\\', '/')
                            else:
                                # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä½œä¸ºåç§°
                                display_name = str(rel_path).replace('\\', '/')
                        else:
                            display_name = file
                    except Exception:
                        display_name = file

                    dataset_info = {
                        'name': display_name,
                        'path': str(file_path),
                        'relative_path': str(file_path.relative_to(self.root_dir)),
                        'type': dtype,
                        'format': file_path.suffix.lower()[1:],
                        'size': stat_info.st_size,
                        'size_mb': file_size_mb,
                        'size_human': self._format_size(stat_info.st_size),
                        'created_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                        'modified_time': datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                        'create_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                        'row_count': 0,
                        'has_meta': False
                    }
                    
                    datasets.append(dataset_info)
                    
                except Exception:
                    continue
    
    def _scan_processed_directory(self, dir_path: Path, datasets: List, dtype: str):
        """æ‰«æå¤„ç†æ•°æ®ç›®å½•ï¼ˆæµ…å±‚æ‰«æï¼‰"""
        for pattern in ['*.jsonl', '*.json', '*.csv']:
            # æ‰«æå½“å‰ç›®å½•
            for file_path in dir_path.glob(pattern):
                # è·³è¿‡å…ƒæ•°æ®å’Œç³»ç»Ÿæ–‡ä»¶
                if file_path.name in ['meta.json', 'checkpoint.json', 'quality_report.json', 'dataset_info.json']:
                    continue
                    
                try:
                    stat_info = file_path.stat()
                    file_size_mb = stat_info.st_size / (1024 * 1024)
                    
                    dataset_info = {
                        'name': file_path.name,
                        'path': str(file_path),
                        'relative_path': str(file_path.relative_to(self.root_dir)),
                        'type': dtype,
                        'format': file_path.suffix.lower()[1:],
                        'size': stat_info.st_size,
                        'size_mb': file_size_mb,
                        'size_human': self._format_size(stat_info.st_size),
                        'created_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                        'modified_time': datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                        'create_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                        'row_count': 0,
                        'has_meta': False
                    }
                    
                    datasets.append(dataset_info)
                    
                except Exception:
                    continue
            
            # æ‰«æä¸€çº§å­ç›®å½•
            for subdir in dir_path.iterdir():
                if subdir.is_dir():
                    for file_path in subdir.glob(pattern):
                        # è·³è¿‡å…ƒæ•°æ®å’Œç³»ç»Ÿæ–‡ä»¶
                        if file_path.name in ['meta.json', 'checkpoint.json', 'quality_report.json', 'dataset_info.json']:
                            continue
                            
                        try:
                            stat_info = file_path.stat()
                            file_size_mb = stat_info.st_size / (1024 * 1024)
                            
                            dataset_info = {
                                'name': file_path.name,
                                'path': str(file_path),
                                'relative_path': str(file_path.relative_to(self.root_dir)),
                                'type': dtype,
                                'format': file_path.suffix.lower()[1:],
                                'size': stat_info.st_size,
                                'size_mb': file_size_mb,
                                'size_human': self._format_size(stat_info.st_size),
                                'created_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                                'modified_time': datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                                'create_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                                'row_count': 0,
                                'has_meta': False
                            }
                            
                            datasets.append(dataset_info)
                            
                        except Exception:
                            continue

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0B"
        
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        
        return f"{size_bytes:.1f}{size_names[i]}"
    
    def _get_row_count(self, file_path: Path) -> int:
        """è·å–æ–‡ä»¶è¡Œæ•°"""
        return self._get_row_count_fast(file_path)
    
    def _get_row_count_fast(self, file_path: Path) -> int:
        """å¿«é€Ÿè·å–æ–‡ä»¶è¡Œæ•°ï¼ˆå¸¦è¶…æ—¶å’Œå¤§å°é™åˆ¶ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œè¶…è¿‡100MBçš„æ–‡ä»¶è·³è¿‡è¡Œæ•°è®¡ç®—
            file_size = file_path.stat().st_size
            if file_size > 100 * 1024 * 1024:  # 100MB
                return -1  # è¡¨ç¤ºæœªè®¡ç®—
            
            if file_path.suffix.lower() == '.jsonl':
                # å¯¹äºJSONLæ–‡ä»¶ï¼Œå¿«é€Ÿè®¡ç®—è¡Œæ•°
                with open(file_path, 'r', encoding='utf-8') as f:
                    count = 0
                    for _ in f:
                        count += 1
                        # è¶…è¿‡10ä¸‡è¡Œå°±åœæ­¢è®¡ç®—ï¼Œé¿å…è€—æ—¶è¿‡é•¿
                        if count > 100000:
                            return -1
                    return count
            elif file_path.suffix.lower() == '.csv':
                try:
                    # å¯¹äºCSVæ–‡ä»¶ï¼Œå°è¯•å¿«é€Ÿè¯»å–
                    df = pd.read_csv(file_path, nrows=0)  # åªè¯»å–å¤´éƒ¨è·å–åˆ—ä¿¡æ¯
                    # å¦‚æœæ–‡ä»¶ä¸å¤§ï¼Œè¯»å–å®Œæ•´æ–‡ä»¶
                    if file_size < 10 * 1024 * 1024:  # 10MBä»¥ä¸‹
                        df = pd.read_csv(file_path)
                        return len(df)
                    else:
                        return -1  # å¤§æ–‡ä»¶è·³è¿‡
                except:
                    return 0
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                try:
                    if file_size < 5 * 1024 * 1024:  # 5MBä»¥ä¸‹çš„Excelæ–‡ä»¶
                        df = pd.read_excel(file_path)
                        return len(df)
                    else:
                        return -1  # å¤§æ–‡ä»¶è·³è¿‡
                except:
                    return 0
            else:
                # å…¶ä»–æ ¼å¼æš‚æ—¶è¿”å›0
                return 0
        except Exception:
            return 0
    
    def _has_meta_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„meta.jsonæ–‡ä»¶"""
        meta_path = file_path.parent / 'meta.json'
        return meta_path.exists()
    
    def preview_data(self, file_path: str, rows: int = None) -> Dict[str, Any]:
        """
        é¢„è§ˆæ•°æ®ï¼ˆå‰Nè¡Œï¼‰
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            rows (int, optional): é¢„è§ˆè¡Œæ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            dict: é¢„è§ˆç»“æœï¼ŒåŒ…å«æ•°æ®å’Œå…ƒä¿¡æ¯
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
            
            rows = rows or self.preview_rows
            format_type = file_path.suffix.lower()[1:]
            
            preview_data = []
            total_rows = 0
            columns = []
            
            if format_type == 'jsonl':
                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= rows:
                            break
                        try:
                            data = json.loads(line.strip())
                            preview_data.append(data)
                            
                            # æ”¶é›†æ‰€æœ‰é”®ä½œä¸ºåˆ—å
                            if isinstance(data, dict):
                                columns.extend(data.keys())
                        except json.JSONDecodeError:
                            continue
                
                # è·å–æ€»è¡Œæ•°
                total_rows = self._get_row_count(file_path)
                columns = list(set(columns))  # å»é‡
                
            elif format_type == 'csv':
                df = pd.read_csv(file_path, nrows=rows)
                preview_data = df.to_dict('records')
                columns = df.columns.tolist()
                
                # è·å–æ€»è¡Œæ•°
                total_rows = self._get_row_count(file_path)
                
            elif format_type in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, nrows=rows)
                preview_data = df.to_dict('records')
                columns = df.columns.tolist()
                total_rows = self._get_row_count(file_path)
                
            elif format_type == 'json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, list):
                    preview_data = data[:rows]
                    total_rows = len(data)
                    if data and isinstance(data[0], dict):
                        columns = list(data[0].keys())
                else:
                    preview_data = [data]
                    total_rows = 1
                    columns = list(data.keys()) if isinstance(data, dict) else []
            
            result = {
                'success': True,
                'file_path': str(file_path),
                'format': format_type,
                'total_rows': total_rows,
                'preview_rows': len(preview_data),
                'columns': columns,
                'data': preview_data,
                'preview_limit': rows
            }
            
            self.logger.info(f'é¢„è§ˆæ•°æ®æˆåŠŸ: {file_path.name}, é¢„è§ˆè¡Œæ•°: {len(preview_data)}')
            return result
            
        except Exception as e:
            self.logger.error(f'é¢„è§ˆæ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {e}')
            return {
                'success': False,
                'error': str(e),
                'file_path': str(file_path),
                'data': []
            }
    
    def search_data(self, keyword: str, fields: Optional[List[str]] = None, 
                   data_type: Optional[str] = None) -> Dict[str, Any]:
        """
        å…³é”®è¯æœç´¢æ•°æ®
        
        Args:
            keyword (str): æœç´¢å…³é”®è¯
            fields (list, optional): æŒ‡å®šæœç´¢çš„å­—æ®µ
            data_type (str, optional): æŒ‡å®šæ•°æ®ç±»å‹
            
        Returns:
            dict: æœç´¢ç»“æœ
        """
        try:
            search_results = []
            total_matches = 0
            
            # è·å–è¦æœç´¢çš„æ•°æ®é›†
            datasets = self.list_datasets(data_type)
            
            for dataset in datasets:
                try:
                    file_path = Path(dataset['path'])
                    format_type = dataset['format']
                    
                    # æ ¹æ®æ ¼å¼è¯»å–æ•°æ®å¹¶æœç´¢
                    matches = self._search_in_file(file_path, keyword, fields, format_type)
                    
                    if matches:
                        search_results.append({
                            'dataset': dataset,
                            'matches': matches[:50],  # é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„åŒ¹é…ç»“æœ
                            'match_count': len(matches)
                        })
                        total_matches += len(matches)
                    
                    # é™åˆ¶æ€»æœç´¢ç»“æœ
                    if total_matches >= self.search_limit:
                        break
                        
                except Exception as e:
                    self.logger.warning(f'æœç´¢æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}')
            
            result = {
                'success': True,
                'keyword': keyword,
                'fields': fields,
                'data_type': data_type,
                'total_matches': total_matches,
                'datasets_count': len(search_results),
                'results': search_results,
                'search_limit': self.search_limit
            }
            
            self.logger.info(f'æœç´¢å®Œæˆ: å…³é”®è¯="{keyword}", åŒ¹é…={total_matches}')
            return result
            
        except Exception as e:
            self.logger.error(f'æœç´¢æ•°æ®å¤±è´¥: {e}')
            return {
                'success': False,
                'error': str(e),
                'keyword': keyword,
                'results': []
            }
    
    def _search_in_file(self, file_path: Path, keyword: str, 
                       fields: Optional[List[str]], format_type: str) -> List[Dict[str, Any]]:
        """åœ¨æ–‡ä»¶ä¸­æœç´¢å…³é”®è¯"""
        matches = []
        keyword_lower = keyword.lower()
        
        try:
            if format_type == 'jsonl':
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        try:
                            data = json.loads(line.strip())
                            if self._match_in_record(data, keyword_lower, fields):
                                matches.append({
                                    'line_number': line_num,
                                    'data': data
                                })
                        except json.JSONDecodeError:
                            continue
                            
            elif format_type == 'csv':
                df = pd.read_csv(file_path)
                for idx, row in df.iterrows():
                    if self._match_in_record(row.to_dict(), keyword_lower, fields):
                        matches.append({
                            'row_number': idx + 1,
                            'data': row.to_dict()
                        })
                        
            elif format_type in ['xlsx', 'xls']:
                df = pd.read_excel(file_path)
                for idx, row in df.iterrows():
                    if self._match_in_record(row.to_dict(), keyword_lower, fields):
                        matches.append({
                            'row_number': idx + 1,
                            'data': row.to_dict()
                        })
                        
        except Exception as e:
            self.logger.warning(f'æ–‡ä»¶å†…æœç´¢å¤±è´¥: {file_path}, é”™è¯¯: {e}')
        
        return matches
    
    def _match_in_record(self, record: Dict[str, Any], keyword: str, 
                        fields: Optional[List[str]]) -> bool:
        """æ£€æŸ¥è®°å½•æ˜¯å¦åŒ¹é…å…³é”®è¯"""
        if not isinstance(record, dict):
            return False
        
        # ç¡®å®šè¦æœç´¢çš„å­—æ®µ
        search_fields = fields if fields else record.keys()
        
        for field in search_fields:
            if field in record:
                value = str(record[field]).lower()
                if keyword in value:
                    return True
        
        return False
    
    def backup_data(self, file_paths: List[str], backup_date: Optional[str] = None) -> str:
        """
        å¤‡ä»½æŒ‡å®šæ•°æ®é›†
        
        Args:
            file_paths (list): è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            backup_date (str, optional): å¤‡ä»½æ—¥æœŸï¼Œé»˜è®¤å½“å‰æ—¥æœŸ
            
        Returns:
            str: å¤‡ä»½è·¯å¾„
        """
        try:
            backup_date = backup_date or datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_dir = self.data_dirs[DataType.BACKUP] / backup_date
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            backup_manifest = {
                'backup_date': backup_date,
                'created_time': datetime.now().isoformat(),
                'files': []
            }
            
            for file_path_str in file_paths:
                try:
                    file_path = Path(file_path_str)
                    if not file_path.exists():
                        self.logger.warning(f'å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
                        continue
                    
                    # ç¡®å®šå¤‡ä»½è·¯å¾„ï¼Œä¿æŒç›¸å¯¹ç›®å½•ç»“æ„
                    if file_path.is_relative_to(self.root_dir):
                        rel_path = file_path.relative_to(self.root_dir)
                    else:
                        rel_path = file_path.name
                    
                    backup_file_path = backup_dir / rel_path
                    backup_file_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(file_path, backup_file_path)
                    
                    # å¤åˆ¶metaæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    meta_path = file_path.parent / 'meta.json'
                    if meta_path.exists():
                        backup_meta_path = backup_file_path.parent / 'meta.json'
                        shutil.copy2(meta_path, backup_meta_path)
                    
                    backup_manifest['files'].append({
                        'original_path': str(file_path),
                        'backup_path': str(backup_file_path),
                        'size': file_path.stat().st_size,
                        'backup_time': datetime.now().isoformat()
                    })
                    
                except Exception as e:
                    self.logger.error(f'å¤‡ä»½å•ä¸ªæ–‡ä»¶å¤±è´¥: {file_path_str}, é”™è¯¯: {e}')
            
            # ä¿å­˜å¤‡ä»½æ¸…å•
            manifest_path = backup_dir / 'backup_manifest.json'
            with open(manifest_path, 'w', encoding='utf-8') as f:
                json.dump(backup_manifest, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f'å¤‡ä»½å®Œæˆ: {len(backup_manifest["files"])}ä¸ªæ–‡ä»¶, è·¯å¾„: {backup_dir}')
            return str(backup_dir)
            
        except Exception as e:
            self.logger.error(f'å¤‡ä»½æ•°æ®å¤±è´¥: {e}')
            raise
    
    def restore_data(self, backup_path: str, target_dir: Optional[str] = None) -> bool:
        """
        ä»å¤‡ä»½æ¢å¤æ•°æ®
        
        Args:
            backup_path (str): å¤‡ä»½è·¯å¾„
            target_dir (str, optional): ç›®æ ‡ç›®å½•ï¼Œé»˜è®¤ä¸ºåŸå§‹ä½ç½®
            
        Returns:
            bool: æ¢å¤æˆåŠŸè¿”å›True
        """
        try:
            backup_path = Path(backup_path)
            if not backup_path.exists():
                raise FileNotFoundError(f'å¤‡ä»½è·¯å¾„ä¸å­˜åœ¨: {backup_path}')
            
            # è¯»å–å¤‡ä»½æ¸…å•
            manifest_path = backup_path / 'backup_manifest.json'
            if not manifest_path.exists():
                raise FileNotFoundError(f'å¤‡ä»½æ¸…å•ä¸å­˜åœ¨: {manifest_path}')
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest = json.load(f)
            
            restored_count = 0
            target_base = Path(target_dir) if target_dir else self.root_dir
            
            for file_info in manifest['files']:
                try:
                    backup_file_path = Path(file_info['backup_path'])
                    if not backup_file_path.exists():
                        self.logger.warning(f'å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file_path}')
                        continue
                    
                    # ç¡®å®šæ¢å¤è·¯å¾„
                    if target_dir:
                        # æ¢å¤åˆ°æŒ‡å®šç›®å½•
                        relative_path = backup_file_path.relative_to(backup_path)
                        restore_path = target_base / relative_path
                    else:
                        # æ¢å¤åˆ°åŸå§‹è·¯å¾„
                        restore_path = Path(file_info['original_path'])
                    
                    # åˆ›å»ºç›®å½•
                    restore_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(backup_file_path, restore_path)
                    restored_count += 1
                    
                except Exception as e:
                    self.logger.error(f'æ¢å¤å•ä¸ªæ–‡ä»¶å¤±è´¥: {file_info}, é”™è¯¯: {e}')
            
            self.logger.info(f'æ¢å¤å®Œæˆ: {restored_count}ä¸ªæ–‡ä»¶')
            return restored_count > 0
            
        except Exception as e:
            self.logger.error(f'æ¢å¤æ•°æ®å¤±è´¥: {e}')
            return False
    
    def delete_data(self, file_path: str) -> bool:
        """
        åˆ é™¤æ•°æ®é›†ï¼ˆå«å…ƒæ•°æ®ï¼‰
        æ”¯æŒåˆ é™¤å•ä¸ªæ–‡ä»¶æˆ–æ•´ä¸ªæ•°æ®é›†ç›®å½•
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
            
        Returns:
            bool: åˆ é™¤æˆåŠŸè¿”å›True
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                self.logger.warning(f'è¦åˆ é™¤çš„è·¯å¾„ä¸å­˜åœ¨: {file_path}')
                return False
            
            if file_path.is_file():
                # åˆ é™¤å•ä¸ªæ–‡ä»¶
                self.logger.info(f'åˆ é™¤æ–‡ä»¶: {file_path}')
                file_path.unlink()
                
                # åˆ é™¤metaæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                meta_path = file_path.parent / 'meta.json'
                if meta_path.exists():
                    meta_path.unlink()
                    self.logger.info(f'åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶: {meta_path}')
                
                # å¦‚æœç›®å½•ä¸ºç©ºï¼Œåˆ é™¤ç›®å½•
                try:
                    if not any(file_path.parent.iterdir()):
                        file_path.parent.rmdir()
                        self.logger.info(f'åˆ é™¤ç©ºç›®å½•: {file_path.parent}')
                except OSError:
                    pass  # ç›®å½•ä¸ä¸ºç©ºæˆ–å…¶ä»–åŸå› 
                    
            elif file_path.is_dir():
                # åˆ é™¤æ•´ä¸ªæ•°æ®é›†ç›®å½•
                self.logger.info(f'åˆ é™¤æ•°æ®é›†ç›®å½•: {file_path}')
                import shutil
                shutil.rmtree(file_path)
                
            else:
                self.logger.warning(f'æœªçŸ¥çš„è·¯å¾„ç±»å‹: {file_path}')
                return False
            
            self.logger.info(f'åˆ é™¤æˆåŠŸ: {file_path}')
            return True
            
        except Exception as e:
            self.logger.error(f'åˆ é™¤å¤±è´¥: {file_path}, é”™è¯¯: {e}')
            return False
    
    def get_storage_statistics(self) -> Dict[str, Any]:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            stats = {
                'total_size': 0,
                'total_files': 0,
                'by_type': {},
                'by_format': {},
                'largest_files': [],
                'recent_files': []
            }
            
            # è·å–æ‰€æœ‰æ•°æ®é›†
            all_datasets = self.list_datasets()
            
            # ç»Ÿè®¡ä¿¡æ¯
            format_stats = {}
            type_stats = {}
            
            for dataset in all_datasets:
                # æ€»ä½“ç»Ÿè®¡
                stats['total_size'] += dataset['size']
                stats['total_files'] += 1
                
                # æŒ‰ç±»å‹ç»Ÿè®¡
                data_type = dataset['type']
                if data_type not in type_stats:
                    type_stats[data_type] = {'size': 0, 'count': 0}
                type_stats[data_type]['size'] += dataset['size']
                type_stats[data_type]['count'] += 1
                
                # æŒ‰æ ¼å¼ç»Ÿè®¡
                file_format = dataset['format']
                if file_format not in format_stats:
                    format_stats[file_format] = {'size': 0, 'count': 0}
                format_stats[file_format]['size'] += dataset['size']
                format_stats[file_format]['count'] += 1
            
            # æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯
            for data_type, type_stat in type_stats.items():
                stats['by_type'][data_type] = {
                    'size': type_stat['size'],
                    'size_human': self._format_size(type_stat['size']),
                    'count': type_stat['count'],
                    'percentage': (type_stat['size'] / stats['total_size'] * 100) if stats['total_size'] > 0 else 0
                }
            
            for file_format, format_stat in format_stats.items():
                stats['by_format'][file_format] = {
                    'size': format_stat['size'],
                    'size_human': self._format_size(format_stat['size']),
                    'count': format_stat['count'],
                    'percentage': (format_stat['size'] / stats['total_size'] * 100) if stats['total_size'] > 0 else 0
                }
            
            # æœ€å¤§æ–‡ä»¶ï¼ˆå‰10ä¸ªï¼‰
            stats['largest_files'] = sorted(all_datasets, key=lambda x: x['size'], reverse=True)[:10]
            
            # æœ€è¿‘æ–‡ä»¶ï¼ˆå‰10ä¸ªï¼‰
            stats['recent_files'] = sorted(all_datasets, key=lambda x: x['modified_time'], reverse=True)[:10]
            
            # æ ¼å¼åŒ–æ€»å¤§å°
            stats['total_size_human'] = self._format_size(stats['total_size'])
            
            return stats
            
        except Exception as e:
            self.logger.error(f'è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}')
            return {}
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
        
        Returns:
            list: å¤‡ä»½åˆ—è¡¨
        """
        try:
            backups = []
            backup_dir = self.data_dirs[DataType.BACKUP]
            
            if not backup_dir.exists():
                return backups
            
            for backup_subdir in backup_dir.iterdir():
                if backup_subdir.is_dir():
                    manifest_path = backup_subdir / 'backup_manifest.json'
                    if manifest_path.exists():
                        try:
                            with open(manifest_path, 'r', encoding='utf-8') as f:
                                manifest = json.load(f)
                            
                            # è®¡ç®—å¤‡ä»½å¤§å°
                            total_size = sum(file_info['size'] for file_info in manifest.get('files', []))
                            
                            backup_info = {
                                'backup_id': backup_subdir.name,
                                'path': str(backup_subdir),
                                'created_time': manifest.get('created_time', ''),
                                'file_count': len(manifest.get('files', [])),
                                'total_size': total_size,
                                'total_size_human': self._format_size(total_size),
                                'manifest': manifest
                            }
                            
                            backups.append(backup_info)
                            
                        except Exception as e:
                            self.logger.warning(f'è¯»å–å¤‡ä»½æ¸…å•å¤±è´¥: {manifest_path}, é”™è¯¯: {e}')
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
            backups.sort(key=lambda x: x['created_time'], reverse=True)
            
            return backups
            
        except Exception as e:
            self.logger.error(f'åˆ—å‡ºå¤‡ä»½å¤±è´¥: {e}')
            return []


# å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹ - å»¶è¿Ÿåˆå§‹åŒ–
_data_manager_instance = None

def get_data_manager():
    """è·å–æ•°æ®ç®¡ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _data_manager_instance
    if _data_manager_instance is None:
        _data_manager_instance = DataManager()
    return _data_manager_instance

# ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™data_managerå˜é‡
class DataManagerProxy:
    """æ•°æ®ç®¡ç†å™¨ä»£ç†ï¼Œç”¨äºå»¶è¿Ÿåˆå§‹åŒ–"""
    def __getattr__(self, name):
        return getattr(get_data_manager(), name)

data_manager = DataManagerProxy()


if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œå…¥å£ï¼Œç”¨äºæ•°æ®ç®¡ç†æ“ä½œ
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®ç®¡ç†å·¥å…·')
    subparsers = parser.add_subparsers(dest='action', help='å¯ç”¨æ“ä½œ')
    
    # listå‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ•°æ®é›†')
    list_parser.add_argument('--type', choices=['raw', 'processed', 'distilled'], help='æ•°æ®ç±»å‹')
    
    # previewå‘½ä»¤
    preview_parser = subparsers.add_parser('preview', help='é¢„è§ˆæ•°æ®')
    preview_parser.add_argument('--file', required=True, help='æ–‡ä»¶è·¯å¾„')
    preview_parser.add_argument('--rows', type=int, default=10, help='é¢„è§ˆè¡Œæ•°')
    
    # searchå‘½ä»¤
    search_parser = subparsers.add_parser('search', help='æœç´¢æ•°æ®')
    search_parser.add_argument('--keyword', required=True, help='æœç´¢å…³é”®è¯')
    search_parser.add_argument('--fields', nargs='+', help='æœç´¢å­—æ®µ')
    search_parser.add_argument('--type', choices=['raw', 'processed', 'distilled'], help='æ•°æ®ç±»å‹')
    
    # backupå‘½ä»¤
    backup_parser = subparsers.add_parser('backup', help='å¤‡ä»½æ•°æ®')
    backup_parser.add_argument('--files', nargs='+', required=True, help='è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„')
    backup_parser.add_argument('--date', help='å¤‡ä»½æ—¥æœŸæ ‡è¯†')
    
    # restoreå‘½ä»¤
    restore_parser = subparsers.add_parser('restore', help='æ¢å¤æ•°æ®')
    restore_parser.add_argument('--backup', required=True, help='å¤‡ä»½è·¯å¾„')
    restore_parser.add_argument('--target', help='ç›®æ ‡ç›®å½•')
    
    # deleteå‘½ä»¤
    delete_parser = subparsers.add_parser('delete', help='åˆ é™¤æ•°æ®')
    delete_parser.add_argument('--file', required=True, help='æ–‡ä»¶è·¯å¾„')
    
    # statså‘½ä»¤
    stats_parser = subparsers.add_parser('stats', help='å­˜å‚¨ç»Ÿè®¡')
    
    # list-backupså‘½ä»¤
    backups_parser = subparsers.add_parser('list-backups', help='åˆ—å‡ºå¤‡ä»½')
    
    args = parser.parse_args()
    
    if args.action == 'list':
        datasets = data_manager.list_datasets(args.type)
        print(f"æ•°æ®é›†åˆ—è¡¨ (ç±»å‹: {args.type or 'all'}):")
        for dataset in datasets:
            print(f"  ğŸ“„ {dataset['name']}")
            print(f"     è·¯å¾„: {dataset['relative_path']}")
            print(f"     ç±»å‹: {dataset['type']} | æ ¼å¼: {dataset['format']}")
            print(f"     å¤§å°: {dataset['size_human']} | è¡Œæ•°: {dataset['row_count']}")
            print(f"     ä¿®æ”¹: {dataset['modified_time']}")
            print()
    
    elif args.action == 'preview':
        result = data_manager.preview_data(args.file, args.rows)
        if result['success']:
            print(f"æ–‡ä»¶é¢„è§ˆ: {result['file_path']}")
            print(f"æ ¼å¼: {result['format']} | æ€»è¡Œæ•°: {result['total_rows']}")
            print(f"åˆ—: {', '.join(result['columns'])}")
            print(f"\nå‰ {result['preview_rows']} è¡Œæ•°æ®:")
            for i, row in enumerate(result['data'], 1):
                print(f"  {i}: {row}")
        else:
            print(f"é¢„è§ˆå¤±è´¥: {result['error']}")
    
    elif args.action == 'search':
        result = data_manager.search_data(args.keyword, args.fields, args.type)
        if result['success']:
            print(f"æœç´¢ç»“æœ: '{result['keyword']}'")
            print(f"æ€»åŒ¹é…: {result['total_matches']} | æ•°æ®é›†: {result['datasets_count']}")
            for dataset_result in result['results'][:5]:  # æ˜¾ç¤ºå‰5ä¸ªæ•°æ®é›†çš„ç»“æœ
                dataset = dataset_result['dataset']
                print(f"\nğŸ“„ {dataset['name']} ({dataset_result['match_count']} åŒ¹é…)")
                for match in dataset_result['matches'][:3]:  # æ¯ä¸ªæ•°æ®é›†æ˜¾ç¤ºå‰3ä¸ªåŒ¹é…
                    print(f"   {match}")
        else:
            print(f"æœç´¢å¤±è´¥: {result['error']}")
    
    elif args.action == 'backup':
        try:
            backup_path = data_manager.backup_data(args.files, args.date)
            print(f"âœ“ å¤‡ä»½æˆåŠŸ: {backup_path}")
        except Exception as e:
            print(f"âœ— å¤‡ä»½å¤±è´¥: {e}")
    
    elif args.action == 'restore':
        success = data_manager.restore_data(args.backup, args.target)
        if success:
            print(f"âœ“ æ¢å¤æˆåŠŸ")
        else:
            print(f"âœ— æ¢å¤å¤±è´¥")
    
    elif args.action == 'delete':
        success = data_manager.delete_data(args.file)
        if success:
            print(f"âœ“ åˆ é™¤æˆåŠŸ: {args.file}")
        else:
            print(f"âœ— åˆ é™¤å¤±è´¥: {args.file}")
    
    elif args.action == 'stats':
        stats = data_manager.get_storage_statistics()
        print("å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»å¤§å°: {stats['total_size_human']}")
        print(f"  æ€»æ–‡ä»¶: {stats['total_files']}")
        
        print("\næŒ‰ç±»å‹ç»Ÿè®¡:")
        for data_type, type_stats in stats['by_type'].items():
            print(f"  {data_type}: {type_stats['size_human']} ({type_stats['count']} æ–‡ä»¶, {type_stats['percentage']:.1f}%)")
        
        print("\næŒ‰æ ¼å¼ç»Ÿè®¡:")
        for file_format, format_stats in stats['by_format'].items():
            print(f"  {file_format}: {format_stats['size_human']} ({format_stats['count']} æ–‡ä»¶)")
        
        print(f"\næœ€å¤§æ–‡ä»¶ (å‰5ä¸ª):")
        for dataset in stats['largest_files'][:5]:
            print(f"  {dataset['name']}: {dataset['size_human']}")
    
    elif args.action == 'list-backups':
        backups = data_manager.list_backups()
        print("å¤‡ä»½åˆ—è¡¨:")
        for backup in backups:
            print(f"  ğŸ“¦ {backup['backup_id']}")
            print(f"     åˆ›å»ºæ—¶é—´: {backup['created_time']}")
            print(f"     æ–‡ä»¶æ•°é‡: {backup['file_count']}")
            print(f"     æ€»å¤§å°: {backup['total_size_human']}")
            print()
    
    else:
        parser.print_help()
