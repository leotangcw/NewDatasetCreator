#!/usr/bin/env python3
"""
æ•°æ®é›†é¢„è§ˆå™¨ - æ”¯æŒå¤šç§æ ¼å¼çš„æ•°æ®é›†é¢„è§ˆåŠŸèƒ½

æ”¯æŒåŠŸèƒ½ï¼š
1. å¤šç§æ ¼å¼ï¼šJSONã€JSONLã€CSVã€TSVç­‰
2. å¤§æ–‡ä»¶å¤„ç†ï¼šæµå¼è¯»å–ã€åˆ†é¡µé¢„è§ˆ
3. é•¿æ–‡æœ¬å¤„ç†ï¼šæ™ºèƒ½æˆªæ–­ã€å±•å¼€åŠŸèƒ½
4. å¤šæ–‡ä»¶æ•°æ®é›†ï¼šç›®å½•éå†ã€æ–‡ä»¶ç»„åˆ
5. æ•°æ®è´¨é‡æ£€æŸ¥ï¼šæ ¼å¼éªŒè¯ã€ç»Ÿè®¡ä¿¡æ¯
"""

import json
import csv
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
from dataclasses import dataclass
from enum import Enum
import re


class DatasetFormat(Enum):
    """æ•°æ®é›†æ ¼å¼æšä¸¾"""
    JSON = "json"
    JSONL = "jsonl"
    CSV = "csv"
    TSV = "tsv"
    TXT = "txt"
    ARROW = "arrow"
    PARQUET = "parquet"
    UNKNOWN = "unknown"


@dataclass
class PreviewConfig:
    """é¢„è§ˆé…ç½®"""
    max_rows: int = 100  # æœ€å¤§é¢„è§ˆè¡Œæ•°
    max_files: int = 10  # æœ€å¤§é¢„è§ˆæ–‡ä»¶æ•°
    max_text_length: int = 200  # æ–‡æœ¬æˆªæ–­é•¿åº¦
    max_file_size_mb: int = 100  # æœ€å¤§å•æ–‡ä»¶å¤§å°(MB)
    enable_truncation: bool = True  # å¯ç”¨æ–‡æœ¬æˆªæ–­
    show_stats: bool = True  # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    include_metadata: bool = True  # åŒ…å«å…ƒæ•°æ®
    smart_columns: bool = True  # æ™ºèƒ½åˆ—æ˜¾ç¤º
    show_all_columns: bool = False  # æ˜¾ç¤ºæ‰€æœ‰åˆ—
    min_column_width: int = 20  # æœ€å°åˆ—å®½
    max_column_width: int = 100  # æœ€å¤§åˆ—å®½


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""
    path: str
    name: str
    size: int
    format: DatasetFormat
    encoding: str = "utf-8"
    row_count: Optional[int] = None
    columns: Optional[List[str]] = None


@dataclass
class PreviewResult:
    """é¢„è§ˆç»“æœ"""
    success: bool
    data: List[Dict[str, Any]]
    files: List[FileInfo]
    total_rows: int
    total_files: int
    format: DatasetFormat
    metadata: Dict[str, Any]
    error_message: str = ""
    truncated_fields: List[str] = None  # è¢«æˆªæ–­çš„å­—æ®µ
    hidden_columns: List[str] = None  # è¢«éšè—çš„åˆ—
    column_info: Dict[str, Any] = None  # åˆ—ä¿¡æ¯ç»Ÿè®¡


class DatasetPreviewer:
    """æ•°æ®é›†é¢„è§ˆå™¨ä¸»ç±»"""
    
    def __init__(self, config: Optional[PreviewConfig] = None):
        self.config = config or PreviewConfig()
        self.logger = logging.getLogger(__name__)
        
    def preview_dataset(self, path: Union[str, Path], 
                       max_rows: Optional[int] = None) -> PreviewResult:
        """
        é¢„è§ˆæ•°æ®é›†
        
        Args:
            path: æ•°æ®é›†è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
            max_rows: æœ€å¤§é¢„è§ˆè¡Œæ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰
            
        Returns:
            PreviewResult: é¢„è§ˆç»“æœ
        """
        try:
            path = Path(path)
            if not path.exists():
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"è·¯å¾„ä¸å­˜åœ¨: {path}"
                )
            
            # ä½¿ç”¨ä¼ å…¥çš„max_rowsæˆ–é…ç½®ä¸­çš„å€¼
            effective_max_rows = max_rows or self.config.max_rows
            
            if path.is_file():
                return self._preview_single_file(path, effective_max_rows)
            elif path.is_dir():
                return self._preview_directory(path, effective_max_rows)
            else:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"æœªçŸ¥è·¯å¾„ç±»å‹: {path}"
                )
                
        except Exception as e:
            self.logger.error(f"é¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}")
            return PreviewResult(
                success=False,
                data=[],
                files=[],
                total_rows=0,
                total_files=0,
                format=DatasetFormat.UNKNOWN,
                metadata={},
                error_message=str(e)
            )
    
    def _preview_single_file(self, file_path: Path, max_rows: int) -> PreviewResult:
        """é¢„è§ˆå•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            
            # å¯¹äºArrowå’ŒParquetæ–‡ä»¶ï¼Œå¢å¤§å¤§å°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸è¾ƒå¤§ä½†è¯»å–æ•ˆç‡é«˜
            if file_path.suffix.lower() in {'.arrow', '.parquet'}:
                max_size_limit = self.config.max_file_size_mb * 10  # å¢å¤§10å€é™åˆ¶
            else:
                max_size_limit = self.config.max_file_size_mb
            
            if file_size > max_size_limit * 1024 * 1024:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"æ–‡ä»¶è¿‡å¤§: {file_size / (1024*1024):.1f}MBï¼Œè¶…è¿‡é™åˆ¶ {max_size_limit}MB"
                )
            
            # æ£€æµ‹æ–‡ä»¶æ ¼å¼
            file_format = self._detect_format(file_path)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = FileInfo(
                path=str(file_path),
                name=file_path.name,
                size=file_size,
                format=file_format
            )
            
            # æ ¹æ®æ ¼å¼è¯»å–æ•°æ®
            if file_format == DatasetFormat.JSON:
                data, total_rows = self._read_json_file(file_path, max_rows)
            elif file_format == DatasetFormat.JSONL:
                data, total_rows = self._read_jsonl_file(file_path, max_rows)
            elif file_format in [DatasetFormat.CSV, DatasetFormat.TSV]:
                data, total_rows = self._read_csv_file(file_path, max_rows, file_format)
            elif file_format == DatasetFormat.TXT:
                data, total_rows = self._read_text_file(file_path, max_rows)
            elif file_format == DatasetFormat.ARROW:
                data, total_rows = self._read_arrow_file(file_path, max_rows)
            elif file_format == DatasetFormat.PARQUET:
                data, total_rows = self._read_parquet_file(file_path, max_rows)
            else:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[file_info],
                    total_rows=0,
                    total_files=1,
                    format=file_format,
                    metadata={},
                    error_message=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format.value}"
                )
            
            # å¤„ç†é•¿æ–‡æœ¬æˆªæ–­
            truncated_fields = []
            if self.config.enable_truncation:
                data, truncated_fields = self._apply_text_truncation(data)
            
            # å¤„ç†æ™ºèƒ½åˆ—æ˜¾ç¤º
            hidden_columns = []
            column_info = {}
            if self.config.smart_columns and not self.config.show_all_columns:
                data, hidden_columns, column_info = self._apply_smart_columns(data)
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info.row_count = total_rows
            if data:
                file_info.columns = list(data[0].keys()) if isinstance(data[0], dict) else None
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = self._generate_metadata(file_info, data) if self.config.include_metadata else {}
            
            return PreviewResult(
                success=True,
                data=data,
                files=[file_info],
                total_rows=total_rows,
                total_files=1,
                format=file_format,
                metadata=metadata,
                truncated_fields=truncated_fields,
                hidden_columns=hidden_columns,
                column_info=column_info
            )
            
        except Exception as e:
            self.logger.error(f"é¢„è§ˆæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return PreviewResult(
                success=False,
                data=[],
                files=[],
                total_rows=0,
                total_files=0,
                format=DatasetFormat.UNKNOWN,
                metadata={},
                error_message=str(e)
            )
    
    def _preview_directory(self, dir_path: Path, max_rows: int) -> PreviewResult:
        """é¢„è§ˆç›®å½•ä¸­çš„æ•°æ®é›†"""
        try:
            # æ‰«æç›®å½•ä¸­çš„æ•°æ®æ–‡ä»¶
            data_files = self._scan_directory(dir_path)
            
            if not data_files:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶: {dir_path}"
                )
            
            # é™åˆ¶æ–‡ä»¶æ•°é‡
            data_files = data_files[:self.config.max_files]
            
            all_data = []
            file_infos = []
            total_rows = 0
            main_format = DatasetFormat.UNKNOWN
            
            # æŒ‰æ–‡ä»¶è¯»å–æ•°æ®
            rows_per_file = max(1, max_rows // len(data_files))
            
            for file_path in data_files:
                try:
                    result = self._preview_single_file(file_path, rows_per_file)
                    if result.success:
                        all_data.extend(result.data)
                        file_infos.extend(result.files)
                        total_rows += result.total_rows
                        if main_format == DatasetFormat.UNKNOWN:
                            main_format = result.format
                except Exception as e:
                    self.logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                    continue
            
            # é™åˆ¶æ€»è¡Œæ•°
            if len(all_data) > max_rows:
                all_data = all_data[:max_rows]
            
            # å¯¹åˆå¹¶åçš„æ•°æ®åº”ç”¨æ™ºèƒ½åˆ—å¤„ç†
            hidden_columns = []
            column_info = {}
            truncated_fields = []
            
            if all_data:
                # åº”ç”¨æ–‡æœ¬æˆªæ–­
                if self.config.enable_truncation:
                    all_data, truncated_fields = self._apply_text_truncation(all_data)
                
                # åº”ç”¨æ™ºèƒ½åˆ—æ˜¾ç¤º
                if self.config.smart_columns and not self.config.show_all_columns:
                    all_data, hidden_columns, column_info = self._apply_smart_columns(all_data)
            
            # ç”Ÿæˆç›®å½•çº§å…ƒæ•°æ®
            metadata = {
                'directory': str(dir_path),
                'scanned_files': len(data_files),
                'successful_files': len(file_infos),
                'formats': list(set(f.format.value for f in file_infos))
            }
            
            return PreviewResult(
                success=True,
                data=all_data,
                files=file_infos,
                total_rows=total_rows,
                total_files=len(file_infos),
                format=main_format,
                metadata=metadata,
                hidden_columns=hidden_columns,
                column_info=column_info,
                truncated_fields=truncated_fields
            )
            
        except Exception as e:
            self.logger.error(f"é¢„è§ˆç›®å½•å¤±è´¥ {dir_path}: {e}")
            return PreviewResult(
                success=False,
                data=[],
                files=[],
                total_rows=0,
                total_files=0,
                format=DatasetFormat.UNKNOWN,
                metadata={},
                error_message=str(e)
            )
    
    def _detect_format(self, file_path: Path) -> DatasetFormat:
        """æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        suffix = file_path.suffix.lower()
        
        if suffix == '.json':
            return DatasetFormat.JSON
        elif suffix == '.jsonl':
            return DatasetFormat.JSONL
        elif suffix == '.csv':
            return DatasetFormat.CSV
        elif suffix == '.tsv':
            return DatasetFormat.TSV
        elif suffix == '.txt':
            return DatasetFormat.TXT
        elif suffix == '.arrow':
            return DatasetFormat.ARROW
        elif suffix == '.parquet':
            return DatasetFormat.PARQUET
        else:
            # å°è¯•é€šè¿‡å†…å®¹æ£€æµ‹
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    first_line = f.readline().strip()
                    if first_line.startswith('{') and first_line.endswith('}'):
                        return DatasetFormat.JSONL
                    elif ',' in first_line or '\t' in first_line:
                        return DatasetFormat.CSV
            except:
                pass
            
            return DatasetFormat.UNKNOWN
    
    def _read_json_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–JSONæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            total_rows = len(data)
            return data[:max_rows], total_rows
        elif isinstance(data, dict):
            # å•ä¸ªå¯¹è±¡ï¼Œè½¬ä¸ºåˆ—è¡¨
            return [data], 1
        else:
            # å…¶ä»–ç±»å‹ï¼ŒåŒ…è£…ä¸ºå­—å…¸
            return [{'value': data}], 1
    
    def _read_jsonl_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–JSONLæ–‡ä»¶"""
        data = []
        total_rows = 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                total_rows += 1
                if len(data) >= max_rows:
                    continue  # ç»§ç»­è®¡æ•°ä½†ä¸åŠ è½½æ•°æ®
                
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num + 1}: {e}")
        
        return data, total_rows
    
    def _read_csv_file(self, file_path: Path, max_rows: int, 
                      file_format: DatasetFormat) -> Tuple[List[Dict], int]:
        """è¯»å–CSV/TSVæ–‡ä»¶"""
        delimiter = '\t' if file_format == DatasetFormat.TSV else ','
        
        # å…ˆè®¡ç®—æ€»è¡Œæ•°
        with open(file_path, 'r', encoding='utf-8') as f:
            total_rows = sum(1 for _ in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, delimiter=delimiter, nrows=max_rows)
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data = df.to_dict('records')
        
        return data, max(0, total_rows)
    
    def _read_text_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        data = []
        total_rows = 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                total_rows += 1
                if len(data) >= max_rows:
                    continue
                
                data.append({
                    'line_number': line_num + 1,
                    'content': line.rstrip('\n\r')
                })
        
        return data, total_rows
    
    def _read_arrow_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–Arrowæ–‡ä»¶"""
        try:
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨HuggingFace datasetsåº“ï¼ˆæœ€ä½³æ–¹æ³•ï¼‰
            try:
                from datasets import Dataset
                dataset = Dataset.from_file(str(file_path))
                
                # è·å–æ€»è¡Œæ•°
                total_rows = len(dataset)
                
                # é™åˆ¶è¡Œæ•°å¹¶è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                if max_rows > 0:
                    sample = dataset.select(range(min(max_rows, total_rows)))
                    data = []
                    for i in range(len(sample)):
                        item = {}
                        for col in dataset.column_names:
                            item[col] = sample[col][i]
                        data.append(item)
                else:
                    data = []
                    for i in range(total_rows):
                        item = {}
                        for col in dataset.column_names:
                            item[col] = dataset[col][i]
                        data.append(item)
                
                return data, total_rows
                
            except ImportError:
                # å¦‚æœæ²¡æœ‰datasetsåº“ï¼Œä½¿ç”¨PyArrow
                pass
            
            import pyarrow as pa
            import pyarrow.parquet as pq
            
            # å°è¯•ä¸åŒçš„Arrowæ–‡ä»¶è¯»å–æ–¹æ³•
            table = None
            
            # æ–¹æ³•1: æ ‡å‡†Arrow IPCæ–‡ä»¶
            try:
                with pa.ipc.open_file(str(file_path)) as reader:
                    table = reader.read_all()
            except:
                pass
            
            # æ–¹æ³•2: å°è¯•ä½œä¸ºFeatheræ–‡ä»¶è¯»å–
            if table is None:
                try:
                    import pyarrow.feather as feather
                    table = feather.read_table(str(file_path))
                except:
                    pass
            
            # æ–¹æ³•3: å°è¯•å†…å­˜æ˜ å°„è¯»å–
            if table is None:
                try:
                    with pa.memory_map(str(file_path), 'r') as source:
                        table = pa.ipc.open_file(source).read_all()
                except:
                    pass
            
            if table is None:
                raise ValueError("æ— æ³•è¯»å–Arrowæ–‡ä»¶ï¼Œå»ºè®®å®‰è£…datasetsåº“ä»¥è·å¾—æ›´å¥½çš„æ”¯æŒ")
            
            # è½¬æ¢ä¸ºpandas DataFrame
            df = table.to_pandas()
            
            # è·å–æ€»è¡Œæ•°
            total_rows = len(df)
            
            # é™åˆ¶è¡Œæ•°
            if max_rows > 0:
                df = df.head(max_rows)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = df.to_dict('records')
            
            # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
            for item in data:
                for key, value in item.items():
                    # å¤„ç†pandasçš„ç‰¹æ®Šç±»å‹
                    if pd.isna(value):
                        item[key] = None
                    elif hasattr(value, 'item'):  # numpyç±»å‹
                        try:
                            item[key] = value.item()
                        except:
                            item[key] = str(value)
                    elif isinstance(value, (list, dict)):
                        # ä¿æŒå¤æ‚ç±»å‹
                        item[key] = value
            
            return data, total_rows
            
        except ImportError:
            raise ValueError("PyArrowæœªå®‰è£…ï¼Œæ— æ³•è¯»å–Arrowæ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–Arrowæ–‡ä»¶å¤±è´¥: {e}")
    
    def _read_parquet_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–Parquetæ–‡ä»¶"""
        try:
            import pyarrow.parquet as pq
            
            # è¯»å–Parquetæ–‡ä»¶
            table = pq.read_table(str(file_path))
            
            # è½¬æ¢ä¸ºpandas DataFrame
            df = table.to_pandas()
            
            # è·å–æ€»è¡Œæ•°
            total_rows = len(df)
            
            # é™åˆ¶è¡Œæ•°
            if max_rows > 0:
                df = df.head(max_rows)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = df.to_dict('records')
            
            # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
            for item in data:
                for key, value in item.items():
                    # å¤„ç†pandasçš„ç‰¹æ®Šç±»å‹
                    if pd.isna(value):
                        item[key] = None
                    elif hasattr(value, 'item'):  # numpyç±»å‹
                        item[key] = value.item()
            
            return data, total_rows
            
        except ImportError:
            raise ValueError("PyArrowæœªå®‰è£…ï¼Œæ— æ³•è¯»å–Parquetæ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–Parquetæ–‡ä»¶å¤±è´¥: {e}")
    
    def _scan_directory(self, dir_path: Path) -> List[Path]:
        """æ‰«æç›®å½•ä¸­çš„æ•°æ®æ–‡ä»¶"""
        data_files = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•åï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        # Arrowæ–‡ä»¶ä¼˜å…ˆï¼Œå› ä¸ºå®ƒä»¬æ˜¯çœŸå®çš„æ•°æ®æ–‡ä»¶
        priority_extensions = ['.arrow', '.parquet']
        regular_extensions = ['.jsonl', '.json', '.csv', '.tsv', '.txt']
        
        # éœ€è¦æ’é™¤çš„æ–‡ä»¶åæ¨¡å¼
        exclude_patterns = {
            'meta.json', 'dataset_info.json', 'dataset_infos.json',
            'state.json', 'dataset_dict.json', 'merge_meta.json',
            'extract_meta.json', 'config.json'
        }
        
        # é¦–å…ˆæŸ¥æ‰¾é«˜ä¼˜å…ˆçº§æ–‡ä»¶ï¼ˆArrow/Parquetï¼‰
        for file_path in dir_path.rglob('*'):
            if (file_path.is_file() and 
                file_path.suffix.lower() in priority_extensions and
                not file_path.name.startswith('.') and
                file_path.name.lower() not in exclude_patterns):
                data_files.append(file_path)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é«˜ä¼˜å…ˆçº§æ–‡ä»¶ï¼Œå†æŸ¥æ‰¾æ™®é€šæ–‡ä»¶
        if not data_files:
            # ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥æ˜¯å¦æœ‰train/testç­‰å­ç›®å½•
            special_subdirs = ['train', 'test', 'val', 'validation', 'dev']
            for subdir_name in special_subdirs:
                subdir_path = dir_path / subdir_name
                if subdir_path.exists() and subdir_path.is_dir():
                    # åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
                    for file_path in subdir_path.rglob('*'):
                        if (file_path.is_file() and 
                            file_path.suffix.lower() in regular_extensions and
                            not file_path.name.startswith('.') and
                            file_path.name.lower() not in exclude_patterns):
                            data_files.append(file_path)
            
            # å¦‚æœå­ç›®å½•ä¸­ä¹Ÿæ²¡æ‰¾åˆ°ï¼Œå†åœ¨æ ¹ç›®å½•æŸ¥æ‰¾
            if not data_files:
                for file_path in dir_path.rglob('*'):
                    if (file_path.is_file() and 
                        file_path.suffix.lower() in regular_extensions and
                        not file_path.name.startswith('.') and
                        file_path.name.lower() not in exclude_patterns):
                        data_files.append(file_path)
        
        # æŒ‰åç§°æ’åº
        return sorted(data_files)
    
    def _apply_text_truncation(self, data: List[Dict]) -> Tuple[List[Dict], List[str]]:
        """åº”ç”¨æ–‡æœ¬æˆªæ–­"""
        if not data or not self.config.enable_truncation:
            return data, []
        
        truncated_fields = set()
        processed_data = []
        
        for item in data:
            processed_item = {}
            for key, value in item.items():
                if isinstance(value, str) and len(value) > self.config.max_text_length:
                    processed_item[key] = value[:self.config.max_text_length] + "..."
                    processed_item[f"{key}_truncated"] = True
                    processed_item[f"{key}_full_length"] = len(value)
                    truncated_fields.add(key)
                else:
                    processed_item[key] = value
            
            processed_data.append(processed_item)
        
        return processed_data, list(truncated_fields)
    
    def _apply_smart_columns(self, data: List[Dict]) -> Tuple[List[Dict], List[str], Dict[str, Any]]:
        """åº”ç”¨æ™ºèƒ½åˆ—æ˜¾ç¤ºé€»è¾‘"""
        if not data:
            return data, [], {}
        
        # åˆ†ææ‰€æœ‰åˆ—çš„æ•°æ®æƒ…å†µ
        column_stats = self._analyze_columns(data)
        
        # ç¡®å®šè¦éšè—çš„åˆ—
        hidden_columns = []
        important_columns = []
        
        for column, stats in column_stats.items():
            # éšè—æ¡ä»¶ï¼š
            # 1. ç©ºå€¼ç‡è¶…è¿‡90%
            # 2. æ‰€æœ‰å€¼éƒ½ç›¸åŒä¸”ä¸æ˜¯é‡è¦åˆ—åï¼ˆä½†ä¿ç•™idåˆ—ï¼‰
            # 3. åˆ—åä¸é‡è¦ä¸”å†…å®¹ç®€å•
            
            null_rate = stats['null_rate']
            unique_count = stats['unique_count']
            is_important = self._is_important_column(column)
            
            # ç‰¹æ®Šå¤„ç†ï¼šå®Œå…¨ç©ºçš„åˆ—ï¼ˆå¦‚inputåˆ—å…¨ç©ºï¼‰
            if null_rate >= 0.95 and not is_important:
                hidden_columns.append(column)
            # å•ä¸€å€¼åˆ—ï¼šé™¤éæ˜¯idæˆ–é‡è¦åˆ—
            elif unique_count == 1 and not is_important and stats['total_count'] > 5 and column.lower() != 'id':
                hidden_columns.append(column)
            # æ— å…³ç´§è¦çš„åˆ—
            elif not is_important and self._is_trivial_column(column, stats):
                hidden_columns.append(column)
            else:
                important_columns.append(column)
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€äº›å…³é”®åˆ—
        if len(important_columns) == 0:
            # å¦‚æœæ²¡æœ‰é‡è¦åˆ—ï¼Œä¿ç•™å‰å‡ ä¸ªéç©ºåˆ—
            for column, stats in sorted(column_stats.items(), key=lambda x: x[1]['null_rate']):
                if len(important_columns) < 3:
                    important_columns.append(column)
                    if column in hidden_columns:
                        hidden_columns.remove(column)
        
        # è¿‡æ»¤æ•°æ®
        filtered_data = []
        for item in data:
            filtered_item = {k: v for k, v in item.items() if k not in hidden_columns}
            filtered_data.append(filtered_item)
        
        return filtered_data, hidden_columns, column_stats
    
    def _analyze_columns(self, data: List[Dict]) -> Dict[str, Dict[str, Any]]:
        """åˆ†æåˆ—çš„æ•°æ®ç‰¹å¾"""
        if not data:
            return {}
        
        # è·å–æ‰€æœ‰åˆ—å
        all_columns = set()
        for item in data:
            all_columns.update(item.keys())
        
        column_stats = {}
        
        for column in all_columns:
            values = []
            null_count = 0
            
            for item in data:
                value = item.get(column)
                if value is None or value == "" or (isinstance(value, str) and value.strip() == ""):
                    null_count += 1
                else:
                    values.append(value)
            
            total_count = len(data)
            null_rate = null_count / total_count if total_count > 0 else 1
            unique_values = set(str(v) for v in values)
            unique_count = len(unique_values)
            
            # è®¡ç®—å¹³å‡æ–‡æœ¬é•¿åº¦
            text_lengths = [len(str(v)) for v in values if v is not None]
            avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
            max_length = max(text_lengths) if text_lengths else 0
            
            column_stats[column] = {
                'total_count': total_count,
                'null_count': null_count,
                'null_rate': null_rate,
                'unique_count': unique_count,
                'avg_length': avg_length,
                'max_length': max_length,
                'sample_values': list(unique_values)[:5]  # å–å‰5ä¸ªæ ·æœ¬å€¼
            }
        
        return column_stats
    
    def _is_important_column(self, column_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦åˆ—"""
        important_keywords = {
            # å¯¹è¯å’Œé—®ç­”ç±»
            'instruction', 'input', 'output', 'response', 'answer', 'question', 'query', 'prompt',
            'text', 'content', 'message', 'conversation', 'dialogue', 'chat',
            
            # æ ‡ç­¾å’Œåˆ†ç±»
            'label', 'target', 'category', 'class', 'type', 'tag', 'classification',
            
            # æ ‡è¯†ç¬¦
            'id', 'name', 'title', 'subject', 'topic', 'key',
            
            # ä¸­æ–‡å­—æ®µ
            'æŒ‡ä»¤', 'è¾“å…¥', 'è¾“å‡º', 'å›ç­”', 'é—®é¢˜', 'å†…å®¹', 'æ–‡æœ¬', 'æ ‡ç­¾', 'ç±»åˆ«', 'åç§°', 'æ ‡é¢˜',
            'é—®', 'ç­”', 'å¯¹è¯', 'èŠå¤©', 'å›å¤', 'å“åº”'
        }
        
        column_lower = column_name.lower()
        return any(keyword in column_lower for keyword in important_keywords)
    
    def _is_trivial_column(self, column_name: str, stats: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ— å…³ç´§è¦çš„åˆ—"""
        trivial_keywords = {
            'meta', 'metadata', 'version', 'timestamp', 'created', 'updated', 'modified',
            'uuid', 'hash', 'checksum', 'index', 'seq', 'sequence', 'order', 'sort',
            'temp', 'tmp', 'debug', 'test', 'example', 'sample', 'dummy',
            'source', 'origin', 'raw', 'original', 'backup', 'old', 'prev', 'previous'
        }
        
        column_lower = column_name.lower()
        
        # åˆ—ååŒ…å«æ— å…³ç´§è¦çš„å…³é”®è¯
        if any(keyword in column_lower for keyword in trivial_keywords):
            return True
        
        # å†…å®¹å¾ˆçŸ­ä¸”é‡å¤ç‡é«˜
        if stats['avg_length'] < 5 and stats['unique_count'] < 3:
            return True
        
        # å•å­—ç¬¦åˆ—ä¸”ä¸é‡è¦
        if len(column_name) <= 2 and not self._is_important_column(column_name):
            return True
        
        return False
    
    def _generate_metadata(self, file_info: FileInfo, data: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        metadata = {
            'file_info': {
                'name': file_info.name,
                'size_bytes': file_info.size,
                'size_human': self._format_size(file_info.size),
                'format': file_info.format.value,
                'total_rows': file_info.row_count
            }
        }
        
        if data and self.config.show_stats:
            # åˆ†ææ•°æ®ç»“æ„
            if isinstance(data[0], dict):
                metadata['schema'] = {
                    'columns': list(data[0].keys()),
                    'column_count': len(data[0].keys())
                }
                
                # å­—æ®µç±»å‹åˆ†æ
                column_types = {}
                for key in data[0].keys():
                    values = [item.get(key) for item in data[:10]]  # å–å‰10è¡Œåˆ†æ
                    types = set(type(v).__name__ for v in values if v is not None)
                    column_types[key] = list(types)
                
                metadata['column_types'] = column_types
        
        return metadata
    
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f}TB"
    
    def preview_to_dataframe(self, path: Union[str, Path], 
                           max_rows: Optional[int] = None) -> pd.DataFrame:
        """é¢„è§ˆæ•°æ®é›†å¹¶è¿”å›DataFrameæ ¼å¼"""
        result = self.preview_dataset(path, max_rows)
        
        if not result.success or not result.data:
            return pd.DataFrame()
        
        return pd.DataFrame(result.data)
    
    def get_dataset_summary(self, path: Union[str, Path]) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†æ‘˜è¦ä¿¡æ¯ï¼ˆä¸åŠ è½½å…·ä½“æ•°æ®ï¼‰"""
        try:
            path = Path(path)
            if not path.exists():
                return {'error': f'è·¯å¾„ä¸å­˜åœ¨: {path}'}
            
            if path.is_file():
                file_format = self._detect_format(path)
                file_size = path.stat().st_size
                
                # å¿«é€Ÿç»Ÿè®¡è¡Œæ•°ï¼ˆä¸åŠ è½½æ•°æ®ï¼‰
                if file_format == DatasetFormat.JSONL:
                    with open(path, 'r') as f:
                        row_count = sum(1 for _ in f)
                elif file_format in [DatasetFormat.CSV, DatasetFormat.TSV]:
                    with open(path, 'r') as f:
                        row_count = sum(1 for _ in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
                else:
                    row_count = None
                
                return {
                    'type': 'file',
                    'name': path.name,
                    'format': file_format.value,
                    'size_bytes': file_size,
                    'size_human': self._format_size(file_size),
                    'estimated_rows': row_count
                }
            
            elif path.is_dir():
                data_files = self._scan_directory(path)
                total_size = sum(f.stat().st_size for f in data_files)
                formats = list(set(self._detect_format(f).value for f in data_files))
                
                # æ”¹å–„å¤šæ–‡ä»¶æ•°æ®é›†çš„åç§°æ˜¾ç¤º
                dataset_name = path.name
                if data_files:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ç»“æ„çš„æ•°æ®é›†
                    subdirs_with_data = set()
                    for f in data_files:
                        rel_path = f.relative_to(path)
                        if len(rel_path.parts) > 1:  # æ–‡ä»¶åœ¨å­ç›®å½•ä¸­
                            subdirs_with_data.add(rel_path.parts[0])
                    
                    if subdirs_with_data:
                        # å¦‚æœæ•°æ®æ–‡ä»¶åœ¨å­ç›®å½•ä¸­ï¼Œæ˜¾ç¤ºæ›´æè¿°æ€§çš„åç§°
                        subdir_names = sorted(subdirs_with_data)
                        if len(subdir_names) == 1:
                            dataset_name = f"{path.name} ({subdir_names[0]})"
                        else:
                            dataset_name = f"{path.name} ({'/'.join(subdir_names[:2])}{'...' if len(subdir_names) > 2 else ''})"
                
                return {
                    'type': 'directory',
                    'name': dataset_name,
                    'file_count': len(data_files),
                    'formats': formats,
                    'total_size_bytes': total_size,
                    'total_size_human': self._format_size(total_size)
                }
            
        except Exception as e:
            return {'error': str(e)}


# CLIæ¥å£
def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®é›†é¢„è§ˆå·¥å…·')
    parser.add_argument('path', help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--max-rows', type=int, default=10, help='æœ€å¤§é¢„è§ˆè¡Œæ•°')
    parser.add_argument('--max-text-length', type=int, default=200, help='æ–‡æœ¬æˆªæ–­é•¿åº¦')
    parser.add_argument('--no-truncation', action='store_true', help='ç¦ç”¨æ–‡æœ¬æˆªæ–­')
    parser.add_argument('--summary-only', action='store_true', help='ä»…æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯')
    parser.add_argument('--output-format', choices=['table', 'json'], default='table', help='è¾“å‡ºæ ¼å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = PreviewConfig(
        max_rows=args.max_rows,
        max_text_length=args.max_text_length,
        enable_truncation=not args.no_truncation
    )
    
    # åˆ›å»ºé¢„è§ˆå™¨
    previewer = DatasetPreviewer(config)
    
    if args.summary_only:
        # ä»…æ˜¾ç¤ºæ‘˜è¦
        summary = previewer.get_dataset_summary(args.path)
        print(json.dumps(summary, indent=2, ensure_ascii=False))
    else:
        # å®Œæ•´é¢„è§ˆ
        result = previewer.preview_dataset(args.path)
        
        if not result.success:
            print(f"é¢„è§ˆå¤±è´¥: {result.error_message}")
            return
        
        if args.output_format == 'json':
            # JSONè¾“å‡º
            output = {
                'success': result.success,
                'data': result.data,
                'metadata': result.metadata,
                'total_rows': result.total_rows,
                'total_files': result.total_files
            }
            print(json.dumps(output, indent=2, ensure_ascii=False))
        else:
            # è¡¨æ ¼è¾“å‡º
            print(f"\nğŸ“Š æ•°æ®é›†é¢„è§ˆ: {args.path}")
            print(f"æ ¼å¼: {result.format.value}")
            print(f"æ€»è¡Œæ•°: {result.total_rows}")
            print(f"æ–‡ä»¶æ•°: {result.total_files}")
            
            if result.truncated_fields:
                print(f"æˆªæ–­å­—æ®µ: {', '.join(result.truncated_fields)}")
            
            print("\n" + "="*80)
            
            if result.data:
                df = pd.DataFrame(result.data)
                print(df.to_string(max_rows=args.max_rows))
            else:
                print("æ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º")


if __name__ == "__main__":
    main()
