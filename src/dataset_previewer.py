#!/usr/bin/env python3
"""
æ•°æ®é›†é¢„è§ˆå™¨ - æ”¯æŒå¤šç§æ ¼å¼çš„æ•°æ®é›†é¢„è§ˆåŠŸèƒ½

æ”¯æŒåŠŸèƒ½ï¼š
1. å¤šç§æ ¼å¼ï¼šJSONã€JSONLã€CSVã€TSVç­‰
2. å¤§æ–‡ä»¶å¤„ç†ï¼šæµå¼è¯»å–ã€åˆ†é¡µé¢„è§ˆ
3. é•¿æ–‡æœ¬å¤„ç†ï¼šæ™ºèƒ½æˆªæ–­ã€å±•å¼€åŠŸèƒ½
4. å¤šæ–‡ä»¶æ•°æ®é›†ï¼šç›®å½•éå†ã€æ–‡ä»¶ç»„åˆ
5. æ•°æ®è´¨é‡æ£€æŸ¥ï¼šæ ¼å¼éªŒè¯ã€ç»Ÿè®¡ä¿¡æ¯
"""

import json
import csv
from .dependencies import pd, datasets
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
from dataclasses import dataclass
from enum import Enum
import re

# å°è¯•å¯¼å…¥ijson
try:
    import ijson
    HAS_IJSON = True
except ImportError:
    HAS_IJSON = False


class DatasetFormat(Enum):
    """æ•°æ®é›†æ ¼å¼æšä¸¾"""
    JSON = "json"
    JSONL = "jsonl"
    CSV = "csv"
    TSV = "tsv"
    TXT = "txt"
    ARROW = "arrow"
    PARQUET = "parquet"
    UNKNOWN = "unknown"


@dataclass
class PreviewConfig:
    """é¢„è§ˆé…ç½®"""
    max_rows: int = 100  # æœ€å¤§é¢„è§ˆè¡Œæ•°
    max_files: int = 10  # æœ€å¤§é¢„è§ˆæ–‡ä»¶æ•°
    max_text_length: int = 200  # æ–‡æœ¬æˆªæ–­é•¿åº¦
    max_file_size_mb: int = 100  # æœ€å¤§å•æ–‡ä»¶å¤§å°(MB)
    enable_truncation: bool = True  # å¯ç”¨æ–‡æœ¬æˆªæ–­
    show_stats: bool = True  # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    include_metadata: bool = True  # åŒ…å«å…ƒæ•°æ®
    smart_columns: bool = True  # æ™ºèƒ½åˆ—æ˜¾ç¤º
    show_all_columns: bool = False  # æ˜¾ç¤ºæ‰€æœ‰åˆ—
    min_column_width: int = 20  # æœ€å°åˆ—å®½
    max_column_width: int = 100  # æœ€å¤§åˆ—å®½


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""
    path: str
    name: str
    size: int
    format: DatasetFormat
    encoding: str = "utf-8"
    row_count: Optional[int] = None
    columns: Optional[List[str]] = None


@dataclass
class PreviewResult:
    """é¢„è§ˆç»“æœ"""
    success: bool
    data: List[Dict[str, Any]]
    files: List[FileInfo]
    total_rows: int
    total_files: int
    format: DatasetFormat
    metadata: Dict[str, Any]
    error_message: str = ""
    truncated_fields: List[str] = None  # è¢«æˆªæ–­çš„å­—æ®µ
    hidden_columns: List[str] = None  # è¢«éšè—çš„åˆ—
    column_info: Dict[str, Any] = None  # åˆ—ä¿¡æ¯ç»Ÿè®¡


class DatasetPreviewer:
    """æ•°æ®é›†é¢„è§ˆå™¨ä¸»ç±»"""
    
    def __init__(self, config: Optional[PreviewConfig] = None):
        self.config = config or PreviewConfig()
        self.logger = logging.getLogger(__name__)
        
    def preview_dataset(self, path: Union[str, Path], 
                       max_rows: Optional[int] = None) -> PreviewResult:
        """
        é¢„è§ˆæ•°æ®é›†
        
        Args:
            path: æ•°æ®é›†è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
            max_rows: æœ€å¤§é¢„è§ˆè¡Œæ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰
            
        Returns:
            PreviewResult: é¢„è§ˆç»“æœ
        """
        try:
            path = Path(path)
            if not path.exists():
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"è·¯å¾„ä¸å­˜åœ¨: {path}"
                )
            
            # ä½¿ç”¨ä¼ å…¥çš„max_rowsæˆ–é…ç½®ä¸­çš„å€¼
            effective_max_rows = max_rows or self.config.max_rows
            
            if path.is_file():
                return self._preview_single_file(path, effective_max_rows)
            elif path.is_dir():
                return self._preview_directory(path, effective_max_rows)
            else:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"æœªçŸ¥è·¯å¾„ç±»å‹: {path}"
                )
                
        except Exception as e:
            self.logger.error(f"é¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}")
            return PreviewResult(
                success=False,
                data=[],
                files=[],
                total_rows=0,
                total_files=0,
                format=DatasetFormat.UNKNOWN,
                metadata={},
                error_message=str(e)
            )
    
    def _preview_single_file(self, file_path: Path, max_rows: int) -> PreviewResult:
        """é¢„è§ˆå•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            
            # å¯¹äºArrowå’ŒParquetæ–‡ä»¶ï¼Œå¢å¤§å¤§å°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸è¾ƒå¤§ä½†è¯»å–æ•ˆç‡é«˜
            # å¯¹äºæ”¯æŒæµå¼è¯»å–çš„æ–‡æœ¬æ ¼å¼(JSONL, CSV, TSV, TXT)ï¼Œä¹Ÿå…è®¸æ›´å¤§çš„æ–‡ä»¶é™åˆ¶
            suffix = file_path.suffix.lower()
            if suffix in {'.arrow', '.parquet'}:
                max_size_limit = self.config.max_file_size_mb * 100  # å¢å¤§100å€é™åˆ¶ (10GB)
            elif suffix in {'.jsonl', '.csv', '.tsv', '.txt'}:
                max_size_limit = self.config.max_file_size_mb * 200 # å¢å¤§200å€é™åˆ¶ (20GB) - æ”¯æŒæµå¼è¯»å–
            elif suffix == '.json':
                # å¦‚æœæœ‰ijsonæ”¯æŒï¼ŒJSONä¹Ÿå¯ä»¥æµå¼è¯»å–ï¼Œæ”¾å®½é™åˆ¶
                if HAS_IJSON:
                    max_size_limit = self.config.max_file_size_mb * 100 # 10GB
                else:
                    max_size_limit = self.config.max_file_size_mb * 5 # 500MB
            else:
                max_size_limit = self.config.max_file_size_mb
            
            if file_size > max_size_limit * 1024 * 1024:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"æ–‡ä»¶è¿‡å¤§: {file_size / (1024*1024):.1f}MBï¼Œè¶…è¿‡é™åˆ¶ {max_size_limit}MB"
                )
            
            # æ£€æµ‹æ–‡ä»¶æ ¼å¼
            file_format = self._detect_format(file_path)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = FileInfo(
                path=str(file_path),
                name=file_path.name,
                size=file_size,
                format=file_format
            )
            
            # æ ¹æ®æ ¼å¼è¯»å–æ•°æ®
            if file_format == DatasetFormat.JSON:
                data, total_rows = self._read_json_file(file_path, max_rows)
            elif file_format == DatasetFormat.JSONL:
                data, total_rows = self._read_jsonl_file(file_path, max_rows)
            elif file_format in [DatasetFormat.CSV, DatasetFormat.TSV]:
                data, total_rows = self._read_csv_file(file_path, max_rows, file_format)
            elif file_format == DatasetFormat.TXT:
                data, total_rows = self._read_text_file(file_path, max_rows)
            elif file_format == DatasetFormat.ARROW:
                data, total_rows = self._read_arrow_file(file_path, max_rows)
            elif file_format == DatasetFormat.PARQUET:
                data, total_rows = self._read_parquet_file(file_path, max_rows)
            else:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[file_info],
                    total_rows=0,
                    total_files=1,
                    format=file_format,
                    metadata={},
                    error_message=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format.value}"
                )
            
            # å¤„ç†é•¿æ–‡æœ¬æˆªæ–­
            truncated_fields = []
            if self.config.enable_truncation:
                data, truncated_fields = self._apply_text_truncation(data)
            
            # å¤„ç†æ™ºèƒ½åˆ—æ˜¾ç¤º
            hidden_columns = []
            column_info = {}
            if self.config.smart_columns and not self.config.show_all_columns:
                data, hidden_columns, column_info = self._apply_smart_columns(data)
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info.row_count = total_rows
            if data:
                file_info.columns = list(data[0].keys()) if isinstance(data[0], dict) else None
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = self._generate_metadata(file_info, data) if self.config.include_metadata else {}
            
            return PreviewResult(
                success=True,
                data=data,
                files=[file_info],
                total_rows=total_rows,
                total_files=1,
                format=file_format,
                metadata=metadata,
                truncated_fields=truncated_fields,
                hidden_columns=hidden_columns,
                column_info=column_info
            )
            
        except Exception as e:
            self.logger.error(f"é¢„è§ˆæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return PreviewResult(
                success=False,
                data=[],
                files=[],
                total_rows=0,
                total_files=0,
                format=DatasetFormat.UNKNOWN,
                metadata={},
                error_message=str(e)
            )
    
    def _preview_directory(self, dir_path: Path, max_rows: int) -> PreviewResult:
        """é¢„è§ˆç›®å½•ä¸­çš„æ•°æ®é›†"""
        try:
            # æ‰«æç›®å½•ä¸­çš„æ•°æ®æ–‡ä»¶
            data_files = self._scan_directory(dir_path)
            
            if not data_files:
                return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶: {dir_path}"
                )
            
            # é™åˆ¶æ–‡ä»¶æ•°é‡ï¼ˆå¢åŠ æ‰«ææ•°é‡ä»¥æé«˜æ‰¾åˆ°æœ‰æ•ˆæ–‡ä»¶çš„æ¦‚ç‡ï¼‰
            # è¿›ä¸€æ­¥å¢åŠ æ‰«æé™åˆ¶ï¼Œå› ä¸ºMegaScienceç­‰æ•°æ®é›†å¯èƒ½åŒ…å«å¤§é‡éæ•°æ®æ–‡ä»¶æˆ–åˆ†ç‰‡
            scan_limit = self.config.max_files * 20 
            data_files = data_files[:scan_limit]
            
            all_data = []
            file_infos = []
            total_rows = 0
            main_format = DatasetFormat.UNKNOWN
            
            # æŒ‰æ–‡ä»¶è¯»å–æ•°æ®
            # ä¼˜åŒ–è¡Œæ•°åˆ†é…ç­–ç•¥ï¼š
            # 1. å¦‚æœæ–‡ä»¶æ•°å°‘äºæœ€å¤§æ–‡ä»¶æ•°ï¼Œåˆ™æ¯ä¸ªæ–‡ä»¶åˆ†é…æ›´å¤šè¡Œæ•°ï¼Œç¡®ä¿æ€»æ•°è¾¾åˆ° max_rows
            # 2. åŠ¨æ€è®¡ç®—å‰©ä½™éœ€è¦çš„è¡Œæ•°
            
            effective_file_count = min(len(data_files), self.config.max_files)
            if effective_file_count > 0:
                # åŸºç¡€åˆ†é…ï¼šä¿è¯æ¯ä¸ªæ–‡ä»¶è‡³å°‘è¯»ä¸€äº›ï¼Œä½†å¦‚æœæ–‡ä»¶å°‘ï¼Œå°±å¤šè¯»ç‚¹
                # ä¾‹å¦‚ï¼šmax_rows=100, files=1 -> rows_per_file=100
                # max_rows=100, files=2 -> rows_per_file=50
                rows_per_file = max(1, max_rows // effective_file_count)
                # ç¨å¾®å¤šè¯»ä¸€ç‚¹ä»¥é˜²æŸäº›æ–‡ä»¶è¡Œæ•°ä¸è¶³
                rows_per_file = int(rows_per_file * 1.2)
            else:
                rows_per_file = max_rows

            successful_files_count = 0
            
            # è®°å½•å°è¯•è¿‡çš„æ–‡ä»¶å’Œé”™è¯¯ï¼Œä»¥ä¾¿è°ƒè¯•
            attempted_files = 0
            errors = []
            
            for file_path in data_files:
                # å¦‚æœå·²ç»è¯»å–äº†è¶³å¤Ÿå¤šçš„æ–‡ä»¶ï¼Œåœæ­¢
                if successful_files_count >= self.config.max_files:
                    break
                
                # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿçš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥åœæ­¢ï¼ˆå¯é€‰ï¼Œä½†ä¸ºäº†å¤šæ ·æ€§é€šå¸¸ç»§ç»­è¯»å–ï¼‰
                if len(all_data) >= max_rows * 1.5: # æ”¶é›†å¤šä¸€ç‚¹å†æˆªæ–­
                    break

                attempted_files += 1
                try:
                    # åŠ¨æ€è°ƒæ•´æœ¬æ¬¡è¯»å–è¡Œæ•°ï¼šå¦‚æœä¹‹å‰è¯»çš„ä¸å¤Ÿï¼Œè¿™æ¬¡å¤šè¯»ç‚¹
                    remaining_rows = max_rows - len(all_data)
                    if remaining_rows <= 0:
                        current_limit = rows_per_file # å³ä½¿å¤Ÿäº†ä¹Ÿè¯»ä¸€ç‚¹ï¼Œä¿æŒå¤šæ ·æ€§
                    else:
                        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ–‡ä»¶ï¼Œå°è¯•è¯»å®Œå‰©ä½™æ‰€éœ€
                        if successful_files_count == effective_file_count - 1:
                            current_limit = remaining_rows
                        else:
                            current_limit = rows_per_file

                    result = self._preview_single_file(file_path, max(current_limit, 10))
                    if result.success:
                        all_data.extend(result.data)
                        file_infos.extend(result.files)
                        total_rows += result.total_rows
                        successful_files_count += 1
                        if main_format == DatasetFormat.UNKNOWN:
                            main_format = result.format
                    else:
                        # è®°å½•å¤±è´¥åŸå› ï¼Œä½†åªè®°å½•å‰å‡ ä¸ª
                        if len(errors) < 3:
                            errors.append(f"{file_path.name}: {result.error_message}")
                except Exception as e:
                    self.logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                    if len(errors) < 3:
                        errors.append(f"{file_path.name}: {str(e)}")
                    continue
            
            if not all_data and not file_infos:
                 error_detail = "; ".join(errors) if errors else "æ— è¯¦ç»†é”™è¯¯"
                 return PreviewResult(
                    success=False,
                    data=[],
                    files=[],
                    total_rows=0,
                    total_files=0,
                    format=DatasetFormat.UNKNOWN,
                    metadata={},
                    error_message=f"æ— æ³•é¢„è§ˆç›®å½•ä¸­çš„ä»»ä½•æ–‡ä»¶ (å°è¯•äº† {attempted_files} ä¸ªæ–‡ä»¶, æ‰«æäº† {len(data_files)} ä¸ªæ–‡ä»¶). é”™è¯¯ç¤ºä¾‹: {error_detail}"
                )
            
            # é™åˆ¶æ€»è¡Œæ•°
            if len(all_data) > max_rows:
                all_data = all_data[:max_rows]
            
            # å¯¹åˆå¹¶åçš„æ•°æ®åº”ç”¨æ™ºèƒ½åˆ—å¤„ç†
            hidden_columns = []
            column_info = {}
            truncated_fields = []
            
            if all_data:
                # åº”ç”¨æ–‡æœ¬æˆªæ–­
                if self.config.enable_truncation:
                    all_data, truncated_fields = self._apply_text_truncation(all_data)
                
                # åº”ç”¨æ™ºèƒ½åˆ—æ˜¾ç¤º
                if self.config.smart_columns and not self.config.show_all_columns:
                    all_data, hidden_columns, column_info = self._apply_smart_columns(all_data)
            
            # ç”Ÿæˆç›®å½•çº§å…ƒæ•°æ®
            metadata = {
                'directory': str(dir_path),
                'scanned_files': len(data_files),
                'successful_files': len(file_infos),
                'formats': list(set(f.format.value for f in file_infos))
            }
            
            return PreviewResult(
                success=True,
                data=all_data,
                files=file_infos,
                total_rows=total_rows,
                total_files=len(file_infos),
                format=main_format,
                metadata=metadata,
                hidden_columns=hidden_columns,
                column_info=column_info,
                truncated_fields=truncated_fields
            )
            
        except Exception as e:
            self.logger.error(f"é¢„è§ˆç›®å½•å¤±è´¥ {dir_path}: {e}")
            return PreviewResult(
                success=False,
                data=[],
                files=[],
                total_rows=0,
                total_files=0,
                format=DatasetFormat.UNKNOWN,
                metadata={},
                error_message=str(e)
            )
    
    def _detect_format(self, file_path: Path) -> DatasetFormat:
        """æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        suffix = file_path.suffix.lower()
        
        if suffix == '.json':
            return DatasetFormat.JSON
        elif suffix == '.jsonl':
            return DatasetFormat.JSONL
        elif suffix == '.csv':
            return DatasetFormat.CSV
        elif suffix == '.tsv':
            return DatasetFormat.TSV
        elif suffix == '.txt':
            return DatasetFormat.TXT
        elif suffix == '.arrow':
            return DatasetFormat.ARROW
        elif suffix == '.parquet':
            return DatasetFormat.PARQUET
        else:
            # å°è¯•é€šè¿‡å†…å®¹æ£€æµ‹
            try:
                with open(file_path, 'rb') as f:
                    # è¯»å–å¼€å¤´éƒ¨åˆ†å­—èŠ‚
                    chunk = f.read(1024)
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« NULL å­—èŠ‚ï¼Œå¦‚æœæ˜¯åˆ™è§†ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¸æ”¯æŒ
                    if b'\x00' in chunk:
                        return DatasetFormat.UNKNOWN
                        
                    # å°è¯•è§£ç 
                    try:
                        text = chunk.decode('utf-8').strip()
                    except UnicodeDecodeError:
                        return DatasetFormat.UNKNOWN
                        
                    first_line = text.split('\n')[0].strip()
                    if first_line.startswith('{') and first_line.endswith('}'):
                        return DatasetFormat.JSONL
                    elif ',' in first_line or '\t' in first_line:
                        return DatasetFormat.CSV
            except:
                pass
            
            return DatasetFormat.UNKNOWN
    
    def _read_json_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–JSONæ–‡ä»¶"""
        # å¦‚æœæœ‰ijsonï¼Œä½¿ç”¨æµå¼è¯»å–ä»¥æ”¯æŒå¤§æ–‡ä»¶
        if HAS_IJSON:
            try:
                data = []
                with open(file_path, 'rb') as f:
                    # å°è¯•æ£€æµ‹æ˜¯å¦æ˜¯åˆ—è¡¨
                    pos = f.tell()
                    first_char = f.read(1)
                    while first_char and first_char.isspace():
                        first_char = f.read(1)
                    f.seek(pos)
                    
                    if first_char == b'[':
                        # åˆ—è¡¨æ¨¡å¼ï¼šæµå¼è¯»å–å‰max_rowsä¸ªå…ƒç´ 
                        items = ijson.items(f, 'item')
                        for i, item in enumerate(items):
                            if i >= max_rows:
                                break
                            data.append(item)
                        
                        # æ— æ³•è½»æ˜“è·å–æ€»è¡Œæ•°ï¼Œé™¤ééå†æ•´ä¸ªæ–‡ä»¶
                        # è¿™é‡Œè¿”å›-1è¡¨ç¤ºæœªçŸ¥ï¼Œæˆ–è€…å¦‚æœè¯»å–å®Œäº†å°±æ˜¯å½“å‰æ•°é‡
                        total_rows = len(data) if len(data) < max_rows else -1
                        return data, total_rows
                    else:
                        # å¯¹è±¡æ¨¡å¼ï¼šè¯»å–æ•´ä¸ªå¯¹è±¡
                        # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œè¿™é‡Œå¯èƒ½ä¼šå¤±è´¥ï¼Œä½†å¯¹äºéåˆ—è¡¨JSONï¼Œé€šå¸¸ä¸æ˜¯å¤§æ•°æ®é›†æ ¼å¼
                        f.seek(0)
                        data_obj = json.load(f)
                        if isinstance(data_obj, list):
                            return data_obj[:max_rows], len(data_obj)
                        else:
                            return [data_obj], 1
            except Exception as e:
                self.logger.warning(f"ä½¿ç”¨ijsonè¯»å–JSONå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼: {e}")
                # å›é€€åˆ°æ™®é€šæ¨¡å¼
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            total_rows = len(data)
            return data[:max_rows], total_rows
        elif isinstance(data, dict):
            # å•ä¸ªå¯¹è±¡ï¼Œè½¬ä¸ºåˆ—è¡¨
            return [data], 1
        else:
            # å…¶ä»–ç±»å‹ï¼ŒåŒ…è£…ä¸ºå­—å…¸
            return [{'value': data}], 1
    
    def _read_jsonl_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–JSONLæ–‡ä»¶"""
        data = []
        total_rows = 0
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœå¤ªå¤§åˆ™ä¸è®¡ç®—æ€»è¡Œæ•°
        file_size = file_path.stat().st_size
        skip_count = file_size > 100 * 1024 * 1024  # 100MB
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                for line_num, line in enumerate(f):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« NULL å­—ç¬¦ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡ï¼ˆå¯èƒ½æ˜¯æŸåçš„æ•°æ®ï¼‰
                    if '\x00' in line:
                        continue
                        
                    if skip_count and len(data) >= max_rows:
                        total_rows = -1 # è¡¨ç¤ºæœªçŸ¥/æœªè®¡ç®—
                        break
                    
                    total_rows += 1
                    if len(data) >= max_rows:
                        continue  # ç»§ç»­è®¡æ•°ä½†ä¸åŠ è½½æ•°æ®
                    
                    line = line.strip()
                    if line:
                        try:
                            item = json.loads(line)
                            data.append(item)
                        except json.JSONDecodeError as e:
                            # ä»…åœ¨æœªè¾¾åˆ°æœ€å¤§è¡Œæ•°æ—¶è®°å½•è­¦å‘Šï¼Œé¿å…æ—¥å¿—çˆ†ç‚¸
                            if len(data) < max_rows:
                                self.logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num + 1}: {e}")
        except Exception as e:
            self.logger.error(f"è¯»å–JSONLæ–‡ä»¶å‡ºé”™: {e}")
            
        return data, total_rows
    
    def _read_csv_file(self, file_path: Path, max_rows: int, 
                      file_format: DatasetFormat) -> Tuple[List[Dict], int]:
        """è¯»å–CSV/TSVæ–‡ä»¶"""
        delimiter = '\t' if file_format == DatasetFormat.TSV else ','
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size
        skip_count = file_size > 100 * 1024 * 1024  # 100MB
        
        total_rows = -1
        if not skip_count:
            # å…ˆè®¡ç®—æ€»è¡Œæ•°
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    total_rows = sum(1 for _ in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
            except Exception:
                pass
        
        # è¯»å–æ•°æ®
        try:
            # ä½¿ç”¨ on_bad_lines='skip' é¿å…æ ¼å¼é”™è¯¯å¯¼è‡´å´©æºƒ
            df = pd.read_csv(file_path, delimiter=delimiter, nrows=max_rows, on_bad_lines='skip')
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = df.to_dict('records')
        except Exception as e:
            self.logger.error(f"è¯»å–CSVå¤±è´¥: {e}")
            data = []
        
        return data, total_rows if total_rows != -1 else -1
    
    def _read_text_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        data = []
        total_rows = 0
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size
        skip_count = file_size > 100 * 1024 * 1024  # 100MB
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                if skip_count and len(data) >= max_rows:
                    total_rows = -1
                    break

                total_rows += 1
                if len(data) >= max_rows:
                    continue
                
                data.append({
                    'line_number': line_num + 1,
                    'content': line.rstrip('\n\r')
                })
        
        return data, total_rows
    
    def _read_arrow_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–Arrowæ–‡ä»¶"""
        try:
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨HuggingFace datasetsåº“ï¼ˆæœ€ä½³æ–¹æ³•ï¼‰
            if datasets:
                dataset = datasets.Dataset.from_file(str(file_path))
                
                # è·å–æ€»è¡Œæ•°
                total_rows = len(dataset)
                
                # é™åˆ¶è¡Œæ•°å¹¶è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                if max_rows > 0:
                    sample = dataset.select(range(min(max_rows, total_rows)))
                    data = []
                    for i in range(len(sample)):
                        item = {}
                        for col in dataset.column_names:
                            item[col] = sample[col][i]
                        data.append(item)
                else:
                    data = []
                    for i in range(total_rows):
                        item = {}
                        for col in dataset.column_names:
                            item[col] = dataset[col][i]
                        data.append(item)
                
                return data, total_rows
                
            # å¦‚æœæ²¡æœ‰datasetsåº“ï¼Œä½¿ç”¨PyArrow
            from .dependencies import pa, pq
            
            if pa is None:
                 raise ImportError("PyArrow is required for Arrow files")
            
            # å°è¯•ä¸åŒçš„Arrowæ–‡ä»¶è¯»å–æ–¹æ³•
            table = None
            
            # æ–¹æ³•1: æ ‡å‡†Arrow IPCæ–‡ä»¶
            try:
                with pa.ipc.open_file(str(file_path)) as reader:
                    table = reader.read_all()
            except:
                pass
            
            # æ–¹æ³•2: å°è¯•ä½œä¸ºFeatheræ–‡ä»¶è¯»å–
            if table is None:
                try:
                    import pyarrow.feather as feather
                    table = feather.read_table(str(file_path))
                except:
                    pass
            
            # æ–¹æ³•3: å°è¯•å†…å­˜æ˜ å°„è¯»å–
            if table is None:
                try:
                    with pa.memory_map(str(file_path), 'r') as source:
                        table = pa.ipc.open_file(source).read_all()
                except:
                    pass
            
            if table is None:
                raise ValueError("æ— æ³•è¯»å–Arrowæ–‡ä»¶ï¼Œå»ºè®®å®‰è£…datasetsåº“ä»¥è·å¾—æ›´å¥½çš„æ”¯æŒ")
            
            # è½¬æ¢ä¸ºpandas DataFrame
            df = table.to_pandas()
            
            # è·å–æ€»è¡Œæ•°
            total_rows = len(df)
            
            # é™åˆ¶è¡Œæ•°
            if max_rows > 0:
                df = df.head(max_rows)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = df.to_dict('records')
            
            # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
            for item in data:
                for key, value in item.items():
                    # å¤„ç†pandasçš„ç‰¹æ®Šç±»å‹
                    if pd.isna(value):
                        item[key] = None
                    elif hasattr(value, 'item'):  # numpyç±»å‹
                        try:
                            item[key] = value.item()
                        except:
                            item[key] = str(value)
                    elif isinstance(value, (list, dict)):
                        # ä¿æŒå¤æ‚ç±»å‹
                        item[key] = value
            
            return data, total_rows
            
        except ImportError:
            raise ValueError("PyArrowæœªå®‰è£…ï¼Œæ— æ³•è¯»å–Arrowæ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–Arrowæ–‡ä»¶å¤±è´¥: {e}")
    
    def _read_parquet_file(self, file_path: Path, max_rows: int) -> Tuple[List[Dict], int]:
        """è¯»å–Parquetæ–‡ä»¶"""
        try:
            from .dependencies import pq
            if pq is None:
                raise ImportError("pyarrow.parquet is required for Parquet files")
            
            # è¯»å–Parquetæ–‡ä»¶
            table = pq.read_table(str(file_path))
            
            # è½¬æ¢ä¸ºpandas DataFrame
            df = table.to_pandas()
            
            # è·å–æ€»è¡Œæ•°
            total_rows = len(df)
            
            # é™åˆ¶è¡Œæ•°
            if max_rows > 0:
                df = df.head(max_rows)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = df.to_dict('records')
            
            # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
            for item in data:
                for key, value in item.items():
                    # å¤„ç†pandasçš„ç‰¹æ®Šç±»å‹
                    if pd.isna(value):
                        item[key] = None
                    elif hasattr(value, 'item'):  # numpyç±»å‹
                        item[key] = value.item()
            
            return data, total_rows
            
        except ImportError:
            raise ValueError("PyArrowæœªå®‰è£…ï¼Œæ— æ³•è¯»å–Parquetæ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–Parquetæ–‡ä»¶å¤±è´¥: {e}")
    
    def _scan_directory(self, dir_path: Path) -> List[Path]:
        """æ‰«æç›®å½•ä¸­çš„æ•°æ®æ–‡ä»¶"""
        data_files = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        priority_extensions = {'.arrow', '.parquet'}
        regular_extensions = {'.jsonl', '.json', '.csv', '.tsv', '.txt'}
        
        # éœ€è¦æ’é™¤çš„æ–‡ä»¶åæ¨¡å¼
        exclude_patterns = {
            'meta.json', 'dataset_info.json', 'dataset_infos.json',
            'state.json', 'dataset_dict.json', 'merge_meta.json',
            'extract_meta.json', 'config.json', 'tokenizer.json',
            'config.json', 'added_tokens.json', 'special_tokens_map.json',
            'readme.md', 'license', 'license.txt', 'checkpoint.json',
            'checkpoint.json.tmp'
        }
        
        # é€’å½’æ‰«ææ‰€æœ‰æ–‡ä»¶
        try:
            # ä½¿ç”¨rglobæ‰«ææ‰€æœ‰æ–‡ä»¶
            all_files = list(dir_path.rglob('*'))
            
            # è¿‡æ»¤å’Œåˆ†ç±»
            priority_files = []
            regular_files = []
            
            for file_path in all_files:
                if not file_path.is_file():
                    continue
                    
                # æ’é™¤éšè—æ–‡ä»¶å’Œç›®å½•
                if any(part.startswith('.') for part in file_path.relative_to(dir_path).parts):
                    continue
                    
                # æ’é™¤ç‰¹å®šæ¨¡å¼
                if file_path.name.lower() in exclude_patterns:
                    continue
                
                # æ’é™¤æ—¥å¿—æ–‡ä»¶
                if file_path.suffix.lower() == '.log':
                    continue
                
                suffix = file_path.suffix.lower()
                if suffix in priority_extensions:
                    priority_files.append(file_path)
                elif suffix in regular_extensions:
                    regular_files.append(file_path)
            
            # ä¼˜å…ˆè¿”å›ä¼˜å…ˆçº§é«˜çš„æ–‡ä»¶
            if priority_files:
                return sorted(priority_files)
            
            # å…¶æ¬¡è¿”å›æ™®é€šæ–‡ä»¶
            return sorted(regular_files)
            
        except Exception as e:
            self.logger.error(f"æ‰«æç›®å½•å¤±è´¥ {dir_path}: {e}")
            return []
    
    def _apply_text_truncation(self, data: List[Dict]) -> Tuple[List[Dict], List[str]]:
        """åº”ç”¨æ–‡æœ¬æˆªæ–­"""
        if not data or not self.config.enable_truncation:
            return data, []
        
        truncated_fields = set()
        processed_data = []
        
        for item in data:
            processed_item = {}
            for key, value in item.items():
                if isinstance(value, str) and len(value) > self.config.max_text_length:
                    processed_item[key] = value[:self.config.max_text_length] + "..."
                    processed_item[f"{key}_truncated"] = True
                    processed_item[f"{key}_full_length"] = len(value)
                    truncated_fields.add(key)
                else:
                    processed_item[key] = value
            
            processed_data.append(processed_item)
        
        return processed_data, list(truncated_fields)
    
    def _apply_smart_columns(self, data: List[Dict]) -> Tuple[List[Dict], List[str], Dict[str, Any]]:
        """åº”ç”¨æ™ºèƒ½åˆ—æ˜¾ç¤ºé€»è¾‘"""
        if not data:
            return data, [], {}
        
        # åˆ†ææ‰€æœ‰åˆ—çš„æ•°æ®æƒ…å†µ
        column_stats = self._analyze_columns(data)
        
        # ç¡®å®šè¦éšè—çš„åˆ—
        hidden_columns = []
        important_columns = []
        
        for column, stats in column_stats.items():
            # éšè—æ¡ä»¶ï¼š
            # 1. ç©ºå€¼ç‡è¶…è¿‡90%
            # 2. æ‰€æœ‰å€¼éƒ½ç›¸åŒä¸”ä¸æ˜¯é‡è¦åˆ—åï¼ˆä½†ä¿ç•™idåˆ—ï¼‰
            # 3. åˆ—åä¸é‡è¦ä¸”å†…å®¹ç®€å•
            
            null_rate = stats['null_rate']
            unique_count = stats['unique_count']
            is_important = self._is_important_column(column)
            
            # ç‰¹æ®Šå¤„ç†ï¼šå®Œå…¨ç©ºçš„åˆ—ï¼ˆå¦‚inputåˆ—å…¨ç©ºï¼‰
            if null_rate >= 0.95 and not is_important:
                hidden_columns.append(column)
            # å•ä¸€å€¼åˆ—ï¼šé™¤éæ˜¯idæˆ–é‡è¦åˆ—
            elif unique_count == 1 and not is_important and stats['total_count'] > 5 and column.lower() != 'id':
                hidden_columns.append(column)
            # æ— å…³ç´§è¦çš„åˆ—
            elif not is_important and self._is_trivial_column(column, stats):
                hidden_columns.append(column)
            else:
                important_columns.append(column)
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€äº›å…³é”®åˆ—
        if len(important_columns) == 0:
            # å¦‚æœæ²¡æœ‰é‡è¦åˆ—ï¼Œä¿ç•™å‰å‡ ä¸ªéç©ºåˆ—
            for column, stats in sorted(column_stats.items(), key=lambda x: x[1]['null_rate']):
                if len(important_columns) < 3:
                    important_columns.append(column)
                    if column in hidden_columns:
                        hidden_columns.remove(column)
        
        # è¿‡æ»¤æ•°æ®
        filtered_data = []
        for item in data:
            filtered_item = {k: v for k, v in item.items() if k not in hidden_columns}
            filtered_data.append(filtered_item)
        
        return filtered_data, hidden_columns, column_stats
    
    def _analyze_columns(self, data: List[Dict]) -> Dict[str, Dict[str, Any]]:
        """åˆ†æåˆ—çš„æ•°æ®ç‰¹å¾"""
        if not data:
            return {}
        
        # è·å–æ‰€æœ‰åˆ—å
        all_columns = set()
        for item in data:
            all_columns.update(item.keys())
        
        column_stats = {}
        
        for column in all_columns:
            values = []
            null_count = 0
            
            for item in data:
                value = item.get(column)
                if value is None or value == "" or (isinstance(value, str) and value.strip() == ""):
                    null_count += 1
                else:
                    values.append(value)
            
            total_count = len(data)
            null_rate = null_count / total_count if total_count > 0 else 1
            unique_values = set(str(v) for v in values)
            unique_count = len(unique_values)
            
            # è®¡ç®—å¹³å‡æ–‡æœ¬é•¿åº¦
            text_lengths = [len(str(v)) for v in values if v is not None]
            avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
            max_length = max(text_lengths) if text_lengths else 0
            
            column_stats[column] = {
                'total_count': total_count,
                'null_count': null_count,
                'null_rate': null_rate,
                'unique_count': unique_count,
                'avg_length': avg_length,
                'max_length': max_length,
                'sample_values': list(unique_values)[:5]  # å–å‰5ä¸ªæ ·æœ¬å€¼
            }
        
        return column_stats
    
    def _is_important_column(self, column_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦åˆ—"""
        important_keywords = {
            # å¯¹è¯å’Œé—®ç­”ç±»
            'instruction', 'input', 'output', 'response', 'answer', 'question', 'query', 'prompt',
            'text', 'content', 'message', 'conversation', 'dialogue', 'chat',
            
            # æ ‡ç­¾å’Œåˆ†ç±»
            'label', 'target', 'category', 'class', 'type', 'tag', 'classification',
            
            # æ ‡è¯†ç¬¦
            'id', 'name', 'title', 'subject', 'topic', 'key',
            
            # ä¸­æ–‡å­—æ®µ
            'æŒ‡ä»¤', 'è¾“å…¥', 'è¾“å‡º', 'å›ç­”', 'é—®é¢˜', 'å†…å®¹', 'æ–‡æœ¬', 'æ ‡ç­¾', 'ç±»åˆ«', 'åç§°', 'æ ‡é¢˜',
            'é—®', 'ç­”', 'å¯¹è¯', 'èŠå¤©', 'å›å¤', 'å“åº”'
        }
        
        column_lower = column_name.lower()
        return any(keyword in column_lower for keyword in important_keywords)
    
    def _is_trivial_column(self, column_name: str, stats: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ— å…³ç´§è¦çš„åˆ—"""
        trivial_keywords = {
            'meta', 'metadata', 'version', 'timestamp', 'created', 'updated', 'modified',
            'uuid', 'hash', 'checksum', 'index', 'seq', 'sequence', 'order', 'sort',
            'temp', 'tmp', 'debug', 'test', 'example', 'sample', 'dummy',
            'source', 'origin', 'raw', 'original', 'backup', 'old', 'prev', 'previous'
        }
        
        column_lower = column_name.lower()
        
        # åˆ—ååŒ…å«æ— å…³ç´§è¦çš„å…³é”®è¯
        if any(keyword in column_lower for keyword in trivial_keywords):
            return True
        
        # å†…å®¹å¾ˆçŸ­ä¸”é‡å¤ç‡é«˜
        if stats['avg_length'] < 5 and stats['unique_count'] < 3:
            return True
        
        # å•å­—ç¬¦åˆ—ä¸”ä¸é‡è¦
        if len(column_name) <= 2 and not self._is_important_column(column_name):
            return True
        
        return False
    
    def _generate_metadata(self, file_info: FileInfo, data: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        metadata = {
            'file_info': {
                'name': file_info.name,
                'size_bytes': file_info.size,
                'size_human': self._format_size(file_info.size),
                'format': file_info.format.value,
                'total_rows': file_info.row_count
            }
        }
        
        if data and self.config.show_stats:
            # åˆ†ææ•°æ®ç»“æ„
            if isinstance(data[0], dict):
                metadata['schema'] = {
                    'columns': list(data[0].keys()),
                    'column_count': len(data[0].keys())
                }
                
                # å­—æ®µç±»å‹åˆ†æ
                column_types = {}
                for key in data[0].keys():
                    values = [item.get(key) for item in data[:10]]  # å–å‰10è¡Œåˆ†æ
                    types = set(type(v).__name__ for v in values if v is not None)
                    column_types[key] = list(types)
                
                metadata['column_types'] = column_types
        
        return metadata
    
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f}TB"
    
    def preview_to_dataframe(self, path: Union[str, Path], 
                           max_rows: Optional[int] = None) -> pd.DataFrame:
        """é¢„è§ˆæ•°æ®é›†å¹¶è¿”å›DataFrameæ ¼å¼"""
        result = self.preview_dataset(path, max_rows)
        
        if not result.success or not result.data:
            return pd.DataFrame()
        
        return pd.DataFrame(result.data)
    
    def get_dataset_summary(self, path: Union[str, Path]) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†æ‘˜è¦ä¿¡æ¯ï¼ˆä¸åŠ è½½å…·ä½“æ•°æ®ï¼‰"""
        try:
            path = Path(path)
            if not path.exists():
                return {'error': f'è·¯å¾„ä¸å­˜åœ¨: {path}'}
            
            if path.is_file():
                file_format = self._detect_format(path)
                file_size = path.stat().st_size
                
                # å¿«é€Ÿç»Ÿè®¡è¡Œæ•°ï¼ˆä¸åŠ è½½æ•°æ®ï¼‰
                if file_format == DatasetFormat.JSONL:
                    with open(path, 'r') as f:
                        row_count = sum(1 for _ in f)
                elif file_format in [DatasetFormat.CSV, DatasetFormat.TSV]:
                    with open(path, 'r') as f:
                        row_count = sum(1 for _ in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
                else:
                    row_count = None
                
                return {
                    'type': 'file',
                    'name': path.name,
                    'format': file_format.value,
                    'size_bytes': file_size,
                    'size_human': self._format_size(file_size),
                    'estimated_rows': row_count
                }
            
            elif path.is_dir():
                data_files = self._scan_directory(path)
                total_size = sum(f.stat().st_size for f in data_files)
                formats = list(set(self._detect_format(f).value for f in data_files))
                
                # æ”¹å–„å¤šæ–‡ä»¶æ•°æ®é›†çš„åç§°æ˜¾ç¤º
                dataset_name = path.name
                if data_files:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ç»“æ„çš„æ•°æ®é›†
                    subdirs_with_data = set()
                    for f in data_files:
                        rel_path = f.relative_to(path)
                        if len(rel_path.parts) > 1:  # æ–‡ä»¶åœ¨å­ç›®å½•ä¸­
                            subdirs_with_data.add(rel_path.parts[0])
                    
                    if subdirs_with_data:
                        # å¦‚æœæ•°æ®æ–‡ä»¶åœ¨å­ç›®å½•ä¸­ï¼Œæ˜¾ç¤ºæ›´æè¿°æ€§çš„åç§°
                        subdir_names = sorted(subdirs_with_data)
                        if len(subdir_names) == 1:
                            dataset_name = f"{path.name} ({subdir_names[0]})"
                        else:
                            dataset_name = f"{path.name} ({'/'.join(subdir_names[:2])}{'...' if len(subdir_names) > 2 else ''})"
                
                return {
                    'type': 'directory',
                    'name': dataset_name,
                    'file_count': len(data_files),
                    'formats': formats,
                    'total_size_bytes': total_size,
                    'total_size_human': self._format_size(total_size)
                }
            
        except Exception as e:
            return {'error': str(e)}


# CLIæ¥å£
def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®é›†é¢„è§ˆå·¥å…·')
    parser.add_argument('path', help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--max-rows', type=int, default=10, help='æœ€å¤§é¢„è§ˆè¡Œæ•°')
    parser.add_argument('--max-text-length', type=int, default=200, help='æ–‡æœ¬æˆªæ–­é•¿åº¦')
    parser.add_argument('--no-truncation', action='store_true', help='ç¦ç”¨æ–‡æœ¬æˆªæ–­')
    parser.add_argument('--summary-only', action='store_true', help='ä»…æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯')
    parser.add_argument('--output-format', choices=['table', 'json'], default='table', help='è¾“å‡ºæ ¼å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = PreviewConfig(
        max_rows=args.max_rows,
        max_text_length=args.max_text_length,
        enable_truncation=not args.no_truncation
    )
    
    # åˆ›å»ºé¢„è§ˆå™¨
    previewer = DatasetPreviewer(config)
    
    if args.summary_only:
        # ä»…æ˜¾ç¤ºæ‘˜è¦
        summary = previewer.get_dataset_summary(args.path)
        print(json.dumps(summary, indent=2, ensure_ascii=False))
    else:
        # å®Œæ•´é¢„è§ˆ
        result = previewer.preview_dataset(args.path)
        
        if not result.success:
            print(f"é¢„è§ˆå¤±è´¥: {result.error_message}")
            return
        
        if args.output_format == 'json':
            # JSONè¾“å‡º
            output = {
                'success': result.success,
                'data': result.data,
                'metadata': result.metadata,
                'total_rows': result.total_rows,
                'total_files': result.total_files
            }
            print(json.dumps(output, indent=2, ensure_ascii=False))
        else:
            # è¡¨æ ¼è¾“å‡º
            print(f"\nğŸ“Š æ•°æ®é›†é¢„è§ˆ: {args.path}")
            print(f"æ ¼å¼: {result.format.value}")
            print(f"æ€»è¡Œæ•°: {result.total_rows}")
            print(f"æ–‡ä»¶æ•°: {result.total_files}")
            
            if result.truncated_fields:
                print(f"æˆªæ–­å­—æ®µ: {', '.join(result.truncated_fields)}")
            
            print("\n" + "="*80)
            
            if result.data:
                df = pd.DataFrame(result.data)
                print(df.to_string(max_rows=args.max_rows))
            else:
                print("æ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º")


if __name__ == "__main__":
    main()
