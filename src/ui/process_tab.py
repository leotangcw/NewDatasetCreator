import os
import time
from datetime import datetime
import re
import json
from pathlib import Path
import gradio as gr
from typing import Dict, Any, Tuple, List
from ..dependencies import pd
from ..data_cleaner import data_cleaner
from ..universal_field_extractor import get_field_names_universal, extract_fields_universal

class ProcessTabManager:
    def __init__(self, launcher):
        self.launcher = launcher
        self.logger = launcher.logger
        self.format_converter = launcher.format_converter
        self.field_extractor = launcher.field_extractor
        self.data_merger = launcher.data_merger
        self.data_cleaner = data_cleaner
        
        # æ–°å¢ï¼šåˆå¹¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        self.merge_file_paths = []

    def _start_format_convert(self, source_file, target_format: str, output_dir: str) -> str:
        """å¼€å§‹æ ¼å¼è½¬æ¢"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"
            
            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # åˆ›å»ºè½¬æ¢ä»»åŠ¡
            task_id = self.format_converter.add_convert_task(
                source_path=source_path,
                target_format=target_format,
                output_dir=output_dir,
                use_subdirectory=False
            )
            
            # å¯åŠ¨ä»»åŠ¡
            success = self.format_converter.start_task(task_id)
            if not success:
                return f"âŒ å¯åŠ¨è½¬æ¢ä»»åŠ¡å¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}"
            
            return f"âœ… è½¬æ¢ä»»åŠ¡å·²å¯åŠ¨ï¼\nä»»åŠ¡ID: {task_id}\næ–‡ä»¶: {os.path.basename(source_path)}\nç›®æ ‡æ ¼å¼: {target_format.upper()}"
            
        except Exception as e:
            self.logger.error(f'å¯åŠ¨æ ¼å¼è½¬æ¢å¤±è´¥: {e}')
            return f"âŒ å¯åŠ¨è½¬æ¢å¤±è´¥: {str(e)}"

    def _get_convert_tasks_df(self) -> pd.DataFrame:
        """è·å–è½¬æ¢ä»»åŠ¡åˆ—è¡¨"""
        try:
            tasks = self.format_converter.list_tasks()
            if not tasks:
                return pd.DataFrame(columns=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"])
            
            task_data = []
            for task in tasks:
                task_id = task.get('task_id', 'N/A')
                source_path = task.get('source_path', '')
                source_file = os.path.basename(source_path) if source_path else 'N/A'
                target_format = task.get('target_format', 'N/A').upper()
                status = task.get('status', 'unknown')
                progress = f"{task.get('progress', 0)}%"
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                if status == 'completed':
                    source_stem = Path(source_path).stem if source_path else 'unknown'
                    source_ext = Path(source_path).suffix.replace('.', '') if source_path else ''
                    target_ext = 'md' if target_format.lower() == 'markdown' else target_format.lower()
                    output_filename = f"{source_stem}_{source_ext}2{target_ext}.{target_ext}"
                else:
                    output_filename = "è½¬æ¢ä¸­..."
                
                task_data.append([
                    task_id[:15] + "..." if len(task_id) > 15 else task_id,
                    source_file,
                    target_format,
                    status,
                    progress,
                    output_filename
                ])
            
            return pd.DataFrame(task_data, columns=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"])
            
        except Exception as e:
            self.logger.error(f'è·å–è½¬æ¢ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"])

    def _select_convert_task(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©è½¬æ¢ä»»åŠ¡"""
        try:
            row_data = evt.row_value
            if row_data and len(row_data) >= 1:
                return row_data[0]  # è¿”å›ä»»åŠ¡ID
            return ""
        except Exception as e:
            self.logger.error(f'é€‰æ‹©è½¬æ¢ä»»åŠ¡å¤±è´¥: {e}')
            return ""

    def _view_convert_result(self, task_id: str) -> str:
        """æŸ¥çœ‹è½¬æ¢ç»“æœè¯¦æƒ…"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„ä»»åŠ¡"
            
            # è·å–å®Œæ•´ä»»åŠ¡IDï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
            tasks = self.format_converter.list_tasks()
            full_task_id = None
            for task in tasks:
                if task.get('task_id', '').startswith(task_id.replace('...', '')):
                    full_task_id = task.get('task_id')
                    break
            
            if not full_task_id:
                return "âŒ ä»»åŠ¡ä¸å­˜åœ¨"
                
            task = self.format_converter.get_task_progress(full_task_id)
            if not task:
                return "âŒ æ— æ³•è·å–ä»»åŠ¡ä¿¡æ¯"
                
            info = [
                f"ä»»åŠ¡ID: {full_task_id}",
                f"æºæ–‡ä»¶: {task.get('source_path', 'N/A')}",
                f"ç›®æ ‡æ ¼å¼: {task.get('target_format', 'N/A')}",
                f"çŠ¶æ€: {task.get('status', 'unknown')}",
                f"è¿›åº¦: {task.get('progress', 0)}%",
                f"å¼€å§‹æ—¶é—´: {task.get('start_time', 'N/A')}",
                f"ç»“æŸæ—¶é—´: {task.get('end_time', 'N/A')}",
                f"é”™è¯¯ä¿¡æ¯: {task.get('error_msg', 'æ— ')}"
            ]
            
            return "\n".join(info)
            
        except Exception as e:
            self.logger.error(f'æŸ¥çœ‹è½¬æ¢ç»“æœå¤±è´¥: {e}')
            return f"âŒ æŸ¥çœ‹ç»“æœå¤±è´¥: {str(e)}"

    def _preview_extract_fields(self, file_obj) -> gr.CheckboxGroup:
        """è·å–æ–‡ä»¶å­—æ®µåˆ—è¡¨"""
        try:
            if file_obj is None:
                return gr.CheckboxGroup(choices=[], value=[])
            
            file_path = file_obj.name
            fields = get_field_names_universal(file_path)
            
            return gr.CheckboxGroup(choices=fields, value=[])
        except Exception as e:
            self.logger.error(f'è·å–å­—æ®µå¤±è´¥: {e}')
            return gr.CheckboxGroup(choices=[], value=[])

    def _reset_field_selection(self):
        """é‡ç½®å­—æ®µé€‰æ‹©"""
        return gr.update(value=[]), pd.DataFrame(columns=["åŸå­—æ®µ", "æ–°å­—æ®µ"])

    def _update_field_mapping(self, selected_fields):
        """æ›´æ–°å­—æ®µæ˜ å°„è¡¨"""
        if not selected_fields:
            return gr.update(value=pd.DataFrame(columns=["åŸå­—æ®µ", "æ–°å­—æ®µ"]), visible=False)
        
        data = [[field, field] for field in selected_fields]
        df = pd.DataFrame(data, columns=["åŸå­—æ®µ", "æ–°å­—æ®µ"])
        return gr.update(value=df, visible=True)

    def _start_field_extract(self, source_file, fields: List[str], output_dir: str) -> str:
        """å¼€å§‹å­—æ®µæå–"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"

            if not fields:
                return "âŒ è¯·é€‰æ‹©è¦æå–çš„å­—æ®µ"

            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"

            # ä½¿ç”¨é€šç”¨å­—æ®µæå–å™¨
            self.logger.info(f"å¼€å§‹å­—æ®µæå–: {fields}")
            result_path = extract_fields_universal(
                source_path=source_path,
                fields=fields,
                output_dir=output_dir or str(self.launcher.root_dir / 'processed')
            )

            if result_path and os.path.exists(result_path):
                file_size = os.path.getsize(result_path)
                return f"âœ… å­—æ®µæå–å®Œæˆï¼\næå–å­—æ®µ: {', '.join(fields)}\nè¾“å‡ºæ–‡ä»¶: {result_path}\næ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚"
            else:
                return "âŒ å­—æ®µæå–å¤±è´¥"
                
        except Exception as e:
            self.logger.error(f'å­—æ®µæå–å¤±è´¥: {e}')
            return f"âŒ å­—æ®µæå–å¤±è´¥: {str(e)}"
    
    def _add_merge_file(self, file_obj, current_data) -> Tuple[None, Any]:
        """æ·»åŠ åˆå¹¶æ–‡ä»¶"""
        try:
            if file_obj is None:
                return None, current_data
            
            file_path = file_obj.name
            
            # å…è®¸æ·»åŠ é‡å¤æ–‡ä»¶ï¼ˆæ”¯æŒä¸åŒç›®å½•åŒåæ–‡ä»¶ï¼Œæˆ–æœ‰æ„é‡å¤åˆå¹¶ï¼‰
            # if file_path in self.merge_file_paths:
            #     return None, current_data
                
            self.merge_file_paths.append(file_path)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_name = os.path.basename(file_path)
            file_size = self._format_size(os.path.getsize(file_path))
            
            # æ›´æ–°DataFrame
            new_row = [file_name, file_path, file_size]
            
            if isinstance(current_data, pd.DataFrame):
                # å¦‚æœæ˜¯DataFrameï¼Œæ·»åŠ æ–°è¡Œ
                new_df = pd.concat([current_data, pd.DataFrame([new_row], columns=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"])], ignore_index=True)
                return None, new_df
            else:
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼ˆåˆå§‹çŠ¶æ€ï¼‰ï¼Œåˆ›å»ºæ–°DataFrame
                if not current_data:
                    return None, pd.DataFrame([new_row], columns=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"])
                else:
                    # å°è¯•è½¬æ¢ç°æœ‰æ•°æ®
                    try:
                        df = pd.DataFrame(current_data, columns=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"])
                        new_df = pd.concat([df, pd.DataFrame([new_row], columns=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"])], ignore_index=True)
                        return None, new_df
                    except:
                        return None, pd.DataFrame([new_row], columns=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"])
                        
        except Exception as e:
            self.logger.error(f'æ·»åŠ åˆå¹¶æ–‡ä»¶å¤±è´¥: {e}')
            return None, current_data

    def _delete_merge_file(self, evt: gr.SelectData, current_data) -> Any:
        """åˆ é™¤é€‰ä¸­çš„åˆå¹¶æ–‡ä»¶"""
        try:
            if not current_data.empty and evt.index[0] < len(current_data):
                # è·å–è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
                row_index = evt.index[0]
                file_path = current_data.iloc[row_index]["è·¯å¾„"]
                
                # ä»åˆ—è¡¨ä¸­ç§»é™¤
                if file_path in self.merge_file_paths:
                    self.merge_file_paths.remove(file_path)
                
                # ä»DataFrameä¸­ç§»é™¤
                new_df = current_data.drop(row_index).reset_index(drop=True)
                return new_df
            return current_data
        except Exception as e:
            self.logger.error(f'åˆ é™¤åˆå¹¶æ–‡ä»¶å¤±è´¥: {e}')
            return current_data

    def _clear_merge_files(self) -> Any:
        """æ¸…ç©ºåˆå¹¶æ–‡ä»¶åˆ—è¡¨"""
        self.merge_file_paths = []
        return pd.DataFrame(columns=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"])

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f}TB"

    def _start_merge(self, output_filename: str, output_dir: str, merge_mode: str = "merge") -> str:
        """å¼€å§‹åˆå¹¶ä»»åŠ¡"""
        try:
            if not self.merge_file_paths:
                return "âŒ è¯·å…ˆæ·»åŠ è¦åˆå¹¶çš„æ–‡ä»¶"
            
            if len(self.merge_file_paths) < 2:
                return "âŒ è‡³å°‘éœ€è¦ä¸¤ä¸ªæ–‡ä»¶æ‰èƒ½åˆå¹¶"
            
            if not output_filename:
                return "âŒ è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å"
            
            # ç”Ÿæˆä»»åŠ¡IDå’Œæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            task_id = f"merge_{int(time.time())}"
            
            # åˆ›å»ºä»»åŠ¡ä¸“å±ç›®å½• (ç±»ä¼¼æ ¼å¼è½¬æ¢)
            # å¦‚æœç”¨æˆ·æŒ‡å®šäº† output_dirï¼Œæˆ‘ä»¬åœ¨å…¶ä¸‹åˆ›å»ºä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•
            # è¿™æ ·å¯ä»¥é¿å…æ–‡ä»¶è¦†ç›–ï¼Œå¹¶ä¿æŒç›®å½•ç»“æ„æ¸…æ™°
            task_dir_name = f"merge-{timestamp}-{task_id[-6:]}"
            task_dir = os.path.join(output_dir, task_dir_name)
            os.makedirs(task_dir, exist_ok=True)
            
            # æ„å»ºè¾“å‡ºè·¯å¾„
            if not output_filename.endswith('.jsonl'):
                output_filename += '.jsonl'
            target_path = os.path.join(task_dir, output_filename)
            
            # æ„é€ å‚æ•°
            params = {
                "task_id": task_id,
                "input_paths": self.merge_file_paths,
                "merge_mode": merge_mode,
                "target_path": target_path,
                "dedup_field": None,
                "dedup_strategy": "first"
            }
            
            # è°ƒç”¨åˆå¹¶
            result_path = self.data_merger.merge_datasets(params)
            
            if result_path:
                return f"âœ… åˆå¹¶æˆåŠŸï¼\nè¾“å‡ºæ–‡ä»¶: {result_path}"
            else:
                return "âŒ åˆå¹¶å¤±è´¥"
                
        except Exception as e:
            self.logger.error(f'åˆå¹¶å¤±è´¥: {e}')
            return f"âŒ åˆå¹¶å¤±è´¥: {str(e)}"
    
    def _start_clean(self, source_file, operations: List[str], empty_fields: str,
                    empty_mode: str,
                    sensitive_words: str, sensitive_action: str, sensitive_replacement: str,
                    sensitive_fields: str, sensitive_exclude_fields: str, sensitive_field_policies: str,
                    sensitive_use_regex: bool, sensitive_case_sensitive: bool,
                    pii_enable: List[str], pii_repl_default: str, pii_repl_map: str,
                    normalize_modes: List[str]) -> str:
        """å¼€å§‹æ•°æ®æ¸…æ´—"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"
            if not operations:
                return "âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ¸…æ´—æ“ä½œ"
            
            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"
            
            params: Dict[str, Any] = {
                'source_path': source_path,
                'operations': operations
            }
            
            # å»ç©ºå­—æ®µä¸ç­–ç•¥
            if 'remove_empty' in operations:
                if empty_fields and empty_fields.strip():
                    params['remove_empty_fields'] = [f.strip() for f in empty_fields.split(',') if f.strip()]
                if empty_mode:
                    params['empty_mode'] = empty_mode

            # æ•æ„Ÿè¯
            if 'filter_sensitive' in operations:
                if sensitive_words and sensitive_words.strip():
                    params['sensitive_words'] = [w.strip() for w in sensitive_words.split(',') if w.strip()]
                params['sensitive_action'] = sensitive_action or 'drop_record'
                if sensitive_action == 'replace_word':
                    params['sensitive_replacement'] = sensitive_replacement or '***'
                if sensitive_fields and sensitive_fields.strip():
                    params['sensitive_fields'] = [f.strip() for f in sensitive_fields.split(',') if f.strip()]
                if sensitive_exclude_fields and sensitive_exclude_fields.strip():
                    params['sensitive_exclude_fields'] = [f.strip() for f in sensitive_exclude_fields.split(',') if f.strip()]
                if sensitive_field_policies and sensitive_field_policies.strip():
                    mapping = {}
                    for seg in sensitive_field_policies.split(','):
                        seg = seg.strip()
                        if not seg:
                            continue
                        parts = seg.split(':')
                        if len(parts) >= 2:
                            field = parts[0].strip()
                            act = parts[1].strip()
                            repl = None
                            if len(parts) >= 3:
                                repl = ':'.join(parts[2:]).strip()
                            if field:
                                mapping[field] = (act, repl)
                    if mapping:
                        params['sensitive_field_policies_parsed'] = mapping
                        params['sensitive_field_policies'] = sensitive_field_policies
                if sensitive_use_regex:
                    params['sensitive_use_regex'] = True
                if sensitive_case_sensitive:
                    params['sensitive_case_sensitive'] = True

            # PII è„±æ•
            if 'pii_desensitize' in operations:
                if pii_enable:
                    params['pii_enable'] = pii_enable
                repl_map: Dict[str, str] = {}
                if pii_repl_default and pii_repl_default.strip():
                    repl_map['default'] = pii_repl_default.strip()
                if pii_repl_map and pii_repl_map.strip():
                    parts = [p.strip() for p in pii_repl_map.split(',') if p.strip()]
                    for p in parts:
                        if ':' in p:
                            k, v = p.split(':', 1)
                            if k.strip() and v.strip():
                                repl_map[k.strip()] = v.strip()
                if repl_map:
                    params['pii_replacements'] = repl_map

            # æ–‡æœ¬æ ‡å‡†åŒ–
            if 'normalize_text' in operations and normalize_modes:
                params['normalize_modes'] = normalize_modes

            # è°ƒç”¨æ¸…æ´—å™¨
            cleaner = self.data_cleaner.DataCleaner()
            task_id = cleaner.start_clean(params)
            
            return f"âœ… æ¸…æ´—ä»»åŠ¡å·²å¯åŠ¨ï¼\nä»»åŠ¡ID: {task_id}\nè¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†è¿›åº¦ã€‚"
                
        except Exception as e:
            self.logger.error(f'æ¸…æ´—å¤±è´¥: {e}')
            return f"âŒ æ¸…æ´—å¤±è´¥: {str(e)}"

    def _preview_sensitive_words(self, text, sensitive_words, action, replacement,
                               sensitive_fields, sensitive_exclude_fields, field_policies,
                               use_regex, case_sensitive) -> str:
        """é¢„è§ˆæ•æ„Ÿè¯æ¸…æ´—æ•ˆæœ"""
        try:
            if not text or not text.strip():
                return "âš ï¸ è¯·è¾“å…¥è¦é¢„è§ˆçš„æ–‡æœ¬"
            
            cleaner = self.data_cleaner.DataCleaner()
            words = [w.strip() for w in (sensitive_words or '').split(',') if w.strip()] or cleaner.default_sensitive_words
            
            data = {'preview': text}
            allowed = [f.strip() for f in sensitive_fields.split(',') if f.strip()] if sensitive_fields else None
            exclude = [f.strip() for f in sensitive_exclude_fields.split(',') if f.strip()] if sensitive_exclude_fields else None
            
            mapping = {}
            if field_policies and field_policies.strip():
                for seg in field_policies.split(','):
                    seg = seg.strip()
                    if not seg:
                        continue
                    parts = seg.split(':')
                    if len(parts) >= 2:
                        fld = parts[0].strip()
                        act = parts[1].strip()
                        repl = None
                        if len(parts) >= 3:
                            repl = ':'.join(parts[2:]).strip()
                        mapping[fld] = (act, repl)
            
            # ç»Ÿè®¡å®¹å™¨æ¨¡æ‹Ÿ
            stats = {'sensitive_detail': {'field_hits': {}, 'word_hits': {}}}
            
            # è°ƒç”¨ DataCleaner çš„å†…éƒ¨æ–¹æ³• _process_sensitive
            # æ³¨æ„ï¼šè¿™é‡Œä¾èµ– DataCleaner çš„å†…éƒ¨å®ç°ï¼Œå¦‚æœ DataCleaner æ¥å£å˜æ›´å¯èƒ½éœ€è¦è°ƒæ•´
            hit, modified, dropped = cleaner._process_sensitive(
                data,
                words,
                action or 'drop_record',
                replacement or '***',
                allowed,
                exclude,
                mapping,
                bool(use_regex),
                bool(case_sensitive),
                stats
            )
            
            if dropped:
                return "ğŸ›‘ ç»“æœ: è®°å½•å°†è¢«ä¸¢å¼ƒ (drop_record è§¦å‘)\n\nåŸæ–‡æœ¬:\n" + text
            
            new_text = data.get('preview', '')
            if not hit:
                return "âœ… æœªå‘½ä¸­ä»»ä½•æ•æ„Ÿè¯\n\nåŸæ–‡æœ¬:\n" + text
            
            detail = stats['sensitive_detail']
            return (
                "ğŸ¯ å‘½ä¸­æ•æ„Ÿè¯é¢„è§ˆ\n" +
                f"åŠ¨ä½œ: {action}\n\n" +
                "åŸæ–‡æœ¬:\n" + text + "\n\n" +
                "å¤„ç†å:\n" + new_text + "\n\n" +
                "å­—æ®µå‘½ä¸­ç»Ÿè®¡:" + json.dumps(detail['field_hits'], ensure_ascii=False) + "\n" +
                "è¯æ¡å‘½ä¸­ç»Ÿè®¡:" + json.dumps(detail['word_hits'], ensure_ascii=False)
            )
            
        except Exception as e:
            return f"é¢„è§ˆå¤±è´¥: {str(e)}"

    def create_tab(self):
        """åˆ›å»ºæ•°æ®åŠ å·¥æ ‡ç­¾é¡µ"""
        gr.Markdown("## æ•°æ®åŠ å·¥ç®¡ç†")
        gr.Markdown("æ”¯æŒæ ¼å¼è½¬æ¢ã€å­—æ®µæå–ã€æ•°æ®åˆå¹¶ã€æ•°æ®æ¸…æ´—ç­‰æ“ä½œ")
        
        # åŠŸèƒ½é€‰æ‹©æ ‡ç­¾
        with gr.Tabs():
            # æ ¼å¼è½¬æ¢å­æ ‡ç­¾
            with gr.TabItem("æ ¼å¼è½¬æ¢"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### è½¬æ¢é…ç½®")
                        
                        convert_source = gr.File(
                            label="æºæ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json", ".xml", ".md", ".markdown"]
                        )
                        
                        convert_target = gr.Dropdown(
                            choices=["jsonl", "csv", "xlsx", "json", "xml", "markdown"],
                            value=self.launcher.config_manager.get_config("ui_state.process.convert_target", "jsonl"),
                            label="ç›®æ ‡æ ¼å¼",
                            info="é€‰æ‹©è½¬æ¢åçš„æ ¼å¼"
                        )
                        convert_target.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.convert_target", x), inputs=[convert_target])
                        
                        convert_output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=self.launcher.config_manager.get_config("ui_state.process.convert_output_dir", str(self.launcher.root_dir / "processed")),
                            info="è½¬æ¢ç»“æœä¿å­˜è·¯å¾„"
                        )
                        convert_output_dir.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.convert_output_dir", x), inputs=[convert_output_dir])
                        
                        convert_btn = gr.Button("å¼€å§‹è½¬æ¢", variant="primary")
                        refresh_convert_btn = gr.Button("åˆ·æ–°ä»»åŠ¡", size="sm")
                    
                    with gr.Column(scale=2):
                        gr.Markdown("### è½¬æ¢ä»»åŠ¡åˆ—è¡¨")
                        
                        convert_task_list = gr.Dataframe(
                            headers=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"],
                            datatype=["str", "str", "str", "str", "str", "str"],
                            label="",
                            interactive=False,
                            wrap=True,
                            elem_classes="convert-task-table"
                        )
                        
                        # å®šæ—¶åˆ·æ–°ä»»åŠ¡åˆ—è¡¨ (æ¯2ç§’åˆ·æ–°ä¸€æ¬¡)
                        refresh_timer = gr.Timer(value=2)
                        refresh_timer.tick(
                            fn=self._get_convert_tasks_df,
                            outputs=[convert_task_list]
                        )
                        
                        with gr.Row():
                            selected_convert_task = gr.Textbox(
                                label="é€‰ä¸­ä»»åŠ¡",
                                placeholder="ç‚¹å‡»ä»»åŠ¡è¡Œé€‰æ‹©",
                                interactive=False,
                                scale=2
                            )
                            
                            view_convert_result_btn = gr.Button("æŸ¥çœ‹ç»“æœ", size="sm", scale=1)
                
                # ä»»åŠ¡è¯¦æƒ…æ˜¾ç¤º
                with gr.Row():
                    convert_detail_status = gr.Textbox(
                        label="ä»»åŠ¡è¯¦æƒ…",
                        lines=5,
                        interactive=False,
                        show_copy_button=True,
                        info="æ˜¾ç¤ºé€‰ä¸­ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯"
                    )
                    
                    convert_status = gr.Textbox(
                        label="è½¬æ¢çŠ¶æ€",
                        lines=5,
                        interactive=False,
                        show_copy_button=True,
                        info="æ˜¾ç¤ºè½¬æ¢ä»»åŠ¡çš„çŠ¶æ€ä¿¡æ¯"
                    )
            
            # å­—æ®µæå–å­æ ‡ç­¾
            with gr.TabItem("å­—æ®µæå–"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æå–é…ç½®")
                        
                        extract_source = gr.File(
                            label="æºæ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json", ".xml", ".md", ".markdown"]
                        )
                        
                        extract_preview_btn = gr.Button("é¢„è§ˆå­—æ®µ", variant="secondary")
                        
                        extract_fields = gr.CheckboxGroup(
                            label="é€‰æ‹©å­—æ®µ",
                            choices=[],
                            value=[],  # ç¡®ä¿åˆå§‹å€¼ä¸ºç©ºåˆ—è¡¨
                            info="é€‰æ‹©è¦æå–çš„å­—æ®µ"
                        )
                        
                        # å­—æ®µé‡å‘½ååŒºåŸŸ
                        gr.Markdown("### å­—æ®µé‡å‘½å")
                        field_mapping_df = gr.Dataframe(
                            headers=["åŸå­—æ®µå", "æ–°å­—æ®µå"],
                            datatype=["str", "str"],
                            row_count=0,
                            col_count=(2, "fixed"),
                            interactive=True,
                            label="å­—æ®µæ˜ å°„è¡¨",
                            visible=False  # åˆå§‹éšè—
                        )
                        
                        extract_output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=self.launcher.config_manager.get_config("ui_state.process.extract_output_dir", str(self.launcher.root_dir / "processed")),
                            info="æå–ç»“æœä¿å­˜è·¯å¾„"
                        )
                        extract_output_dir.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.extract_output_dir", x), inputs=[extract_output_dir])
                        
                        extract_btn = gr.Button("å¼€å§‹æå–", variant="primary")
                    
                    with gr.Column(scale=1):
                        extract_status = gr.Textbox(
                            label="æå–çŠ¶æ€",
                            lines=8,
                            interactive=False,
                            show_copy_button=True
                        )
            
            # æ•°æ®åˆå¹¶å­æ ‡ç­¾
            with gr.TabItem("æ•°æ®åˆå¹¶"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### åˆå¹¶é…ç½®")
                        
                        # æ–‡ä»¶é€‰æ‹©å’Œç®¡ç†
                        gr.Markdown("#### 1. é€‰æ‹©åˆå¹¶æ–‡ä»¶")
                        
                        with gr.Row():
                            with gr.Column(scale=3):
                                merge_file_upload = gr.File(
                                    label="é€‰æ‹©æ–‡ä»¶",
                                    file_types=[".jsonl", ".csv", ".xlsx", ".json", ".md", ".markdown"],
                                    file_count="single"
                                )
                            with gr.Column(scale=1):
                                add_file_btn = gr.Button("æ·»åŠ åˆ°åˆ—è¡¨", variant="primary")
                        
                        gr.Markdown("#### 2. å¾…åˆå¹¶æ–‡ä»¶åˆ—è¡¨")
                        
                        # ä½¿ç”¨Dataframeæ›¿ä»£Textboxï¼Œæ”¯æŒæ›´ç›´è§‚çš„æ˜¾ç¤ºå’Œæ“ä½œ
                        merge_file_list = gr.Dataframe(
                            headers=["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°"],
                            datatype=["str", "str", "str"],
                            label="",
                            interactive=False,
                            wrap=True,
                            value=[]
                        )
                        
                        with gr.Row():
                            delete_file_btn = gr.Button("åˆ é™¤é€‰ä¸­", size="sm", variant="secondary")
                            clear_files_btn = gr.Button("æ¸…ç©ºåˆ—è¡¨", size="sm", variant="stop")
                        
                        selected_merge_index = gr.Number(
                            value=-1,
                            label="é€‰ä¸­ç´¢å¼•",
                            visible=False
                        )
                        
                        # åˆå¹¶é€‰é¡¹
                        gr.Markdown("#### 3. åˆå¹¶é€‰é¡¹")
                        merge_mode = gr.Radio(
                            choices=[("å‡è¡¡æ‰“æ•£åˆå¹¶", "merge"), ("è¿½åŠ åˆå¹¶", "append")],
                            value=self.launcher.config_manager.get_config("ui_state.process.merge_mode", "merge"),
                            label="åˆå¹¶æ¨¡å¼",
                            info="å‡è¡¡æ‰“æ•£åˆå¹¶: æ‰€æœ‰æ–‡ä»¶æ•°æ®æ··åˆæ‰“æ•£ååˆå¹¶; è¿½åŠ åˆå¹¶: æŒ‰æ–‡ä»¶é¡ºåºä¾æ¬¡è¿½åŠ "
                        )
                        merge_mode.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.merge_mode", x), inputs=[merge_mode])
                        
                        merge_output_filename = gr.Textbox(
                            label="è¾“å‡ºæ–‡ä»¶å",
                            value=self.launcher.config_manager.get_config("ui_state.process.merge_output_filename", "merged_dataset.jsonl"),
                            info="åˆå¹¶åçš„æ–‡ä»¶å"
                        )
                        merge_output_filename.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.merge_output_filename", x), inputs=[merge_output_filename])

                        merge_output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=self.launcher.config_manager.get_config("ui_state.process.merge_output_dir", str(self.launcher.root_dir / "processed")),
                            info="åˆå¹¶ç»“æœä¿å­˜è·¯å¾„"
                        )
                        merge_output_dir.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.merge_output_dir", x), inputs=[merge_output_dir])
                        
                        merge_btn = gr.Button("å¼€å§‹åˆå¹¶", variant="primary")
                    
                    with gr.Column(scale=1):
                        merge_status = gr.Textbox(
                            label="åˆå¹¶çŠ¶æ€",
                            lines=10,
                            interactive=False,
                            show_copy_button=True
                        )
            
            # æ•°æ®æ¸…æ´—å­æ ‡ç­¾
            with gr.TabItem("æ•°æ®æ¸…æ´—"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æ¸…æ´—é…ç½®")
                        
                        clean_source = gr.File(
                            label="æºæ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json"]
                        )
                        
                        clean_operations = gr.CheckboxGroup(
                            choices=[
                                ("å»é™¤ç©ºå€¼", "remove_empty"),
                                ("æ•æ„Ÿè¯å¤„ç†", "filter_sensitive"),
                                ("PIIè„±æ•", "pii_desensitize"),
                                ("æ–‡æœ¬æ ‡å‡†åŒ–", "normalize_text")
                            ],
                            value=self.launcher.config_manager.get_config("ui_state.process.clean_operations", []),
                            label="æ¸…æ´—æ“ä½œ",
                            info="é€‰æ‹©è¦æ‰§è¡Œçš„æ¸…æ´—æ“ä½œ (æ”¯æŒå¤šé€‰)"
                        )
                        clean_operations.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.clean_operations", x), inputs=[clean_operations])
                        
                        clean_empty_fields = gr.Textbox(
                            label="å»ç©ºå­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šquestion,answer",
                            info="æŒ‡å®šæ£€æŸ¥ç©ºå€¼çš„å­—æ®µï¼Œé€—å·åˆ†éš”",
                            value=self.launcher.config_manager.get_config("ui_state.process.clean_empty_fields", "")
                        )
                        clean_empty_fields.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.clean_empty_fields", x), inputs=[clean_empty_fields])

                        clean_empty_mode = gr.Radio(
                            choices=["any", "all"],
                            value="any",
                            label="ç©ºå€¼ç­–ç•¥",
                            info="any: ä»»ä¸€å­—æ®µä¸ºç©ºå³ä¸¢å¼ƒ; all: æ‰€æœ‰æŒ‡å®šå­—æ®µéƒ½ä¸ºç©ºæ‰ä¸¢å¼ƒ"
                        )
                        
                        clean_sensitive_words = gr.Textbox(
                            label="æ•æ„Ÿè¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šå¯†ç ,èº«ä»½è¯,æ‰‹æœºå·",
                            info="æŒ‡å®šæ•æ„Ÿè¯ï¼Œé€—å·åˆ†éš”",
                            value=self.launcher.config_manager.get_config("ui_state.process.clean_sensitive_words", "")
                        )
                        clean_sensitive_words.change(lambda x: self.launcher.config_manager.update_config("ui_state.process.clean_sensitive_words", x), inputs=[clean_sensitive_words])

                        clean_sensitive_fields = gr.Textbox(
                            label="æ•æ„Ÿè¯æ‰«æå­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šinstruction,output",
                            info="ä»…å¯¹è¿™äº›å­—æ®µæ‰§è¡Œæ•æ„Ÿè¯å¤„ç†ï¼›ç•™ç©ºåˆ™æ‰«ææ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µ"
                        )

                        clean_sensitive_action = gr.Radio(
                            choices=["drop_record", "remove_word", "replace_word"],
                            value="drop_record",
                            label="æ•æ„Ÿè¯åŠ¨ä½œ",
                            info="drop_record: ä¸¢å¼ƒæ•´æ¡è®°å½•; remove_word: åˆ é™¤è¯æœ¬èº«; replace_word: æ›¿æ¢ä¸ºæŒ‡å®šå†…å®¹"
                        )

                        clean_sensitive_replacement = gr.Textbox(
                            label="æ•æ„Ÿè¯æ›¿æ¢æ–‡æœ¬ï¼ˆå½“é€‰æ‹© replace_word æ—¶ï¼‰",
                            value="***",
                            placeholder="ä¾‹å¦‚ï¼š***",
                            info="ä»…åœ¨æ•æ„Ÿè¯åŠ¨ä½œ=replace_word æ—¶ä½¿ç”¨"
                        )

                        clean_sensitive_exclude_fields = gr.Textbox(
                            label="æ•æ„Ÿè¯æ’é™¤å­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šmeta,source",
                            info="è¿™äº›å­—æ®µå°†å¿½ç•¥æ•æ„Ÿè¯å¤„ç†"
                        )

                        clean_sensitive_field_policies = gr.Textbox(
                            label="å­—æ®µçº§ç­–ç•¥ (å¯é€‰)",
                            placeholder="æ ¼å¼: å­—æ®µ:åŠ¨ä½œ[:æ›¿æ¢]; ä¾‹ instruction:replace_word:@@@,note:remove_word",
                            info="è¦†ç›–å…¨å±€åŠ¨ä½œï¼Œå¯é€‰æ›¿æ¢æ–‡æœ¬ã€‚åŠ¨ä½œ=drop_record/remove_word/replace_word"
                        )

                        clean_sensitive_use_regex = gr.Checkbox(
                            label="æ•æ„Ÿè¯ä½¿ç”¨æ­£åˆ™æ¨¡å¼",
                            value=False
                        )

                        clean_sensitive_case_sensitive = gr.Checkbox(
                            label="å¤§å°å†™æ•æ„ŸåŒ¹é…",
                            value=False
                        )

                        with gr.Accordion("æ•æ„Ÿè¯è§„åˆ™è¯´æ˜", open=False):
                            gr.Markdown(
                                """
**åŒ¹é…æ¨¡å¼è¯´æ˜**

1. æ™®é€šæ¨¡å¼ï¼šæŒ‰æä¾›çš„è¯æ¡é€ä¸€ç²¾ç¡®å­ä¸²åŒ¹é…ï¼ˆé»˜è®¤å¿½ç•¥å¤§å°å†™ï¼Œé™¤éå‹¾é€‰å¤§å°å†™æ•æ„Ÿï¼‰ã€‚
2. æ­£åˆ™æ¨¡å¼ï¼šå‹¾é€‰â€œæ•æ„Ÿè¯ä½¿ç”¨æ­£åˆ™æ¨¡å¼â€åï¼Œåˆ—è¡¨ä¸­æ¯ä¸€é¡¹è§†ä¸ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œæ”¯æŒåˆ†ç»„ä¸é‡è¯ã€‚
3. å­—æ®µç™½åå• / æ’é™¤ï¼š
   - â€œæ•æ„Ÿè¯æ‰«æå­—æ®µâ€å¡«å†™åï¼Œä»…è¿™äº›å­—æ®µä¼šè¢«æ£€æµ‹ã€‚
   - â€œæ•æ„Ÿè¯æ’é™¤å­—æ®µâ€ä¼˜å…ˆç”Ÿæ•ˆï¼Œå¯æ’é™¤éƒ¨åˆ†å­—æ®µã€‚
4. å­—æ®µçº§ç­–ç•¥ä¼˜å…ˆçº§ï¼šå­—æ®µç­–ç•¥ > å…¨å±€åŠ¨ä½œã€‚æ ¼å¼: `å­—æ®µ:åŠ¨ä½œ[:æ›¿æ¢]`ï¼›åŠ¨ä½œæ”¯æŒ `drop_record|remove_word|replace_word`ã€‚
5. ç»Ÿè®¡ä¿¡æ¯ï¼šæ¸…æ´—å®ŒæˆåæŠ¥å‘Šä¸­ `sensitive_detail.field_hits` è®°å½•å„å­—æ®µå‘½ä¸­æ¬¡æ•°ï¼Œ`word_hits` è®°å½•å„è¯å‘½ä¸­æ¬¡æ•°ã€‚`unused_parameters` å¯å¸®åŠ©ç¡®è®¤æœªç”Ÿæ•ˆçš„å¤šä½™å‚æ•°ã€‚
6. Drop Record æå‰ï¼šä¸€æ—¦æŸå­—æ®µç­–ç•¥æˆ–å…¨å±€åŠ¨ä½œè§¦å‘ `drop_record` ä¸”åŒ¹é…å‘½ä¸­ï¼Œè¯¥è®°å½•ç«‹å³ä¸¢å¼ƒï¼Œä¸å†ç»§ç»­æ›¿æ¢å…¶å®ƒå­—æ®µã€‚

**ç¤ºä¾‹**
```
æ•æ„Ÿè¯åˆ—è¡¨: å¯†é’¥,å¯†ç 
å­—æ®µçº§ç­–ç•¥: instruction:remove_word,note:replace_word:[SENSITIVE]
```
è¡¨ç¤º instruction åˆ é™¤è¯æœ¬èº«ï¼Œnote ç”¨ [SENSITIVE] æ›¿æ¢ã€‚
                                """
                            )

                        gr.Markdown("### æ•æ„Ÿè¯è¯•è¿è¡Œ (ä¸è½åœ°æ–‡ä»¶)")
                        sensitive_preview_text = gr.Textbox(
                            label="é¢„è§ˆè¾“å…¥æ–‡æœ¬",
                            placeholder="åœ¨è¿™é‡Œç²˜è´´ä¸€æ®µæ–‡æœ¬ï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æŸ¥çœ‹å¤„ç†æ•ˆæœ",
                            lines=3
                        )
                        sensitive_preview_btn = gr.Button("è¯•è¿è¡Œé¢„è§ˆ", size="sm")
                        sensitive_preview_result = gr.Textbox(
                            label="é¢„è§ˆç»“æœ",
                            lines=5,
                            interactive=False,
                            show_copy_button=True
                        )

                        pii_enable = gr.CheckboxGroup(
                            choices=["id_card", "phone", "email", "bank_card", "ip", "passport"],
                            label="å¯ç”¨çš„ PII ç±»å‹",
                            info="é€‰æ‹©éœ€è¦åŒ¹é…å¹¶è„±æ•çš„ä¸ªäººä¿¡æ¯ç±»å‹"
                        )

                        pii_repl_default = gr.Textbox(
                            label="PII é»˜è®¤æ›¿æ¢æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼š<PII>",
                            info="æœªä¸ºåˆ†ç±»å•ç‹¬æŒ‡å®šæ—¶ä½¿ç”¨"
                        )

                        pii_repl_map = gr.Textbox(
                            label="PII åˆ†ç±»æ›¿æ¢ (å¯é€‰)",
                            placeholder="æ ¼å¼: ç±»å‹:æ›¿æ¢æ–‡æœ¬, ä¾‹å¦‚ id_card:<ID>,phone:<TEL>",
                            info="æŒ‰é€—å·åˆ†éš”çš„ é”®:å€¼ åˆ—è¡¨"
                        )

                        normalize_modes = gr.CheckboxGroup(
                            choices=["unicode_nfc", "fullwidth", "lowercase", "collapse_newlines"],
                            label="æ–‡æœ¬æ ‡å‡†åŒ–æ¨¡å¼",
                            info="å¤šé€‰ç»„åˆï¼Œå¯¹æ–‡æœ¬åšç»Ÿä¸€å¤„ç†ï¼ˆç‚¹å‡»ä¸‹æ–¹è¯´æ˜è·å–è¯¦ç»†å·®å¼‚ï¼‰"
                        )

                        with gr.Accordion("æ–‡æœ¬æ ‡å‡†åŒ–è¯´æ˜", open=False):
                            gr.Markdown(
                                """
**å„é€‰é¡¹å«ä¹‰ä¸åœºæ™¯**

1. `unicode_nfc` ç»Ÿä¸€ç­‰ä»·å­—ç¬¦çš„å†…éƒ¨è¡¨ç¤ºï¼ˆNFC è§„èŒƒåŒ–ï¼‰ã€‚
    - è§£å†³ï¼šåŒæ ·æ˜¾ç¤ºçš„å­—ç¬¦å› ä¸ºåˆ†è§£/ç»„åˆå½¢å¼ä¸åŒå¯¼è‡´åŒ¹é…/å»é‡å¤±è´¥ã€‚
    - ä¾‹ï¼š`e + Ì` -> `Ã©`ã€‚

2. `fullwidth` å…¨è§’è½¬åŠè§’ï¼ˆåªä½œç”¨äºå­—æ¯ / æ•°å­— / å¸¸è§è‹±æ–‡æ ‡ç‚¹ï¼‰ã€‚
    - è§£å†³ï¼šè¾“å…¥æ³•å…¨è§’æ¨¡å¼ / ç½‘é¡µå¤åˆ¶å¯¼è‡´çš„ï¼¡ï¼¢ï¼£ï¼‘ï¼’ï¼“ï¼Œé¿å…åŒ¹é…å¤±è´¥ã€‚
    - ä¾‹ï¼š`ï¼¡ï¼¢ï¼£ï¼‘ï¼’ï¼“ï¼Œï¼ï¼` -> `ABC123,./`ã€‚

3. `lowercase` æ‰€æœ‰å­—æ¯è½¬å°å†™ã€‚
    - é€‚åˆï¼šåç»­åŒ¹é…/å»é‡ä¸å…³å¿ƒå¤§å°å†™ï¼ˆå¦‚è‹±æ–‡æ™®é€šæè¿°ã€æ ‡ç­¾ï¼‰ã€‚
    - ä¸æ¨èï¼šåŒºåˆ†å¤§å°å†™æœ‰æ„ä¹‰ï¼ˆä¸“æœ‰åè¯ã€ä»£ç ç‰‡æ®µã€å˜é‡ã€æƒ…æ„Ÿå¼ºè°ƒï¼‰ã€‚

4. `collapse_newlines` æŠ˜å å¤šä½™ç©ºè¡Œï¼Œé¿å…å¤§æ®µç©ºç™½ã€‚
    - å¤„ç†ï¼šå°†è¿ç»­çš„ç©ºè¡Œæ”¶ç¼©ä¸º 1 è¡Œï¼Œå¹¶æ¸…ç†å¤šä½™ç©ºç™½ï¼›å¯å‡å°‘ token / å™ªå£°ã€‚
    - ä¿ç•™ï¼šæ­£å¸¸æ®µè½çš„å•ä¸ªæ¢è¡Œã€‚

**æ‰§è¡Œé¡ºåºï¼ˆå½“å‰å®ç°ï¼‰** å…ˆåšç©ºç™½æŠ˜å ï¼Œå†æŒ‰æ‰€é€‰æ¨¡å¼åº”ç”¨ï¼ˆNFC â†’ å…¨è§’ â†’ å°å†™ â†’ ç©ºè¡ŒæŠ˜å ï¼‰ã€‚
å¦‚éœ€æ›´ç²¾ç»†é¡ºåºæˆ–å¢åŠ â€œä¿ç•™å¤§å°å†™é‡è¦å­—æ®µâ€ç™½åå•ï¼Œå¯åç»­æ‰©å±•ã€‚
"""
                            )
                        
                        clean_btn = gr.Button("å¼€å§‹æ¸…æ´—", variant="primary")
                    
                    with gr.Column(scale=1):
                        clean_status = gr.Textbox(
                            label="æ¸…æ´—çŠ¶æ€",
                            lines=8,
                            interactive=False,
                            show_copy_button=True
                        )
        
        # è‡ªåŠ¨åˆ·æ–°å®šæ—¶å™¨
        auto_refresh_timer = gr.Timer(value=2)
        auto_refresh_timer.tick(
            fn=self._get_convert_tasks_df,
            outputs=[convert_task_list]
        )
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.launcher.components['process'] = {
            'auto_refresh_timer': auto_refresh_timer,
            'convert_source': convert_source,
            'convert_target': convert_target,
            'convert_output_dir': convert_output_dir,
            'convert_status': convert_status,
            'convert_task_list': convert_task_list,
            'selected_convert_task': selected_convert_task,
            'convert_detail_status': convert_detail_status,
            'extract_source': extract_source,
            'extract_fields': extract_fields,
            'extract_output_dir': extract_output_dir,
            'extract_status': extract_status,
            'merge_file_upload': merge_file_upload,
            'add_file_btn': add_file_btn,
            'clear_files_btn': clear_files_btn,
            'merge_file_list': merge_file_list,
            'merge_mode': merge_mode,
            'merge_output_dir': merge_output_dir,
            'merge_status': merge_status,
            'clean_source': clean_source,
            'clean_operations': clean_operations,
            'clean_empty_fields': clean_empty_fields,
            'clean_empty_mode': clean_empty_mode,
            'clean_sensitive_words': clean_sensitive_words,
            'clean_sensitive_fields': clean_sensitive_fields,
            'clean_sensitive_action': clean_sensitive_action,
            'clean_sensitive_replacement': clean_sensitive_replacement,
            'clean_sensitive_exclude_fields': clean_sensitive_exclude_fields,
            'clean_sensitive_field_policies': clean_sensitive_field_policies,
            'clean_sensitive_use_regex': clean_sensitive_use_regex,
            'clean_sensitive_case_sensitive': clean_sensitive_case_sensitive,
            'sensitive_preview_text': sensitive_preview_text,
            'sensitive_preview_btn': sensitive_preview_btn,
            'sensitive_preview_result': sensitive_preview_result,
            'pii_enable': pii_enable,
            'pii_repl_default': pii_repl_default,
            'pii_repl_map': pii_repl_map,
            'normalize_modes': normalize_modes,
            'clean_status': clean_status
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        convert_btn.click(
            fn=self._start_format_convert,
            inputs=[convert_source, convert_target, convert_output_dir],
            outputs=[convert_status]
        )
        
        # æ–°å¢ï¼šå¼‚æ­¥ä»»åŠ¡ç®¡ç†äº‹ä»¶
        refresh_convert_btn.click(
            fn=self._get_convert_tasks_df,
            outputs=[convert_task_list]
        )
        
        convert_task_list.select(
            fn=self._select_convert_task,
            outputs=[selected_convert_task]
        )
        
        view_convert_result_btn.click(
            fn=self._view_convert_result,
            inputs=[selected_convert_task],
            outputs=[convert_detail_status]
        )
        
        extract_preview_btn.click(
            fn=self._preview_extract_fields,
            inputs=[extract_source],
            outputs=[extract_fields]
        )
        
        # å½“æ–‡ä»¶ä¸Šä¼ æ—¶è‡ªåŠ¨é‡ç½®å­—æ®µé€‰æ‹©
        extract_source.change(
            fn=self._reset_field_selection,
            inputs=[],
            outputs=[extract_fields, field_mapping_df]
        )
        
        # å­—æ®µé€‰æ‹©å˜åŒ–æ—¶æ›´æ–°æ˜ å°„è¡¨
        extract_fields.change(
            fn=self._update_field_mapping,
            inputs=[extract_fields],
            outputs=[field_mapping_df]
        )
        
        extract_btn.click(
            fn=self._start_field_extract_with_progress,
            inputs=[extract_source, extract_fields, field_mapping_df, extract_output_dir],
            outputs=[extract_status]
        )
        
        # æ–‡ä»¶ç®¡ç†äº‹ä»¶ç»‘å®š
        add_file_btn.click(
            fn=self._add_merge_file,
            inputs=[merge_file_upload, merge_file_list],
            outputs=[merge_file_upload, merge_file_list]
        )
        
        clear_files_btn.click(
            fn=self._clear_merge_files,
            inputs=[],
            outputs=[merge_file_list]
        )
        
        # åˆ é™¤é€‰ä¸­æ–‡ä»¶äº‹ä»¶
        # æ”¹ä¸ºä½¿ç”¨ç´¢å¼•åˆ é™¤ï¼Œä»¥æ”¯æŒé‡å¤æ–‡ä»¶åçš„æ­£ç¡®åˆ é™¤
        
        def on_select_merge_file(evt: gr.SelectData):
            return evt.index[0]

        merge_file_list.select(
            fn=on_select_merge_file,
            inputs=[],
            outputs=[selected_merge_index]
        )
        
        def delete_selected_file(selected_idx, df):
            if selected_idx is None or selected_idx < 0:
                return df
            
            idx = int(selected_idx)
            if idx >= len(df):
                return df

            # ä»åˆ—è¡¨ä¸­ç§»é™¤ (æŒ‰ç´¢å¼•)
            # æ³¨æ„ï¼šself.merge_file_paths å’Œ df å¿…é¡»ä¿æŒåŒæ­¥
            if 0 <= idx < len(self.merge_file_paths):
                self.merge_file_paths.pop(idx)
            
            # ä»DataFrameä¸­ç§»é™¤
            new_df = df.drop(idx).reset_index(drop=True)
            return new_df

        delete_file_btn.click(
            fn=delete_selected_file,
            inputs=[selected_merge_index, merge_file_list],
            outputs=[merge_file_list]
        )
        
        merge_btn.click(
            fn=self._start_merge,
            inputs=[merge_output_filename, merge_output_dir, merge_mode],
            outputs=[merge_status]
        )

        # æ¸…æ´—ä»»åŠ¡å¯åŠ¨æŒ‰é’®äº‹ä»¶ç»‘å®š
        clean_btn.click(
            fn=self._start_clean,
            inputs=[clean_source, clean_operations, clean_empty_fields,
                clean_empty_mode,
                clean_sensitive_words, clean_sensitive_action, clean_sensitive_replacement,
                clean_sensitive_fields, clean_sensitive_exclude_fields, clean_sensitive_field_policies,
                clean_sensitive_use_regex, clean_sensitive_case_sensitive,
                pii_enable, pii_repl_default, pii_repl_map,
                normalize_modes],
            outputs=[clean_status]
        )

        # æ•æ„Ÿè¯è¯•è¿è¡Œç»‘å®š
        sensitive_preview_btn.click(
            fn=self._preview_sensitive_words,
            inputs=[sensitive_preview_text, clean_sensitive_words, clean_sensitive_action, clean_sensitive_replacement,
                    clean_sensitive_fields, clean_sensitive_exclude_fields, clean_sensitive_field_policies,
                    clean_sensitive_use_regex, clean_sensitive_case_sensitive],
            outputs=[sensitive_preview_result]
        )

    def _start_field_extract_with_progress(self, source_file, fields, field_mapping_df, output_dir: str):
        """å¼€å§‹å­—æ®µæå–ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
        try:
            if source_file is None:
                yield "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"
                return

            if not fields:
                yield "âŒ è¯·é€‰æ‹©è¦æå–çš„å­—æ®µ"
                return

            source_path = source_file.name
            if not os.path.exists(source_path):
                yield "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"
                return

            # è¿›åº¦ä¿¡æ¯æ”¶é›†
            progress_log = []
            
            def progress_callback(message, percent):
                """è¿›åº¦å›è°ƒå‡½æ•°"""
                timestamp = time.strftime("%H:%M:%S")
                progress_info = f"[{timestamp}] {percent:3.0f}% - {message}"
                progress_log.append(progress_info)
                self.logger.info(progress_info)

            try:
                # å¼€å§‹æå–å‰çš„å‡†å¤‡å·¥ä½œ
                progress_callback("ğŸš€ å¼€å§‹åˆå§‹åŒ–å­—æ®µæå–ä»»åŠ¡...", 0)
                yield "\n".join(progress_log)
                
                # å¤„ç†å­—æ®µæ˜ å°„
                field_mapping = {}
                mapping_data = []
                
                if field_mapping_df is not None:
                    try:
                        if hasattr(field_mapping_df, 'values'):
                            mapping_data = field_mapping_df.values.tolist()
                        elif isinstance(field_mapping_df, list):
                            mapping_data = field_mapping_df
                        elif hasattr(field_mapping_df, '__iter__'):
                            mapping_data = list(field_mapping_df)
                        
                        progress_callback("âœ… å­—æ®µæ˜ å°„æ•°æ®å¤„ç†å®Œæˆ", 5)
                        yield "\n".join(progress_log)
                        
                    except Exception as e:
                        progress_callback(f"âš ï¸ æ˜ å°„æ•°æ®å¤„ç†å¼‚å¸¸: {e}", 5)
                        mapping_data = []
                        yield "\n".join(progress_log)
                
                # è§£ææ˜ å°„æ•°æ®
                if mapping_data:
                    for row in mapping_data:
                        try:
                            if isinstance(row, (list, tuple)) and len(row) >= 2:
                                original_field = str(row[0]).strip() if row[0] else ""
                                new_field = str(row[1]).strip() if row[1] else ""
                                
                                if original_field and new_field:
                                    field_mapping[original_field] = new_field
                        except Exception as e:
                            continue
                    
                    if field_mapping:
                        progress_callback(f"ğŸ·ï¸ å­—æ®µé‡å‘½åæ˜ å°„å·²è®¾ç½®: {len(field_mapping)} ä¸ªå­—æ®µ", 8)
                        yield "\n".join(progress_log)

                # ä½¿ç”¨é€šç”¨å­—æ®µæå–å™¨ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
                progress_callback("ğŸ”„ å¯åŠ¨å­—æ®µæå–å¼•æ“...", 10)
                yield "\n".join(progress_log)
                
                # æ–¹æ¡ˆï¼šä½¿ç”¨çº¿ç¨‹è¿è¡Œæå–ä»»åŠ¡ï¼Œä¸»çº¿ç¨‹å¾ªç¯yieldè¿›åº¦
                import threading
                import queue
                
                msg_queue = queue.Queue()
                result_queue = queue.Queue()
                
                def thread_callback(message, percent):
                    timestamp = time.strftime("%H:%M:%S")
                    progress_info = f"[{timestamp}] {percent:3.0f}% - {message}"
                    msg_queue.put(progress_info)
                
                def run_extract():
                    try:
                        res = extract_fields_universal(
                            source_path=source_path,
                            fields=fields,
                            output_dir=output_dir or str(self.launcher.root_dir / 'processed'),
                            field_mapping=field_mapping,
                            progress_callback=thread_callback
                        )
                        result_queue.put(('success', res))
                    except Exception as e:
                        result_queue.put(('error', str(e)))
                
                t = threading.Thread(target=run_extract)
                t.start()
                
                # å¾ªç¯ç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»“æŸ
                while t.is_alive():
                    # è·å–æ‰€æœ‰æ–°æ¶ˆæ¯
                    while not msg_queue.empty():
                        msg = msg_queue.get()
                        progress_log.append(msg)
                    
                    yield "\n".join(progress_log)
                    time.sleep(0.1)
                
                # å¤„ç†å‰©ä½™æ¶ˆæ¯
                while not msg_queue.empty():
                    msg = msg_queue.get()
                    progress_log.append(msg)
                yield "\n".join(progress_log)
                
                # è·å–ç»“æœ
                status, result = result_queue.get()

                if status == 'success':
                    result_path = result
                    if result_path and os.path.exists(result_path):
                        file_size = os.path.getsize(result_path)
                        
                        # æ„å»ºè¯¦ç»†çš„ç»“æœæŠ¥å‘Š
                        mapping_info = ""
                        if field_mapping:
                            mapping_list = [f"{k} â†’ {v}" for k, v in field_mapping.items()]
                            mapping_info = f"\nğŸ“‹ å­—æ®µæ˜ å°„: {', '.join(mapping_list)}"
                        
                        # åˆå¹¶è¿›åº¦æ—¥å¿—
                        progress_summary = "\n".join(progress_log)
                        
                        final_result = f"""âœ… å­—æ®µæå–ä»»åŠ¡å®Œæˆï¼

ğŸ“Š æå–è¯¦æƒ…:
â€¢ é€‰æ‹©å­—æ®µ: {', '.join(fields)}{mapping_info}
â€¢ è¾“å‡ºæ–‡ä»¶: {result_path}  
â€¢ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚

ğŸ“ˆ æ‰§è¡Œæ—¥å¿—:
{progress_summary}

ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼"""
                        
                        self.logger.info(f"å­—æ®µæå–å®Œæˆ: {result_path}")
                        yield final_result
                    else:
                        error_summary = "\n".join(progress_log)
                        yield f"""âŒ å­—æ®µæå–å¤±è´¥

æ‰§è¡Œæ—¥å¿—:
{error_summary}

è¯·æ£€æŸ¥æºæ–‡ä»¶æ ¼å¼å’Œé€‰æ‹©çš„å­—æ®µ"""
                else:
                    error_summary = "\n".join(progress_log)
                    yield f"âŒ å­—æ®µæå–å¼‚å¸¸: {result}\n\næ‰§è¡Œæ—¥å¿—:\n{error_summary}"

            except Exception as e:
                self.logger.error(f"å­—æ®µæå–è¿‡ç¨‹å¼‚å¸¸: {e}")
                error_summary = "\n".join(progress_log)
                yield f"âŒ å­—æ®µæå–å¼‚å¸¸: {str(e)}\n\næ‰§è¡Œæ—¥å¿—:\n{error_summary}"
                
        except Exception as e:
            self.logger.error(f'å­—æ®µæå–å¤±è´¥: {e}')
            yield f"âŒ å­—æ®µæå–å¤±è´¥: {str(e)}"

def create_process_tab(launcher):
    manager = ProcessTabManager(launcher)
    manager.create_tab()
    return manager
