import os
import json
import time
import threading
from datetime import datetime
from pathlib import Path
import gradio as gr
from typing import Dict, Any, Tuple, List, Optional
from ..dependencies import pd
from ..data_manager import data_manager
from ..dataset_previewer import DatasetPreviewer
from ..universal_field_extractor import get_field_names_universal, extract_fields_universal

class ManageTabManager:
    def __init__(self, launcher):
        self.launcher = launcher
        self.logger = launcher.logger
        self.root_dir = launcher.root_dir
        self.dataset_previewer = DatasetPreviewer()
        
        # ç¼“å­˜å½“å‰æ•°æ®é›†çš„æ•°æ®å’Œå­—æ®µä¿¡æ¯ï¼Œé¿å…é‡å¤è¯»å–
        self.current_dataset_cache = {
            'path': None,
            'data': None,
            'fields': [],
            'original_preview': None
        }
        
    def create_tab(self):
        """åˆ›å»ºæ•°æ®ç®¡ç†æ ‡ç­¾é¡µ"""
        gr.Markdown("## æ•°æ®ç®¡ç†ä¸­å¿ƒ")
        gr.Markdown("ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®é›†ï¼Œæ”¯æŒæŒ‰ç±»å‹æŸ¥çœ‹å’Œé¢„è§ˆæ“ä½œ")
        
        # ç­›é€‰å’Œæœç´¢åŒºåŸŸ
        with gr.Row():
            data_type_filter = gr.Dropdown(
                choices=["å…¨éƒ¨", "åŸå§‹æ•°æ®", "å¤„ç†æ•°æ®", "è’¸é¦æ•°æ®"],
                value="å…¨éƒ¨",
                label="æ•°æ®ç±»å‹ç­›é€‰",
                info="æŒ‰æ•°æ®ç±»å‹ç­›é€‰æ˜¾ç¤º",
                scale=2
            )
            
            dataset_name_search = gr.Textbox(
                label="æ•°æ®é›†åç§°æœç´¢",
                placeholder="è¾“å…¥æ•°æ®é›†åç§°å…³é”®è¯",
                info="æ¨¡ç³Šæœç´¢æ•°æ®é›†åç§°",
                scale=2
            )
            
            search_dataset_btn = gr.Button("æœç´¢", size="sm", scale=1)
        
        # æ•°æ®é›†åˆ—è¡¨åŒºåŸŸ
        gr.Markdown("### æ•°æ®é›†åˆ—è¡¨")
        gr.Markdown("ğŸ’¡ **æ“ä½œæç¤º**ï¼šç‚¹å‡»è¡¨æ ¼è¡Œé€‰æ‹©æ•°æ®é›† â€¢ è¡¨æ ¼æ”¯æŒæ°´å¹³æ»šåŠ¨æŸ¥çœ‹å®Œæ•´å†…å®¹ â€¢ æ‚¬åœå•å…ƒæ ¼æ˜¾ç¤ºå®Œæ•´æ–‡æœ¬")
        gr.Markdown("âš ï¸ **åˆ é™¤è­¦å‘Š**ï¼šåˆ é™¤æ•°æ®ä¼šæ°¸ä¹…åˆ é™¤æ•°æ®é›†æ–‡ä»¶æˆ–æ•´ä¸ªæ•°æ®é›†ç›®å½•ï¼Œè¯·è°¨æ…æ“ä½œï¼")
                
        dataset_list = gr.Dataframe(
            headers=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"],
            datatype=["str", "str", "str", "str", "str"],
            label="",
            interactive=False,
            wrap=True,
            elem_classes="dataset-list-table"
        )
        
        # æ“ä½œæŒ‰é’®åŒºåŸŸ - æ”¾åœ¨åˆ—è¡¨ä¸‹æ–¹
        with gr.Row():
            refresh_data_btn = gr.Button("åˆ·æ–°åˆ—è¡¨", size="sm")
            preview_data_btn = gr.Button("é¢„è§ˆæ•°æ®", size="sm")
            delete_data_btn = gr.Button("å±é™©åˆ é™¤", size="sm", variant="stop")
        
        selected_dataset = gr.Textbox(
            label="é€‰ä¸­æ•°æ®é›†",
            placeholder="ç‚¹å‡»æ•°æ®é›†è¡Œé€‰æ‹©",
            interactive=False
        )
        
        # é¡µé¢åŠ è½½æ—¶è‡ªåŠ¨åˆ·æ–°åˆ—è¡¨
        # ä½¿ç”¨ Timer è§¦å‘ä¸€æ¬¡ï¼Œå¹¶åœ¨å›è°ƒä¸­å…³é—­ Timer
        init_refresh_timer = gr.Timer(value=0.5)
        
        def _init_refresh():
            return self._refresh_datasets("å…¨éƒ¨", ""), gr.Timer(active=False)
            
        init_refresh_timer.tick(
            fn=_init_refresh,
            inputs=[],
            outputs=[dataset_list, init_refresh_timer]
        )
        
        # æ•°æ®é¢„è§ˆåŒºåŸŸ - æ”¹ä¸ºä¸‹æ–¹å®Œæ•´åŒºåŸŸ
        gr.Markdown("### æ•°æ®é¢„è§ˆ")
        
        with gr.Row():
            with gr.Column(scale=1):
                # é¢„è§ˆæ§åˆ¶é€‰é¡¹
                preview_rows = gr.Slider(
                    minimum=10,
                    maximum=1000,
                    value=100,
                    step=10,
                    label="é¢„è§ˆè¡Œæ•°",
                    info="è®¾ç½®è¦é¢„è§ˆçš„æ•°æ®è¡Œæ•°"
                )
                
                auto_preview = gr.Checkbox(
                    label="è‡ªåŠ¨é¢„è§ˆ",
                    value=True,
                    info="é€‰æ‹©æ•°æ®é›†æ—¶è‡ªåŠ¨é¢„è§ˆ"
                )
                
                text_truncation = gr.Checkbox(
                    label="æ–‡æœ¬æˆªæ–­",
                    value=True,
                    info="å¯¹é•¿æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æˆªæ–­"
                )
                
                max_text_length = gr.Slider(
                    minimum=50,
                    maximum=1000,
                    value=300,
                    step=50,
                    label="æ–‡æœ¬æˆªæ–­é•¿åº¦",
                    info="è¶…è¿‡æ­¤é•¿åº¦çš„æ–‡æœ¬å°†è¢«æˆªæ–­"
                )
            
            with gr.Column(scale=2):
                # å­—æ®µé€‰æ‹©åŒºåŸŸ
                gr.Markdown("#### å­—æ®µé€‰æ‹©")
                gr.Markdown("ğŸ’¡ é€‰æ‹©è¦æ˜¾ç¤ºçš„å­—æ®µï¼Œå®æ—¶æ›´æ–°é¢„è§ˆè¡¨æ ¼")
                
                with gr.Row():
                    select_all_fields = gr.Button("å…¨é€‰", size="sm", variant="secondary")
                    clear_all_fields = gr.Button("æ¸…é™¤", size="sm", variant="secondary")
                    select_common_fields = gr.Button("å¸¸ç”¨å­—æ®µ", size="sm", variant="primary")
                
                # å­—æ®µé€‰æ‹©æ¡† - è¿™é‡Œæ˜¯æ ¸å¿ƒç»„ä»¶
                field_selector = gr.CheckboxGroup(
                    label="å¯ç”¨å­—æ®µ",
                    choices=[],
                    value=[],
                    interactive=True,
                    info="é€‰æ‹©è¦åœ¨é¢„è§ˆä¸­æ˜¾ç¤ºçš„å­—æ®µ"
                )
        
        # æ•°æ®é›†ä¿¡æ¯æ˜¾ç¤º
        with gr.Row():
            dataset_info = gr.Markdown("*é€‰æ‹©æ•°æ®é›†ä»¥æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯*")
        
        # é¢„è§ˆç»“æœæ˜¾ç¤º - ä½¿ç”¨HTMLç»„ä»¶æ›¿ä»£Dataframeä»¥è·å¾—æ›´å¥½çš„æ§åˆ¶
        result_display_html = gr.HTML(
            label="æ•°æ®å†…å®¹é¢„è§ˆ",
            value="<div style='padding: 20px; text-align: center; color: #666;'>è¯·é€‰æ‹©æ•°æ®é›†è¿›è¡Œé¢„è§ˆ</div>",
            elem_classes="preview-table-container"
        )
        
        # éšè—çš„åŸDataframeç»„ä»¶ï¼Œç”¨äºä¿æŒæ¥å£å…¼å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        result_display = gr.Dataframe(visible=False)
        
        # å±•å¼€/æŠ˜å æ–‡æœ¬æ§åˆ¶
        with gr.Row():
            with gr.Column():
                expanded_text = gr.Textbox(
                    label="å®Œæ•´æ–‡æœ¬å†…å®¹",
                    lines=10,
                    interactive=False,
                    visible=False,
                    info="ç‚¹å‡»é¢„è§ˆè¡¨æ ¼ä¸­çš„æˆªæ–­æ–‡æœ¬å¯åœ¨æ­¤å¤„æŸ¥çœ‹å®Œæ•´å†…å®¹"
                )
                expand_text_btn = gr.Button("æŸ¥çœ‹å®Œæ•´æ–‡æœ¬", visible=False, size="sm")
        
        # æ“ä½œçŠ¶æ€æ˜¾ç¤ºåŒºåŸŸ - ç§»åˆ°æœ€ä¸‹é¢
        with gr.Row():
            data_status = gr.Textbox(
                label="æ“ä½œçŠ¶æ€",
                lines=3,
                interactive=False,
                show_copy_button=True,
                info="æ˜¾ç¤ºæ•°æ®æ“ä½œçš„çŠ¶æ€ä¿¡æ¯"
            )
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.launcher.components['manage'] = {
            'data_type_filter': data_type_filter,
            'dataset_name_search': dataset_name_search,
            'search_dataset_btn': search_dataset_btn,
            'dataset_list': dataset_list,
            'selected_dataset': selected_dataset,
            'preview_rows': preview_rows,
            'auto_preview': auto_preview,
            'text_truncation': text_truncation,
            'max_text_length': max_text_length,
            'field_selector': field_selector,
            'select_all_fields': select_all_fields,
            'clear_all_fields': clear_all_fields,
            'select_common_fields': select_common_fields,
            'dataset_info': dataset_info,
            'result_display': result_display,
            'result_display_html': result_display_html,
            'data_status': data_status,
            'expanded_text': expanded_text,
            'expand_text_btn': expand_text_btn
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        data_type_filter.change(
            fn=self._filter_datasets,
            inputs=[data_type_filter, dataset_name_search],
            outputs=[dataset_list]
        )
        
        search_dataset_btn.click(
            fn=self._filter_datasets,
            inputs=[data_type_filter, dataset_name_search],
            outputs=[dataset_list]
        )
        
        refresh_data_btn.click(
            fn=self._refresh_datasets,
            inputs=[data_type_filter, dataset_name_search],
            outputs=[dataset_list]
        )
        
        preview_data_btn.click(
            fn=self._preview_dataset_with_field_filter,
            inputs=[selected_dataset, preview_rows, text_truncation, max_text_length, field_selector],
            outputs=[result_display_html, data_status]
        )
        
        delete_data_btn.click(
            fn=self._delete_dataset,
            inputs=[selected_dataset, data_type_filter],
            outputs=[data_status, dataset_list, selected_dataset, dataset_info, result_display_html]
        )
        
        # æ•°æ®é›†åˆ—è¡¨ç‚¹å‡»äº‹ä»¶ - æ”¯æŒè‡ªåŠ¨é¢„è§ˆå’Œå­—æ®µåŠ è½½
        dataset_list.select(
            fn=self._select_dataset_with_auto_preview,
            inputs=[auto_preview, preview_rows, text_truncation, max_text_length],
            outputs=[selected_dataset, dataset_info, result_display_html, field_selector, data_status]
        )
        
        # é¢„è§ˆè¡Œæ•°å˜åŒ–äº‹ä»¶ï¼Œå®æ—¶æ›´æ–°é¢„è§ˆ
        preview_rows.change(
            fn=self._update_preview_rows,
            inputs=[selected_dataset, preview_rows, text_truncation, max_text_length, field_selector],
            outputs=[result_display_html, data_status]
        )
        
        # å­—æ®µé€‰æ‹©å™¨å˜åŒ–äº‹ä»¶ - å®æ—¶æ›´æ–°é¢„è§ˆ
        field_selector.change(
            fn=self._update_preview_by_fields,
            inputs=[field_selector, preview_rows, text_truncation, max_text_length],
            outputs=[result_display_html, data_status]
        )
        
        # å­—æ®µé€‰æ‹©æŒ‰é’®äº‹ä»¶
        select_all_fields.click(
            fn=self._select_all_dataset_fields,
            outputs=[field_selector]
        )
        
        clear_all_fields.click(
            fn=self._clear_all_dataset_fields,
            outputs=[field_selector]
        )
        
        select_common_fields.click(
            fn=self._select_common_dataset_fields,
            outputs=[field_selector]
        )

    def _filter_datasets(self, data_type: str, search_name: str = "") -> Any:
        """æ ¹æ®æ•°æ®ç±»å‹å’Œåç§°ç­›é€‰æ•°æ®é›†"""
        return self._get_datasets_df(data_type, search_name)
    
    def _refresh_datasets(self, data_type: str, search_name: str = "") -> Any:
        """åˆ·æ–°æ•°æ®é›†åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            self.logger.info(f"å¼€å§‹åˆ·æ–°æ•°æ®é›†åˆ—è¡¨: ç±»å‹={data_type}, æœç´¢={search_name}")
            
            # ç›´æ¥åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œï¼Œé¿å…çº¿ç¨‹æ± å¸¦æ¥çš„ä¸Šä¸‹æ–‡é—®é¢˜å’Œæ½œåœ¨çš„æ­»é”
            # å¯¹äºæ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼ŒPythonçš„GILä¼šé‡Šæ”¾ï¼Œæ‰€ä»¥ä¸ä¼šå®Œå…¨é˜»å¡
            return self._get_datasets_df(data_type, search_name)
            
        except Exception as e:
            self.logger.error(f"åˆ·æ–°æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}")
            return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
            self.logger.error(f'åˆ·æ–°æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}')
            # è¿”å›ç©ºçš„DataFrameï¼Œä½†åŒ…å«æ­£ç¡®çš„åˆ—å
            return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
    
    def _preview_dataset(self, dataset_path: str, rows: int, 
                        enable_truncation: bool = True, max_text_length: int = 300,
                        show_metadata: bool = True, show_stats: bool = True,
                        smart_columns: bool = True, show_all_columns: bool = False,
                        column_info_display: bool = False) -> Tuple[Any, str]:
        """é¢„è§ˆæ•°æ®é›† - ä½¿ç”¨æ–°çš„å¢å¼ºé¢„è§ˆå™¨"""
        try:
            if not dataset_path.strip():
                return "", "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            if not os.path.exists(dataset_path):
                return "", "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨"
            
            # æ›´æ–°é¢„è§ˆå™¨é…ç½®
            self.dataset_previewer.config.max_rows = rows
            self.dataset_previewer.config.enable_truncation = enable_truncation
            self.dataset_previewer.config.max_text_length = max_text_length
            self.dataset_previewer.config.show_stats = show_stats
            self.dataset_previewer.config.include_metadata = show_metadata
            self.dataset_previewer.config.smart_columns = smart_columns
            self.dataset_previewer.config.show_all_columns = show_all_columns
            
            # ä½¿ç”¨å¢å¼ºé¢„è§ˆå™¨é¢„è§ˆæ•°æ®
            preview_result = self.dataset_previewer.preview_dataset(dataset_path, rows)
            
            if not preview_result.success:
                return "", f"âŒ é¢„è§ˆå¤±è´¥: {preview_result.error_message}"
            
            if not preview_result.data:
                return "", "âŒ æ•°æ®é›†ä¸ºç©º"
            
            # è½¬æ¢ä¸ºHTMLè¡¨æ ¼
            df = pd.DataFrame(preview_result.data)
            
            # ç”ŸæˆHTMLè¡¨æ ¼ï¼Œæ·»åŠ è‡ªå®šä¹‰ç±»ä»¥ä¾¿CSSæ§åˆ¶
            html_table = df.to_html(classes="dataframe", index=False, escape=True)
            # åŒ…è£…åœ¨divä¸­ä»¥æ”¯æŒæ»šåŠ¨
            html_content = f'<div class="dataframe-wrap">{html_table}</div>'
            
            # æ„å»ºè¯¦ç»†çŠ¶æ€ä¿¡æ¯
            status_parts = [f"âœ… é¢„è§ˆæˆåŠŸï¼"]
            
            # åŸºç¡€ä¿¡æ¯
            status_parts.append(f"ğŸ“ è·¯å¾„: {dataset_path}")
            status_parts.append(f"ğŸ“Š æ ¼å¼: {preview_result.format.value.upper()}")
            
            total_rows_str = f"{preview_result.total_rows:,}" if preview_result.total_rows != -1 else "æœªçŸ¥ (æ–‡ä»¶è¿‡å¤§)"
            status_parts.append(f"ğŸ“ˆ æ€»è¡Œæ•°: {total_rows_str}")
            
            status_parts.append(f"ğŸ‘€ é¢„è§ˆè¡Œæ•°: {len(preview_result.data)}")
            status_parts.append(f"ğŸ“‚ æ–‡ä»¶æ•°: {preview_result.total_files}")
            
            # æ–‡ä»¶ä¿¡æ¯
            if preview_result.files:
                file_info = preview_result.files[0]  # ä¸»æ–‡ä»¶
                status_parts.append(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {self._format_size(file_info.size)}")
                if file_info.columns:
                    status_parts.append(f"ğŸ“‹ æ€»åˆ—æ•°: {len(file_info.columns)}")
                    # æ˜¾ç¤ºå½“å‰æ˜¾ç¤ºçš„åˆ—æ•°
                    if preview_result.data:
                        displayed_cols = len(preview_result.data[0].keys())
                        status_parts.append(f"ğŸ‘ï¸ æ˜¾ç¤ºåˆ—æ•°: {displayed_cols}")
                    
                    column_names = list(preview_result.data[0].keys())[:5] if preview_result.data else []
                    if len(column_names) > 5:
                        column_names.append("...")
                    status_parts.append(f"ğŸ”¤ å½“å‰åˆ—å: {', '.join(column_names)}")
            
            # éšè—åˆ—ä¿¡æ¯
            if preview_result.hidden_columns:
                status_parts.append(f"ğŸ™ˆ éšè—åˆ—æ•°: {len(preview_result.hidden_columns)}")
                hidden_names = preview_result.hidden_columns[:3]
                if len(preview_result.hidden_columns) > 3:
                    hidden_names.append("...")
                status_parts.append(f"ğŸ”’ éšè—åˆ—å: {', '.join(hidden_names)}")
            
            # æˆªæ–­ä¿¡æ¯
            if preview_result.truncated_fields:
                status_parts.append(f"âœ‚ï¸ æˆªæ–­å­—æ®µ: {', '.join(preview_result.truncated_fields)}")
            
            # åˆ—ç»Ÿè®¡ä¿¡æ¯
            if column_info_display and preview_result.column_info:
                status_parts.append("\nğŸ“Š åˆ—ç»Ÿè®¡ä¿¡æ¯:")
                for col, stats in list(preview_result.column_info.items())[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—
                    null_rate = stats.get('null_rate', 0) * 100
                    unique_count = stats.get('unique_count', 0)
                    avg_length = stats.get('avg_length', 0)
                    status_parts.append(f"  â€¢ {col}: ç©ºå€¼{null_rate:.1f}%, å”¯ä¸€å€¼{unique_count}, å¹³å‡é•¿åº¦{avg_length:.1f}")
            
            # å…ƒæ•°æ®ä¿¡æ¯
            if show_metadata and preview_result.metadata:
                metadata = preview_result.metadata
                if 'column_types' in metadata:
                    type_summary = {}
                    for col, types in metadata['column_types'].items():
                        for t in types:
                            type_summary[t] = type_summary.get(t, 0) + 1
                    
                    type_str = ", ".join([f"{t}: {count}" for t, count in type_summary.items()])
                    status_parts.append(f"ğŸ”¢ æ•°æ®ç±»å‹: {type_str}")
            
            status_msg = "\n".join(status_parts)
            
            return html_content, status_msg
            
        except Exception as e:
            self.logger.error(f'é¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}')
            return "", f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f}TB"
    
    def _preview_dataset_enhanced(self, dataset_path: str, rows: int, 
                                 enable_truncation: bool, max_text_length: int,
                                 show_metadata: bool, show_stats: bool,
                                 smart_columns: bool, show_all_columns: bool,
                                 column_info_display: bool) -> Tuple[str, Any, str]:
        """å¢å¼ºé¢„è§ˆæ•°æ®é›†ï¼ˆé€šè¿‡é¢„è§ˆæŒ‰é’®è§¦å‘ï¼‰"""
        try:
            if not dataset_path.strip():
                return "*è¯·å…ˆé€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†*", "", "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            # è°ƒç”¨å¢å¼ºé¢„è§ˆåŠŸèƒ½
            preview_html, status_msg = self._preview_dataset(
                dataset_path, rows, enable_truncation, max_text_length, 
                show_metadata, show_stats, smart_columns, show_all_columns, column_info_display
            )
            
            # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å†æœ‰preview_dfï¼Œæ‰€ä»¥éœ€è¦è°ƒæ•´_generate_dataset_info
            # æˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨_preview_datasetä¸­è¿”å›dfå’Œhtml
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬è¿™é‡Œåªä¼ é€’status_msg
            dataset_info = self._generate_dataset_info(dataset_path, None, status_msg)
            
            return dataset_info, preview_html, status_msg
            
        except Exception as e:
            self.logger.error(f'å¢å¼ºé¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}')
            error_msg = f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
            return "*é¢„è§ˆå¤±è´¥*", "", error_msg
    
    def _generate_dataset_info(self, dataset_path: str, preview_df: Any, status_msg: str) -> str:
        """ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯Markdown"""
        try:
            # if preview_df.empty:
            #     return "*æš‚æ— æ•°æ®é›†ä¿¡æ¯*"
            
            # è·å–æ•°æ®é›†æ‘˜è¦
            summary = self.dataset_previewer.get_dataset_summary(dataset_path)
            
            if 'error' in summary:
                return f"*æ•°æ®é›†ä¿¡æ¯è·å–å¤±è´¥: {summary['error']}*"
            
            info_parts = [
                f"### ğŸ“Š æ•°æ®é›†è¯¦æƒ…",
                f"**åç§°**: {summary.get('name', 'Unknown')}",
                f"**ç±»å‹**: {summary.get('type', 'Unknown')}",
                f"**æ ¼å¼**: {summary.get('format', 'Unknown').upper()}"
            ]
            
            if summary.get('size_human'):
                info_parts.append(f"**å¤§å°**: {summary['size_human']}")
            
            if summary.get('estimated_rows'):
                info_parts.append(f"**é¢„ä¼°è¡Œæ•°**: {summary['estimated_rows']:,}")
            
            if summary.get('file_count'):
                info_parts.append(f"**æ–‡ä»¶æ•°é‡**: {summary['file_count']}")
            
            if summary.get('formats'):
                info_parts.append(f"**åŒ…å«æ ¼å¼**: {', '.join(summary['formats'])}")
            
            # é¢„è§ˆæ•°æ®ä¿¡æ¯
            if hasattr(preview_df, 'empty') and not preview_df.empty:
                info_parts.extend([
                    "",
                    f"### ğŸ“ˆ é¢„è§ˆç»Ÿè®¡",
                    f"**é¢„è§ˆè¡Œæ•°**: {len(preview_df)}",
                    f"**åˆ—æ•°**: {len(preview_df.columns)}",
                    f"**åˆ—å**: {', '.join(preview_df.columns.tolist()[:5])}{'...' if len(preview_df.columns) > 5 else ''}"
                ])
            
            return "\n".join(info_parts)
            
        except Exception as e:
            self.logger.error(f'ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}')
            return f"*æ•°æ®é›†ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {str(e)}*"
    
    def _delete_dataset(self, dataset_path: str, data_type: str) -> Tuple[str, Any, str, str, Any]:
        """åˆ é™¤æ•°æ®é›†"""
        try:
            if not dataset_path.strip():
                return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ•°æ®é›†", pd.DataFrame(), "", "", ""
            
            # æ£€æŸ¥è·¯å¾„ç±»å‹
            from pathlib import Path
            path_obj = Path(dataset_path)
            if path_obj.is_file():
                path_type = "æ–‡ä»¶"
            elif path_obj.is_dir():
                path_type = "æ•°æ®é›†ç›®å½•"
            else:
                path_type = "è·¯å¾„"
            
            # è°ƒç”¨æ•°æ®ç®¡ç†æ¨¡å—åˆ é™¤æ•°æ®
            success = data_manager.delete_data(dataset_path)
            
            if success:
                # åˆ é™¤æˆåŠŸååˆ·æ–°å½“å‰é€‰ä¸­çš„æ•°æ®ç±»å‹åˆ—è¡¨
                refreshed_df = self._get_datasets_df(data_type)
                
                # æ¸…ç©ºé€‰ä¸­çŠ¶æ€å’Œæ˜¾ç¤ºå†…å®¹
                return (f"âœ… åˆ é™¤{path_type}å®Œæˆï¼\n{path_type}: {dataset_path}", 
                       refreshed_df, 
                       "",  # æ¸…ç©ºselected_dataset
                       "",  # æ¸…ç©ºdataset_info
                       "")  # æ¸…ç©ºresult_displayç”¨ç©ºå­—ç¬¦ä¸²
            else:
                return f"âŒ åˆ é™¤{path_type}å¤±è´¥ï¼šå¯èƒ½æ–‡ä»¶ä¸å­˜åœ¨æˆ–æƒé™ä¸è¶³", pd.DataFrame(), dataset_path, "", ""
                
        except Exception as e:
            self.logger.error(f'åˆ é™¤æ•°æ®é›†å¤±è´¥: {e}')
            return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}", pd.DataFrame(), dataset_path, "", ""
    
    def _get_storage_stats(self) -> str:
        """è·å–å­˜å‚¨ç»Ÿè®¡"""
        try:
            # è°ƒç”¨æ•°æ®ç®¡ç†æ¨¡å—è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = data_manager.get_storage_statistics()
            
            # æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯
            stats_text = f"""ğŸ“Š å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯

ğŸ“ æ•°æ®ç±»å‹ç»Ÿè®¡:
  åŸå§‹æ•°æ®: {stats.get('raw_count', 0)} ä¸ªæ–‡ä»¶
  å¤„ç†æ•°æ®: {stats.get('processed_count', 0)} ä¸ªæ–‡ä»¶  
  è’¸é¦æ•°æ®: {stats.get('distilled_count', 0)} ä¸ªæ–‡ä»¶
  å¤‡ä»½æ•°æ®: {stats.get('backup_count', 0)} ä¸ªæ–‡ä»¶

ğŸ’¾ å­˜å‚¨ç©ºé—´:
  æ€»å¤§å°: {stats.get('total_size_mb', 0):.1f} MB
  åŸå§‹æ•°æ®: {stats.get('raw_size_mb', 0):.1f} MB
  å¤„ç†æ•°æ®: {stats.get('processed_size_mb', 0):.1f} MB
  è’¸é¦æ•°æ®: {stats.get('distilled_size_mb', 0):.1f} MB

ğŸ“ˆ æœ€æ–°æ´»åŠ¨:
  æœ€æ–°æ–‡ä»¶: {stats.get('latest_file', 'N/A')}
  åˆ›å»ºæ—¶é—´: {stats.get('latest_time', 'N/A')}
"""
            
            return stats_text
            
        except Exception as e:
            self.logger.error(f'è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}')
            return f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"
    
    def _select_dataset(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©æ•°æ®é›† - æ”¯æŒç‚¹å‡»ä»»æ„åˆ—"""
        try:
            # è·å–å½“å‰è¡Œçš„æ‰€æœ‰æ•°æ®
            row_data = evt.row_value
            if row_data and len(row_data) >= 5:
                # è¿”å›è·¯å¾„åˆ—ï¼ˆç¬¬5åˆ—ï¼Œç´¢å¼•4ï¼‰çš„å€¼
                return row_data[4]
            return ""
        except Exception as e:
            self.logger.error(f'é€‰æ‹©æ•°æ®é›†å¤±è´¥: {e}')
            return ""
    
    def _select_dataset_with_enhanced_preview(self, evt: gr.SelectData, auto_preview: bool, preview_rows: int,
                                            enable_truncation: bool, max_text_length: int,
                                            show_metadata: bool, show_stats: bool,
                                            smart_columns: bool, show_all_columns: bool,
                                            column_info_display: bool) -> Tuple[str, str, Any, str]:
        """é€‰æ‹©æ•°æ®é›†å¹¶ä½¿ç”¨å¢å¼ºé¢„è§ˆ"""
        try:
            # è·å–å½“å‰è¡Œçš„æ‰€æœ‰æ•°æ®
            row_data = evt.row_value
            if not row_data or len(row_data) < 5:
                return "", "*è¯·é€‰æ‹©æœ‰æ•ˆçš„æ•°æ®é›†*", "", ""
            
            # æå–æ•°æ®é›†ä¿¡æ¯
            dataset_name = row_data[0]
            dataset_type = row_data[1]
            dataset_size = row_data[2]
            dataset_time = row_data[3]
            dataset_path = row_data[4]
            
            # å¦‚æœå¯ç”¨è‡ªåŠ¨é¢„è§ˆï¼Œåˆ™æ‰§è¡Œå¢å¼ºé¢„è§ˆ
            if auto_preview and dataset_path:
                preview_html, status_msg = self._preview_dataset(
                    dataset_path, preview_rows, enable_truncation, max_text_length, 
                    show_metadata, show_stats, smart_columns, show_all_columns, column_info_display
                )
                dataset_info = self._generate_dataset_info(dataset_path, preview_html, status_msg)
                return dataset_path, dataset_info, preview_html, status_msg
            else:
                # æ„å»ºåŸºç¡€æ•°æ®é›†ä¿¡æ¯æ˜¾ç¤º
                info_text = f"""**ğŸ“Š æ•°æ®é›†ä¿¡æ¯**
- **åç§°**: {dataset_name}
- **ç±»å‹**: {dataset_type}
- **å¤§å°**: {dataset_size}
- **åˆ›å»ºæ—¶é—´**: {dataset_time}
- **è·¯å¾„**: `{os.path.basename(dataset_path)}`"""
                
                return dataset_path, info_text, "", "âœ… æ•°æ®é›†å·²é€‰æ‹©ï¼Œç‚¹å‡»é¢„è§ˆæŒ‰é’®æŸ¥çœ‹æ•°æ®å†…å®¹"
                
        except Exception as e:
            self.logger.error(f'é€‰æ‹©æ•°æ®é›†å¤±è´¥: {e}')
            return "", f"âŒ é€‰æ‹©å¤±è´¥: {str(e)}", "", ""
    
    def _get_datasets_df(self, data_type: str = "å…¨éƒ¨", search_name: str = "") -> Any:
        """è·å–æ•°æ®é›†åˆ—è¡¨æ•°æ®æ¡†"""
        try:
            # æ•°æ®ç±»å‹æ˜ å°„
            type_map = {
                "å…¨éƒ¨": None,
                "åŸå§‹æ•°æ®": "raw",
                "å¤„ç†æ•°æ®": "processed", 
                "è’¸é¦æ•°æ®": "distilled"
            }
            
            filter_type = type_map.get(data_type)
            
            # è°ƒç”¨æ•°æ®ç®¡ç†æ¨¡å—è·å–æ•°æ®é›†åˆ—è¡¨
            datasets = data_manager.list_datasets(data_type=filter_type)
            
            if not datasets:
                return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
            
            # è¿‡æ»¤å’Œä¼˜åŒ–æ•°æ®é›†åˆ—è¡¨
            filtered_datasets = self._filter_datasets_for_display(datasets)
            
            # æ ¹æ®åç§°æœç´¢è¿‡æ»¤
            if search_name and search_name.strip():
                search_keyword = search_name.strip().lower()
                filtered_datasets = [
                    ds for ds in filtered_datasets 
                    if search_keyword in ds.get('display_name', '').lower() or
                       search_keyword in ds.get('path', '').lower()
                ]
            
            # æ„å»ºæ•°æ®æ¡†
            rows = []
            for dataset in filtered_datasets:
                file_path = dataset.get('path', '')
                
                # ä½¿ç”¨æ–°çš„åˆ†å±‚æ˜¾ç¤ºé€»è¾‘
                if 'display_name' in dataset:
                    # ä½¿ç”¨åˆ†å±‚æ˜¾ç¤ºåç§°
                    display_name = dataset['display_name']
                    # åˆ†å±‚æ¡ç›®ä½¿ç”¨ provider ä½œä¸ºç±»å‹æ˜¾ç¤ºï¼ˆä¾‹å¦‚ï¼šè’¸é¦æ•°æ®/å¤„ç†æ•°æ®/æä¾›å•†åï¼‰
                    data_type_cn = dataset.get('provider') or dataset.get('type', 'æœªçŸ¥')
                    
                    # æ ¼å¼åŒ–å¤§å°æ˜¾ç¤º
                    size_mb = dataset.get('size_mb', 0)
                    if size_mb > 1024:
                        size_display = f"{size_mb/1024:.1f} GB"
                    elif size_mb > 0:
                        size_display = f"{size_mb:.1f} MB"
                    else:
                        size_display = "è®¡ç®—ä¸­..."
                    
                    # æ–‡ä»¶æ•°é‡ä¿¡æ¯
                    file_count = dataset.get('file_count', 0)
                    if file_count > 1:
                        size_display += f" ({file_count}ä¸ªæ–‡ä»¶)"
                    
                else:
                    # å…¼å®¹æ—§æ ¼å¼ - æ£€æŸ¥æ˜¯å¦ä¸ºæ±‡æ€»æ¡ç›®
                    if dataset.get('is_summary', False):
                        # æ•°æ®é›†æ±‡æ€»æ¡ç›®çš„ç‰¹æ®Šå¤„ç†
                        dataset_name = dataset.get('dataset_name', '')
                        file_count = dataset.get('file_count', 0)
                        organized_count = dataset.get('organized_count', 0)
                        cache_count = dataset.get('cache_count', 0)
                        
                        # æ˜¾ç¤ºåç§°åŒ…å«ç»Ÿè®¡ä¿¡æ¯
                        display_name = f"ğŸ“ {dataset_name} ({file_count}ä¸ªæ–‡ä»¶)"
                        if organized_count > 0:
                            display_name += f" âœ¨{organized_count}æ•´ç†"
                        if cache_count > 0:
                            display_name += f" ğŸ“¦{cache_count}ç¼“å­˜"
                        
                        data_type_cn = "æ•°æ®é›†æ±‡æ€»"
                        
                    else:
                        # æ™®é€šæ–‡ä»¶æ¡ç›®
                        file_name = os.path.basename(file_path)
                        display_name = file_name
                        
                        # ç¡®å®šæ•°æ®ç±»å‹
                        path_norm = (file_path or '').replace('\\', '/')
                        if '/raw/' in path_norm:
                            if '/organized_files/' in path_norm:
                                data_type_cn = "åŸå§‹æ•°æ®(æ•´ç†)"
                            elif '/cache/' in path_norm:
                                data_type_cn = "åŸå§‹æ•°æ®(ç¼“å­˜)"
                            else:
                                data_type_cn = "åŸå§‹æ•°æ®"
                        elif '/distilled/' in path_norm:
                            data_type_cn = "è’¸é¦æ•°æ®"
                        elif '/processed/' in path_norm:
                            data_type_cn = "å¤„ç†æ•°æ®"
                        else:
                            data_type_cn = "å…¶ä»–"
                    
                    # æ–‡ä»¶å¤§å° - æ”¹è¿›æ˜¾ç¤ºæ ¼å¼
                    size_mb = dataset.get('size_mb', 0)
                    if size_mb > 1024:
                        size_display = f"{size_mb/1024:.1f} GB"
                    elif size_mb > 1:
                        size_display = f"{size_mb:.1f} MB"
                    elif size_mb > 0:
                        size_display = f"{size_mb*1024:.0f} KB"
                    else:
                        size_display = "è®¡ç®—ä¸­..."
                
                # åˆ›å»ºæ—¶é—´
                create_time = dataset.get('create_time', '')
                if create_time:
                    try:
                        dt = datetime.fromisoformat(create_time.replace('Z', '+00:00'))
                        time_str = dt.strftime('%Y-%m-%d %H:%M')
                    except:
                        time_str = create_time[:16] if len(create_time) > 16 else create_time
                else:
                    time_str = ""
                
                rows.append([
                    display_name,
                    data_type_cn,
                    size_display,
                    time_str,
                    file_path
                ])
            
            return pd.DataFrame(rows, columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
            
        except Exception as e:
            self.logger.error(f'è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
    
    def _select_dataset_with_auto_preview(self, evt: gr.SelectData, auto_preview: bool, 
                                        preview_rows: int, enable_truncation: bool, 
                                        max_text_length: int) -> Tuple[str, str, Any, gr.CheckboxGroup, str]:
        """é€‰æ‹©æ•°æ®é›†å¹¶è‡ªåŠ¨é¢„è§ˆï¼ˆæ–°ç‰ˆæœ¬ï¼ŒåŒ…å«å­—æ®µé€‰æ‹©ï¼‰"""
        try:
            # è·å–å½“å‰è¡Œçš„æ‰€æœ‰æ•°æ®
            row_data = evt.row_value
            if not row_data or len(row_data) < 5:
                return "", "*è¯·é€‰æ‹©æœ‰æ•ˆçš„æ•°æ®é›†*", "", gr.CheckboxGroup(choices=[], value=[]), ""
            
            # æå–æ•°æ®é›†ä¿¡æ¯
            dataset_name = row_data[0]
            dataset_type = row_data[1]
            dataset_size = row_data[2]
            dataset_time = row_data[3]
            dataset_path = row_data[4]
            
            # å¦‚æœå¯ç”¨è‡ªåŠ¨é¢„è§ˆï¼Œåˆ™æ‰§è¡Œé¢„è§ˆå¹¶åŠ è½½å­—æ®µ
            if auto_preview and dataset_path:
                return self._load_dataset_with_fields(dataset_path, preview_rows, enable_truncation, max_text_length)
            else:
                # æ„å»ºåŸºç¡€æ•°æ®é›†ä¿¡æ¯æ˜¾ç¤º
                info_text = f"""**ğŸ“Š æ•°æ®é›†ä¿¡æ¯**
- **åç§°**: {dataset_name}
- **ç±»å‹**: {dataset_type}
- **å¤§å°**: {dataset_size}
- **åˆ›å»ºæ—¶é—´**: {dataset_time}
- **è·¯å¾„**: `{os.path.basename(dataset_path)}`"""
                
                return dataset_path, info_text, "", gr.CheckboxGroup(choices=[], value=[]), "âœ… æ•°æ®é›†å·²é€‰æ‹©ï¼Œç­‰å¾…é¢„è§ˆ..."
                
        except Exception as e:
            self.logger.error(f'é€‰æ‹©æ•°æ®é›†å¤±è´¥: {e}')
            return "", f"âŒ é€‰æ‹©å¤±è´¥: {str(e)}", "", gr.CheckboxGroup(choices=[], value=[]), ""
    
    def _load_dataset_with_fields(self, dataset_path: str, preview_rows: int, 
                                enable_truncation: bool, max_text_length: int) -> Tuple[str, str, Any, gr.CheckboxGroup, str]:
        """åŠ è½½æ•°æ®é›†å¹¶åˆ†æå­—æ®µï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰"""
        try:
            if not dataset_path.strip():
                return "", "*è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†*", "", gr.CheckboxGroup(choices=[], value=[]), "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            if not os.path.exists(dataset_path):
                return "", "*æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨*", "", gr.CheckboxGroup(choices=[], value=[]), "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ•°æ®é›†ï¼Œå¦‚æœæ˜¯åˆ™æ¸…ç©ºç¼“å­˜
            if self.current_dataset_cache['path'] != dataset_path:
                self.current_dataset_cache = {
                    'path': dataset_path,
                    'data': None,
                    'fields': [],
                    'original_preview': None
                }
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰æ•°æ®ï¼Œåˆ™è¯»å–æ•°æ®é›†
            if self.current_dataset_cache['data'] is None:
                # æ›´æ–°é¢„è§ˆå™¨é…ç½®
                self.dataset_previewer.config.max_rows = preview_rows
                self.dataset_previewer.config.enable_truncation = False  # å…ˆä¸æˆªæ–­ï¼Œä¿å­˜å®Œæ•´æ•°æ®
                self.dataset_previewer.config.max_text_length = max_text_length
                self.dataset_previewer.config.smart_columns = False  # ä¸æ™ºèƒ½éšè—åˆ—ï¼Œæ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
                self.dataset_previewer.config.show_all_columns = True
                
                # ä½¿ç”¨å¢å¼ºé¢„è§ˆå™¨é¢„è§ˆæ•°æ®
                preview_result = self.dataset_previewer.preview_dataset(dataset_path, preview_rows)
                
                if not preview_result.success:
                    return "", "*é¢„è§ˆå¤±è´¥*", "", gr.CheckboxGroup(choices=[], value=[]), f"âŒ é¢„è§ˆå¤±è´¥: {preview_result.error_message}"
                
                if not preview_result.data:
                    return "", "*æ•°æ®é›†ä¸ºç©º*", "", gr.CheckboxGroup(choices=[], value=[]), "âŒ æ•°æ®é›†ä¸ºç©º"
                
                # ç¼“å­˜æ•°æ®
                self.current_dataset_cache['data'] = preview_result.data
                self.current_dataset_cache['original_preview'] = preview_result
                
                # ä½¿ç”¨universal_field_extractoråˆ†æå­—æ®µï¼ˆä¿®å¤HuggingFaceæ•°æ®é›†é—®é¢˜ï¼‰
                try:
                    field_names = self._get_simple_field_names(dataset_path)
                    if field_names:
                        self.current_dataset_cache['fields'] = field_names
                    else:
                        # å¦‚æœuniversal_field_extractorå¤±è´¥ï¼Œå›é€€åˆ°ä»æ•°æ®ä¸­æå–å­—æ®µ
                        if preview_result.data:
                            available_fields = list(preview_result.data[0].keys())
                            self.current_dataset_cache['fields'] = available_fields
                        else:
                            self.current_dataset_cache['fields'] = []
                except Exception as field_error:
                    self.logger.warning(f'å­—æ®µæå–å¤±è´¥ï¼Œä½¿ç”¨é¢„è§ˆæ•°æ®å­—æ®µ: {field_error}')
                    # å›é€€åˆ°ä»æ•°æ®ä¸­æå–å­—æ®µ
                    if preview_result.data:
                        available_fields = list(preview_result.data[0].keys())
                        self.current_dataset_cache['fields'] = available_fields
                    else:
                        self.current_dataset_cache['fields'] = []
            
            # è·å–å¯ç”¨å­—æ®µ
            available_fields = self.current_dataset_cache['fields']
            
            # è¯†åˆ«å¸¸ç”¨å­—æ®µå¹¶è®¾ä¸ºé»˜è®¤é€‰ä¸­
            common_fields = self._identify_common_fields(available_fields)
            
            # åˆ›å»ºå­—æ®µé€‰æ‹©å™¨
            field_choices = [(field, field) for field in available_fields]
            field_selector = gr.CheckboxGroup(
                label="å¯ç”¨å­—æ®µ",
                choices=field_choices,
                value=common_fields,  # é»˜è®¤é€‰ä¸­å¸¸ç”¨å­—æ®µ
                interactive=True,
                info=f"æ•°æ®é›†åŒ…å« {len(available_fields)} ä¸ªå­—æ®µï¼Œå·²é»˜è®¤é€‰ä¸­å¸¸ç”¨å­—æ®µ"
            )
            
            # ä½¿ç”¨é»˜è®¤é€‰ä¸­çš„å­—æ®µç”Ÿæˆé¢„è§ˆ
            preview_df, status_msg = self._filter_preview_by_fields(common_fields, enable_truncation, max_text_length)
            
            # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
            dataset_info = self._generate_dataset_info_from_cache(dataset_path, preview_df)
            
            # è½¬æ¢ä¸ºHTML
            if not preview_df.empty:
                html_table = preview_df.to_html(classes="dataframe", index=False, escape=True)
                html_content = f'<div class="dataframe-wrap">{html_table}</div>'
            else:
                html_content = ""
            
            return dataset_path, dataset_info, html_content, field_selector, status_msg
            
        except Exception as e:
            self.logger.error(f'åŠ è½½æ•°æ®é›†å’Œå­—æ®µå¤±è´¥: {e}')
            error_msg = f"âŒ åŠ è½½å¤±è´¥: {str(e)}"
            return "", "*åŠ è½½å¤±è´¥*", "", gr.CheckboxGroup(choices=[], value=[]), error_msg
    
    def _identify_common_fields(self, available_fields: List[str]) -> List[str]:
        """è¯†åˆ«å¸¸ç”¨å­—æ®µ"""
        # å¸¸ç”¨å­—æ®µä¼˜å…ˆçº§åˆ—è¡¨ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
        common_patterns = [
            'instruction', 'input', 'output', 'response', 'answer', 'question', 
            'text', 'content', 'prompt', 'completion', 'query', 'reply',
            'conversations', 'messages', 'dialogue', 'chat',
            'title', 'description', 'summary', 'context'
        ]
        
        selected_fields = []
        available_lower = [field.lower() for field in available_fields]
        
        # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©å­—æ®µ
        for pattern in common_patterns:
            for i, field_lower in enumerate(available_lower):
                if pattern in field_lower and available_fields[i] not in selected_fields:
                    selected_fields.append(available_fields[i])
                    break  # æ¯ç§æ¨¡å¼åªé€‰æ‹©ä¸€ä¸ªå­—æ®µ
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å¸¸ç”¨å­—æ®µï¼Œé€‰æ‹©å‰å‡ ä¸ªå­—æ®µ
        if not selected_fields:
            selected_fields = available_fields[:min(3, len(available_fields))]
        
        # é™åˆ¶é€‰ä¸­å­—æ®µæ•°é‡ï¼ˆé¿å…æ˜¾ç¤ºè¿‡å¤šåˆ—ï¼‰
        max_fields = 5
        if len(selected_fields) > max_fields:
            selected_fields = selected_fields[:max_fields]
        
        return selected_fields
    
    def _filter_preview_by_fields(self, selected_fields: List[str], 
                                enable_truncation: bool, max_text_length: int) -> Tuple[Any, str]:
        """æ ¹æ®é€‰ä¸­å­—æ®µè¿‡æ»¤é¢„è§ˆæ•°æ®"""
        try:
            if not self.current_dataset_cache['data']:
                return pd.DataFrame(), "âŒ æ²¡æœ‰ç¼“å­˜çš„æ•°æ®"
            
            if not selected_fields:
                return pd.DataFrame(), "âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå­—æ®µ"
            
            # ä»ç¼“å­˜çš„å®Œæ•´æ•°æ®ä¸­è¿‡æ»¤å­—æ®µ
            filtered_data = []
            for row in self.current_dataset_cache['data']:
                filtered_row = {}
                for field in selected_fields:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—å­—æ®µè·¯å¾„
                    if '.' in field or '[' in field:
                        # ä½¿ç”¨universal_field_extractorçš„æ–¹æ³•æå–åµŒå¥—å€¼
                        try:
                            from ..universal_field_extractor import _extractor
                            value = _extractor._get_nested_value(row, field)
                        except:
                            value = None
                    else:
                        # ç®€å•å­—æ®µç›´æ¥è·å–
                        value = row.get(field)
                    
                    # å¤„ç†Noneå€¼
                    if value is None:
                        filtered_row[field] = ""
                    else:
                        # åº”ç”¨æ–‡æœ¬æˆªæ–­
                        if enable_truncation and isinstance(value, str) and len(value) > max_text_length:
                            value = value[:max_text_length] + "..."
                        # ç¡®ä¿å€¼å¯ä»¥è¢«pandaså¤„ç†
                        if isinstance(value, (list, dict)):
                            filtered_row[field] = str(value)
                        else:
                            filtered_row[field] = value
                    
                filtered_data.append(filtered_row)
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(filtered_data)
            
            # æ„å»ºçŠ¶æ€ä¿¡æ¯
            preview_result = self.current_dataset_cache['original_preview']
            status_parts = [
                f"âœ… å­—æ®µè¿‡æ»¤é¢„è§ˆæˆåŠŸï¼",
                f"ğŸ“ æ•°æ®é›†: {os.path.basename(self.current_dataset_cache['path'])}",
                f"ğŸ“Š æ€»å­—æ®µæ•°: {len(self.current_dataset_cache['fields'])}",
                f"ğŸ‘ï¸ æ˜¾ç¤ºå­—æ®µæ•°: {len(selected_fields)}",
                f"ğŸ“ˆ æ€»è¡Œæ•°: {preview_result.total_rows:,}",
                f"ğŸ‘€ é¢„è§ˆè¡Œæ•°: {len(filtered_data)}",
                f"ğŸ“‹ é€‰ä¸­å­—æ®µ: {', '.join(selected_fields)}"
            ]
            
            if enable_truncation:
                status_parts.append(f"âœ‚ï¸ æ–‡æœ¬æˆªæ–­: {max_text_length} å­—ç¬¦")
            
            status_msg = "\n".join(status_parts)
            
            return df, status_msg
            
        except Exception as e:
            self.logger.error(f'è¿‡æ»¤é¢„è§ˆæ•°æ®å¤±è´¥: {e}')
            return pd.DataFrame(), f"âŒ è¿‡æ»¤å¤±è´¥: {str(e)}"
    
    def _generate_dataset_info_from_cache(self, dataset_path: str, preview_df: Any) -> str:
        """ä»ç¼“å­˜ä¿¡æ¯ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯"""
        try:
            if not self.current_dataset_cache['original_preview']:
                return "*æš‚æ— æ•°æ®é›†ä¿¡æ¯*"
            
            preview_result = self.current_dataset_cache['original_preview']
            
            info_parts = [
                f"### ğŸ“Š æ•°æ®é›†è¯¦æƒ…",
                f"**åç§°**: {os.path.basename(dataset_path)}",
                f"**æ ¼å¼**: {preview_result.format.value.upper()}",
                f"**æ€»è¡Œæ•°**: {preview_result.total_rows:,}",
                f"**æ€»å­—æ®µæ•°**: {len(self.current_dataset_cache['fields'])}",
                f"**æ–‡ä»¶æ•°**: {preview_result.total_files}"
            ]
            
            # æ–‡ä»¶ä¿¡æ¯
            if preview_result.files:
                file_info = preview_result.files[0]
                info_parts.append(f"**æ–‡ä»¶å¤§å°**: {self._format_size(file_info.size)}")
            
            # é¢„è§ˆä¿¡æ¯
            if not preview_df.empty:
                info_parts.extend([
                    "",
                    f"### ğŸ“ˆ å½“å‰é¢„è§ˆ",
                    f"**é¢„è§ˆè¡Œæ•°**: {len(preview_df)}",
                    f"**æ˜¾ç¤ºå­—æ®µ**: {len(preview_df.columns)}",
                    f"**å­—æ®µåˆ—è¡¨**: {', '.join(preview_df.columns.tolist())}"
                ])
            
            # æ‰€æœ‰å¯ç”¨å­—æ®µ
            all_fields = self.current_dataset_cache['fields']
            if len(all_fields) > len(preview_df.columns):
                hidden_fields = [f for f in all_fields if f not in preview_df.columns]
                info_parts.append(f"**éšè—å­—æ®µ**: {', '.join(hidden_fields[:5])}{'...' if len(hidden_fields) > 5 else ''}")
            
            return "\n".join(info_parts)
            
        except Exception as e:
            self.logger.error(f'ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}')
            return f"*æ•°æ®é›†ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {str(e)}*"

    def _preview_dataset_with_field_filter(self, dataset_path: str, preview_rows: int,
                                         enable_truncation: bool, max_text_length: int,
                                         selected_fields: List[str]) -> Tuple[str, str]:
        """ä½¿ç”¨å­—æ®µè¿‡æ»¤å™¨é¢„è§ˆæ•°æ®é›†ï¼ˆæ‰‹åŠ¨é¢„è§ˆæŒ‰é’®ï¼‰"""
        try:
            if not dataset_path.strip():
                return "", "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            # å¦‚æœä¸æ˜¯å½“å‰ç¼“å­˜çš„æ•°æ®é›†ï¼Œé‡æ–°åŠ è½½
            if self.current_dataset_cache['path'] != dataset_path:
                # _load_dataset_with_fields returns: dataset_path, dataset_info, html_content, field_selector, status_msg
                _, _, html_content, _, status_msg = self._load_dataset_with_fields(
                    dataset_path, preview_rows, enable_truncation, max_text_length
                )
                return html_content, status_msg
            
            # ä½¿ç”¨é€‰ä¸­çš„å­—æ®µè¿‡æ»¤é¢„è§ˆ
            if selected_fields:
                # _filter_preview_by_fields returns: df, status_msg
                df, status_msg = self._filter_preview_by_fields(selected_fields, enable_truncation, max_text_length)
                if not df.empty:
                    html_table = df.to_html(classes="dataframe", index=False, escape=True)
                    html_content = f'<div class="dataframe-wrap">{html_table}</div>'
                    return html_content, status_msg
                return "", status_msg
            else:
                return "", "ğŸ’¡ è¯·é€‰æ‹©è¦æ˜¾ç¤ºçš„å­—æ®µ"
        except Exception as e:
            self.logger.error(f'é¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}')
            return "", f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    def _update_preview_rows(self, dataset_path: str, preview_rows: int, 
                           enable_truncation: bool, max_text_length: int,
                           selected_fields: List[str]) -> Tuple[Any, str]:
        """æ›´æ–°é¢„è§ˆè¡Œæ•°"""
        try:
            if not dataset_path or not dataset_path.strip():
                return "", "ğŸ’¡ è¯·å…ˆé€‰æ‹©æ•°æ®é›†"
            
            # å¼ºåˆ¶é‡æ–°åŠ è½½æ•°æ®ï¼ˆå› ä¸ºè¡Œæ•°å˜äº†ï¼Œç¼“å­˜çš„æ•°æ®å¯èƒ½ä¸å¤Ÿï¼‰
            # æ¸…ç©ºå½“å‰ç¼“å­˜çš„æ•°æ®ï¼Œè¿«ä½¿ _load_dataset_with_fields é‡æ–°è¯»å–
            self.current_dataset_cache['data'] = None
            
            # é‡æ–°åŠ è½½å¹¶åº”ç”¨å­—æ®µè¿‡æ»¤
            _, _, preview_df, _, status_msg = self._load_dataset_with_fields(
                dataset_path, preview_rows, enable_truncation, max_text_length
            )
            
            # å¦‚æœæœ‰é€‰ä¸­çš„å­—æ®µï¼Œåº”ç”¨è¿‡æ»¤
            if selected_fields:
                # æ³¨æ„ï¼š_load_dataset_with_fields è¿”å›çš„æ˜¯ HTML å­—ç¬¦ä¸² preview_df
                # ä½†æˆ‘ä»¬éœ€è¦é‡æ–°è¿‡æ»¤ï¼Œæ‰€ä»¥è¿™é‡Œé€»è¾‘ç¨å¾®å¤æ‚ç‚¹
                # å®é™…ä¸Š _load_dataset_with_fields å†…éƒ¨å·²ç»æ›´æ–°äº† cache['data']
                # æˆ‘ä»¬ç›´æ¥è°ƒç”¨ _filter_preview_by_fields å³å¯
                df, msg = self._filter_preview_by_fields(selected_fields, enable_truncation, max_text_length)
                if not df.empty:
                    html_table = df.to_html(classes="dataframe", index=False, escape=True)
                    html_content = f'<div class="dataframe-wrap">{html_table}</div>'
                    return html_content, msg
            
            # å¦‚æœæ²¡æœ‰é€‰ä¸­å­—æ®µæˆ–è¿‡æ»¤å¤±è´¥ï¼Œè¿”å›é»˜è®¤åŠ è½½çš„ç»“æœ
            return preview_df, status_msg
            
        except Exception as e:
            self.logger.error(f'æ›´æ–°é¢„è§ˆè¡Œæ•°å¤±è´¥: {e}')
            return "", f"âŒ æ›´æ–°å¤±è´¥: {str(e)}"

    def _update_preview_by_fields(self, selected_fields: List[str], 
                                preview_rows: int, enable_truncation: bool, 
                                max_text_length: int) -> Tuple[str, str]:
        """æ ¹æ®å­—æ®µé€‰æ‹©æ›´æ–°é¢„è§ˆï¼ˆä¸é‡æ–°åŠ è½½æ–‡ä»¶ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜æ•°æ®
            if not self.current_dataset_cache['data']:
                return "", "âŒ è¯·å…ˆé€‰æ‹©æ•°æ®é›†"
            
            # è°ƒç”¨è¿‡æ»¤é€»è¾‘
            df, status_msg = self._filter_preview_by_fields(selected_fields, enable_truncation, max_text_length)
            
            if not df.empty:
                html_table = df.to_html(classes="dataframe", index=False, escape=True)
                html_content = f'<div class="dataframe-wrap">{html_table}</div>'
                return html_content, status_msg
            else:
                return "", status_msg
                
        except Exception as e:
            self.logger.error(f'æ›´æ–°å­—æ®µé¢„è§ˆå¤±è´¥: {e}')
            return "", f"âŒ æ›´æ–°å¤±è´¥: {str(e)}"

    def _select_all_dataset_fields(self) -> Dict[str, Any]:
        """å…¨é€‰æ‰€æœ‰å­—æ®µ"""
        try:
            all_fields = self.current_dataset_cache['fields']
            field_choices = [(field, field) for field in all_fields]
            return gr.update(
                choices=field_choices,
                value=all_fields,
                interactive=True
            )
        except Exception as e:
            self.logger.error(f'å…¨é€‰å­—æ®µå¤±è´¥: {e}')
            return gr.update(choices=[], value=[])
    
    def _clear_all_dataset_fields(self) -> Dict[str, Any]:
        """æ¸…é™¤æ‰€æœ‰å­—æ®µé€‰æ‹©"""
        try:
            all_fields = self.current_dataset_cache['fields']
            field_choices = [(field, field) for field in all_fields]
            return gr.update(
                choices=field_choices,
                value=[],
                interactive=True
            )
        except Exception as e:
            self.logger.error(f'æ¸…é™¤å­—æ®µé€‰æ‹©å¤±è´¥: {e}')
            return gr.update(choices=[], value=[])
    
    def _select_common_dataset_fields(self) -> Dict[str, Any]:
        """é€‰æ‹©å¸¸ç”¨å­—æ®µ"""
        try:
            all_fields = self.current_dataset_cache['fields']
            common_fields = self._identify_common_fields(all_fields)
            field_choices = [(field, field) for field in all_fields]
            return gr.update(
                choices=field_choices,
                value=common_fields,
                interactive=True
            )
        except Exception as e:
            self.logger.error(f'é€‰æ‹©å¸¸ç”¨å­—æ®µå¤±è´¥: {e}')
            return gr.update(choices=[], value=[])

    def _filter_datasets_for_display(self, datasets: list) -> list:
        """
        è¿‡æ»¤æ•°æ®é›†åˆ—è¡¨ï¼ŒæŒ‰æä¾›å•†/æ•°æ®é›†åˆ†å±‚æ˜¾ç¤º
        
        æ˜¾ç¤ºç­–ç•¥ï¼š
        1. æŒ‰æ•°æ®æºæä¾›å•†åˆ†ç»„ï¼ˆhuggingface, modelscopeç­‰ï¼‰
        2. æ¯ä¸ªæä¾›å•†ä¸‹é¢æ˜¾ç¤ºå…·ä½“çš„æ•°æ®é›†
        3. è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„å®é™…ç£ç›˜å¤§å°
        4. è·¯å¾„æŒ‡å‘æ•°æ®é›†ç›®å½•è€Œä¸æ˜¯meta.jsonæ–‡ä»¶
        """
        try:
            # æŒ‰æä¾›å•†å’Œæ•°æ®é›†åˆ†ç»„
            provider_groups = {}
            
            for dataset in datasets:
                file_path = dataset.get('path', '')
                path_norm = (file_path or '').replace('\\', '/')
                
                # æå–æä¾›å•†å’Œæ•°æ®é›†åç§°
                provider = "å…¶ä»–"
                dataset_name = "æœªçŸ¥æ•°æ®é›†"
                dataset_dir = ""
                
                if '/raw/' in path_norm:
                    # è§£æè·¯å¾„ï¼šdata/raw/provider/dataset_name/...
                    parts = path_norm.split('/raw/')
                    if len(parts) > 1:
                        path_parts = parts[1].split('/')
                        if len(path_parts) >= 2:
                            provider = path_parts[0]  # huggingface, modelscopeç­‰
                            dataset_name = path_parts[1]  # å…·ä½“æ•°æ®é›†åç§°
                            # æ„å»ºæ•°æ®é›†ç›®å½•è·¯å¾„
                            dataset_dir = f"{parts[0]}/raw/{provider}/{dataset_name}"
                        elif len(path_parts) == 1:
                            provider = path_parts[0]
                            dataset_name = "æ ¹ç›®å½•æ–‡ä»¶"
                elif '/distilled/' in path_norm:
                    # è’¸é¦æ•°æ®ï¼šè®°å½•è’¸é¦ä¿¡æ¯
                    provider = "è’¸é¦æ•°æ®"  
                    distilled_info = self._extract_distilled_info(file_path)
                    dataset_name = distilled_info['name']
                    # æ„å»ºè’¸é¦æ•°æ®ç›®å½•è·¯å¾„
                    from pathlib import Path
                    dataset_dir = str(Path(file_path).parent)
                elif '/processed/' in path_norm:
                    # å¤„ç†æ•°æ®ï¼šè®°å½•å¤„ç†åŠ¨ä½œå’Œè·¯å¾„
                    provider = "å¤„ç†æ•°æ®"
                    # å°è¯•ä»è·¯å¾„ä¸­æå–å¤„ç†ä¿¡æ¯
                    processed_info = self._extract_processed_info(file_path)
                    dataset_name = processed_info['name']
                    # æ„å»ºå¤„ç†æ•°æ®ç›®å½•è·¯å¾„
                    from pathlib import Path
                    dataset_dir = str(Path(file_path).parent)
                
                # åˆ›å»ºåˆ†ç»„ç»“æ„
                if provider not in provider_groups:
                    provider_groups[provider] = {}
                
                if dataset_name not in provider_groups[provider]:
                    provider_groups[provider][dataset_name] = {
                        'files': [],
                        'dataset_dir': dataset_dir,
                        'total_size': 0,
                        'latest_time': '',
                        'file_count': 0
                    }
                
                group = provider_groups[provider][dataset_name]
                group['files'].append(dataset)
                group['file_count'] += 1
                
                # ç´¯è®¡å¤§å°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if 'size_mb' in dataset:
                    group['total_size'] += dataset.get('size_mb', 0)
                
                # æ›´æ–°æœ€æ–°æ—¶é—´
                file_time = dataset.get('create_time', '')
                if file_time > group['latest_time']:
                    group['latest_time'] = file_time
                
                # å¦‚æœæ²¡æœ‰è®¾ç½®æ•°æ®é›†ç›®å½•ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„çˆ¶ç›®å½•
                if not group['dataset_dir'] and file_path:
                    from pathlib import Path
                    parent_path = str(Path(file_path).parent)
                    parent_norm = parent_path.replace('\\', '/')
                    if '/raw/' in parent_norm:
                        group['dataset_dir'] = parent_path
            
            # æ„å»ºåˆ†å±‚æ˜¾ç¤ºåˆ—è¡¨
            filtered_list = []
            
            for provider, datasets in sorted(provider_groups.items()):
                # æä¾›å•†å±‚çº§æ˜¾ç¤ºè§„åˆ™ï¼š
                # - åŸå§‹æ•°æ®æä¾›å•†ï¼ˆå¦‚ huggingface/modelscopeï¼‰ï¼šå¤šäº1ä¸ªæ•°æ®é›†æˆ–æœ¬å°±åº”åˆ†ç»„
                # - å¤„ç†æ•°æ®/è’¸é¦æ•°æ®ï¼šä¹Ÿæ˜¾ç¤ºé¡¶å±‚åˆ†ç»„ï¼Œä¾¿äºåˆ†ç±»æµè§ˆ
                should_show_provider = (
                    len(datasets) > 1 or provider not in ["å…¶ä»–"] or provider in ["å¤„ç†æ•°æ®", "è’¸é¦æ•°æ®"]
                )
                
                if should_show_provider:
                    provider_total_size = sum(group['total_size'] for group in datasets.values())
                    provider_file_count = sum(group['file_count'] for group in datasets.values())

                    # è®¡ç®—æä¾›å•†åŸºç¡€ç›®å½•
                    if provider in ["å¤„ç†æ•°æ®"]:
                        provider_dir = str(self.root_dir / 'processed')
                    elif provider in ["è’¸é¦æ•°æ®"]:
                        provider_dir = str(self.root_dir / 'distilled')
                    else:
                        # åŸå§‹æ•°æ®çš„å…·ä½“æä¾›å•†ï¼ˆhuggingface/modelscopeç­‰ï¼‰
                        provider_dir = str(self.root_dir / 'raw' / provider)

                    # è®¡ç®—å®é™…å¤§å°
                    provider_actual_size = self._calculate_directory_size(provider_dir)

                    filtered_list.append({
                        'display_name': f"ğŸ“ {provider}",
                        'type': 'æä¾›å•†',
                        'size_mb': provider_actual_size,
                        'create_time': max(group['latest_time'] for group in datasets.values()),
                        'path': provider_dir,
                        'file_count': provider_file_count,
                        'is_provider': True,
                        'provider': provider
                    })
                
                # æ·»åŠ æ•°æ®é›†å±‚çº§
                for dataset_name, group in sorted(datasets.items()):
                    # è®¡ç®—æ•°æ®é›†ç›®å½•çš„å®é™…å¤§å°
                    actual_size = self._calculate_directory_size(group['dataset_dir']) if group['dataset_dir'] else group['total_size']
                    
                    # å¦‚æœæ˜¾ç¤ºäº†æä¾›å•†å±‚çº§ï¼Œæ•°æ®é›†åç§°åŠ ç¼©è¿›
                    display_name = f"  ğŸ“Š {dataset_name}" if should_show_provider else f"ğŸ“Š {dataset_name}"
                    
                    filtered_list.append({
                        'display_name': display_name,
                        'type': 'æ•°æ®é›†',
                        'size_mb': actual_size,
                        'create_time': group['latest_time'],
                        'path': group['dataset_dir'] or (group['files'][0]['path'] if group['files'] else ''),
                        'file_count': group['file_count'],
                        'is_dataset': True,
                        'provider': provider
                    })
            
            return filtered_list
            
        except Exception as e:
            self.logger.error(f'è¿‡æ»¤æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}')
            return datasets  # è¿”å›åŸå§‹åˆ—è¡¨ä½œä¸ºå¤‡ç”¨
    
    def _extract_dataset_name_from_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ•°æ®é›†åç§°"""
        try:
            from pathlib import Path
            path = Path(file_path)
            
            # å»é™¤æ–‡ä»¶åï¼Œè·å–ç›®å½•å
            if path.is_file() or '.' in path.name:
                path = path.parent
            
            # è·å–æœ€æ¥è¿‘çš„æœ‰æ„ä¹‰çš„ç›®å½•å
            parts = path.parts
            for i in reversed(range(len(parts))):
                part = parts[i]
                if part not in ['processed', 'distilled', 'raw', 'data', 'cache', 'organized_files']:
                    return part
            
            return "æœªçŸ¥æ•°æ®é›†"
        except:
            return "æœªçŸ¥æ•°æ®é›†"
    
    def _extract_processed_info(self, file_path: str) -> dict:
        """ä»å¤„ç†æ•°æ®è·¯å¾„ä¸­æå–å¤„ç†ä¿¡æ¯"""
        try:
            from pathlib import Path
            path = Path(file_path)
            
            # è·å–å¤„ç†æ•°æ®çš„ç›®å½•ç»“æ„ä¿¡æ¯
            parts = path.parts
            processed_idx = -1
            for i, part in enumerate(parts):
                if part == 'processed':
                    processed_idx = i
                    break
            
            if processed_idx >= 0 and processed_idx + 1 < len(parts):
                # å¤„ç†ç±»å‹ç›®å½• (å¦‚ cleaned, merged, convertedç­‰)
                process_type = parts[processed_idx + 1]
                
                # æ•°æ®é›†åç§°
                if processed_idx + 2 < len(parts):
                    dataset_name = parts[processed_idx + 2]
                else:
                    dataset_name = process_type
                
                # ç»„åˆæ˜¾ç¤ºåç§°ï¼ŒåŒ…å«å¤„ç†åŠ¨ä½œ
                display_name = f"{dataset_name} ({process_type})"
                
                return {
                    'name': display_name,
                    'process_type': process_type,
                    'dataset_name': dataset_name,
                    'action': self._get_process_action_name(process_type)
                }
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'name': path.stem,
                'process_type': 'æœªçŸ¥å¤„ç†',
                'dataset_name': path.stem,
                'action': 'æ•°æ®å¤„ç†'
            }
            
        except Exception as e:
            return {
                'name': 'æœªçŸ¥å¤„ç†æ•°æ®',
                'process_type': 'æœªçŸ¥',
                'dataset_name': 'æœªçŸ¥',
                'action': 'å¤„ç†'
            }
    
    def _extract_distilled_info(self, file_path: str) -> dict:
        """ä»è’¸é¦æ•°æ®è·¯å¾„ä¸­æå–è’¸é¦ä¿¡æ¯"""
        try:
            from pathlib import Path
            path = Path(file_path)
            
            # è·å–è’¸é¦æ•°æ®çš„ç›®å½•ç»“æ„ä¿¡æ¯
            parts = path.parts
            distilled_idx = -1
            for i, part in enumerate(parts):
                if part == 'distilled':
                    distilled_idx = i
                    break
            
            if distilled_idx >= 0 and distilled_idx + 1 < len(parts):
                # è’¸é¦ä»»åŠ¡ç›®å½•
                task_name = parts[distilled_idx + 1]
                
                # æ•°æ®é›†åç§°
                if distilled_idx + 2 < len(parts):
                    dataset_name = parts[distilled_idx + 2]
                else:
                    dataset_name = task_name
                
                # ç»„åˆæ˜¾ç¤ºåç§°ï¼ŒåŒ…å«è’¸é¦ä¿¡æ¯
                display_name = f"{dataset_name} (è’¸é¦-{task_name})"
                
                return {
                    'name': display_name,
                    'task_name': task_name,
                    'dataset_name': dataset_name,
                    'action': 'æ•°æ®è’¸é¦'
                }
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'name': path.stem,
                'task_name': 'æœªçŸ¥ä»»åŠ¡',
                'dataset_name': path.stem,
                'action': 'æ•°æ®è’¸é¦'
            }
            
        except Exception as e:
            return {
                'name': 'æœªçŸ¥è’¸é¦æ•°æ®',
                'task_name': 'æœªçŸ¥',
                'dataset_name': 'æœªçŸ¥',
                'action': 'è’¸é¦'
            }
    
    def _get_process_action_name(self, process_type: str) -> str:
        """æ ¹æ®å¤„ç†ç±»å‹è·å–å‹å¥½çš„åŠ¨ä½œåç§°"""
        action_map = {
            'cleaned': 'æ•°æ®æ¸…æ´—',
            'merged': 'æ•°æ®åˆå¹¶',
            'converted': 'æ ¼å¼è½¬æ¢',
            'extracted': 'å­—æ®µæå–',
            'filtered': 'æ•°æ®è¿‡æ»¤',
            'augmented': 'æ•°æ®å¢å¼º',
            'normalized': 'æ•°æ®æ ‡å‡†åŒ–',
            'split': 'æ•°æ®åˆ†å‰²'
        }
        return action_map.get(process_type.lower(), f'{process_type}å¤„ç†')
    
    def _calculate_directory_size(self, dir_path: str) -> float:
        """è®¡ç®—ç›®å½•çš„å®é™…ç£ç›˜å¤§å°ï¼ˆMBï¼‰"""
        try:
            from pathlib import Path
            import os
            
            if not dir_path or not os.path.exists(dir_path):
                return 0.0
            
            total_size = 0
            for root, dirs, files in os.walk(dir_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        total_size += os.path.getsize(file_path)
                    except (OSError, IOError):
                        continue  # è·³è¿‡æ— æ³•è®¿é—®çš„æ–‡ä»¶
            
            # è½¬æ¢ä¸ºMB
            return total_size / (1024 * 1024)
        
        except Exception as e:
            self.logger.warning(f'è®¡ç®—ç›®å½•å¤§å°å¤±è´¥ {dir_path}: {e}')
            return 0.0

def create_manage_tab(launcher):
    """åˆ›å»ºæ•°æ®ç®¡ç†æ ‡ç­¾é¡µ"""
    manager = ManageTabManager(launcher)
    manager.create_tab()
    return manager

