#!/usr/bin/env python3
"""
ç½‘é¡µUIå¯åŠ¨å™¨

æœ¬æ¨¡å—åŸºäºGradioå®ç°ç½‘é¡µç•Œé¢ï¼Œæä¾›æ•°æ®é›†ä¸‹è½½ã€æ•°æ®åŠ å·¥ã€æ¨¡å‹é…ç½®ã€è’¸é¦ç”Ÿæˆã€æ•°æ®ç®¡ç†ç­‰åŠŸèƒ½çš„å¯è§†åŒ–æ“ä½œå…¥å£ã€‚
åŠŸèƒ½ç‰¹ç‚¹ï¼š
- 5ä¸ªä¸»è¦æ ‡ç­¾é¡µå¯¹åº”æ ¸å¿ƒåŠŸèƒ½
- å®æ—¶çŠ¶æ€åŒæ­¥å’Œè¿›åº¦æ˜¾ç¤º
- å‹å¥½çš„ç”¨æˆ·äº¤äº’ä½“éªŒ
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæç¤º

è®¾è®¡åŸåˆ™ï¼š
- çº¯UIå±‚ï¼Œä¸åŒ…å«ä¸šåŠ¡é€»è¾‘
- é€šè¿‡å‡½æ•°æ¥å£è°ƒç”¨æ ¸å¿ƒæ¨¡å—
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œç”¨æˆ·æç¤º
- å“åº”å¼è®¾è®¡å’ŒçŠ¶æ€ç®¡ç†

ä½œè€…ï¼šè‡ªåŠ¨æ•°æ®è’¸é¦è½¯ä»¶å›¢é˜Ÿ
ç‰ˆæœ¬ï¼šv1.0
è®¸å¯ï¼šå•†ä¸šè½¯ä»¶
"""

import os
import json
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# æ·»åŠ é€šç”¨å­—æ®µæå–å™¨
try:
    from .universal_field_extractor import extract_fields_universal, get_field_names_universal
except Exception:
    from universal_field_extractor import extract_fields_universal, get_field_names_universal  # å…œåº•
import gradio as gr
import pandas as pd

# åŸºç¡€æ”¯æ’‘å±‚å¯¼å…¥
try:
    # ä½œä¸ºæ¨¡å—å¯¼å…¥æ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from .config_manager import config_manager
    from .log_manager import log_manager
    from .state_manager import state_manager, TaskType
    from .utils import FileOperations
    # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—å¯¼å…¥ - ä½¿ç”¨åŸå§‹å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬
    from .dataset_downloader import DatasetDownloader
    from .format_converter import FormatConverter, convert_format, start_convert, get_convert_progress, list_converts
    from .field_extractor import FieldExtractor, get_fields, extract_fields
    from .data_merger import DataMerger, merge_data
    from .model_manager import model_manager
    from .distill_generator import distill_generator
    from .data_manager import data_manager
    from .data_cleaner import data_cleaner
    from .dataset_previewer import DatasetPreviewer, PreviewConfig
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from config_manager import config_manager
    from log_manager import log_manager
    from state_manager import state_manager, TaskType
    from utils import FileOperations
    # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—å¯¼å…¥ - ä½¿ç”¨åŸå§‹å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬
    from dataset_downloader import DatasetDownloader
    from format_converter import FormatConverter, convert_format, start_convert, get_convert_progress, list_converts
    from field_extractor import FieldExtractor, get_fields, extract_fields
    from data_merger import DataMerger, merge_data
    from model_manager import model_manager
    from distill_generator import distill_generator
    from data_manager import data_manager
    from data_cleaner import data_cleaner
    from dataset_previewer import DatasetPreviewer, PreviewConfig


class UILauncher:
    """
    UIå¯åŠ¨å™¨ç±»
    
    è´Ÿè´£æ„å»ºGradioç•Œé¢ï¼Œå¯¹æ¥æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–UIå¯åŠ¨å™¨"""
        self.logger = log_manager.get_logger('ui_launcher')
        
        # è·å–é…ç½®
        self.root_dir = Path(config_manager.get_config('base.root_dir', './data'))
        self.update_interval = 2  # çŠ¶æ€æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
        
        # åˆå§‹åŒ–æ ¸å¿ƒåŠŸèƒ½æ¨¡å—å®ä¾‹
        self.dataset_downloader = DatasetDownloader()
        self.format_converter = FormatConverter()
        self.field_extractor = FieldExtractor()
        # åˆå§‹åŒ–å­—æ®µæå–å™¨
        self.field_extractor.init_extractor()
        self.data_merger = DataMerger()
        # model_manager, distill_generator, data_manager, data_cleaner å·²ç»æ˜¯å®ä¾‹
        
        # åˆå§‹åŒ–æ•°æ®é¢„è§ˆå™¨
        preview_config = PreviewConfig(
            max_rows=100,
            max_files=10,
            max_text_length=300,
            max_file_size_mb=500,
            enable_truncation=True,
            show_stats=True,
            include_metadata=True,
            smart_columns=True,
            show_all_columns=False
        )
        self.dataset_previewer = DatasetPreviewer(preview_config)
        
        # ç•Œé¢ç»„ä»¶å­˜å‚¨
        self.components = {}
        
        # çŠ¶æ€ç®¡ç†
        self.merge_file_paths = []  # å­˜å‚¨å¾…åˆå¹¶çš„æ–‡ä»¶è·¯å¾„
        self.running_tasks = {}
        self.last_update = 0
        
        # å¤šé€‰ä»»åŠ¡çŠ¶æ€
        self.selected_tasks = set()  # å­˜å‚¨å·²é€‰æ‹©çš„ä»»åŠ¡ID
        
        self.logger.info('UIå¯åŠ¨å™¨åˆå§‹åŒ–å®Œæˆ')
    
    def _get_saved_token(self, platform: str) -> str:
        """
        è·å–ä¿å­˜çš„token
        
        Args:
            platform: å¹³å°åç§° ('huggingface' æˆ– 'modelscope')
            
        Returns:
            str: ä¿å­˜çš„tokenï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            config_key = f'tokens.{platform}'
            return config_manager.get_config(config_key, '')
        except Exception as e:
            self.logger.warning(f'è·å–{platform} tokenå¤±è´¥: {e}')
            return ''
    
    def _save_token(self, platform: str, token: str):
        """
        ä¿å­˜tokenåˆ°é…ç½®
        
        Args:
            platform: å¹³å°åç§° ('huggingface' æˆ– 'modelscope')
            token: tokenå€¼
        """
        try:
            if token.strip():
                config_key = f'tokens.{platform}'
                config_manager.set_config(config_key, token.strip())
                self.logger.info(f'{platform} tokenå·²ä¿å­˜åˆ°é…ç½®')
            else:
                # å¦‚æœtokenä¸ºç©ºï¼Œåˆ é™¤é…ç½®é¡¹
                config_key = f'tokens.{platform}'
                config_manager.set_config(config_key, '')
        except Exception as e:
            self.logger.error(f'ä¿å­˜{platform} tokenå¤±è´¥: {e}')
    
    def launch(self, share: bool = False, server_port: int = 7860):
        """
        å¯åŠ¨Gradioç•Œé¢
        
        Args:
            share (bool): æ˜¯å¦åˆ›å»ºå…¬å…±é“¾æ¥
            server_port (int): æœåŠ¡å™¨ç«¯å£
        """
        # åˆ›å»ºGradioç•Œé¢
        with gr.Blocks(
            title="è‡ªåŠ¨æ•°æ®è’¸é¦è½¯ä»¶",
            theme=gr.themes.Soft(),
            css=self._get_custom_css()
        ) as demo:
            
            gr.Markdown("# ğŸ¤– è‡ªåŠ¨æ•°æ®è’¸é¦è½¯ä»¶")
            gr.Markdown("*ä¸€ç«™å¼æ•°æ®é›†å¤„ç†ä¸AIæ¨¡å‹è’¸é¦å¹³å°*")
            
            # åˆ›å»ºæ ‡ç­¾é¡µ
            with gr.Tabs():
                # æ ‡ç­¾é¡µ1ï¼šæ•°æ®é›†ä¸‹è½½
                with gr.TabItem("ğŸ“¥ æ•°æ®é›†ä¸‹è½½", id="download"):
                    self._create_download_tab()
                
                # æ ‡ç­¾é¡µ2ï¼šæ•°æ®åŠ å·¥
                with gr.TabItem("ğŸ”§ æ•°æ®åŠ å·¥", id="process"):
                    self._create_process_tab()
                
                # æ ‡ç­¾é¡µ3ï¼šæ¨¡å‹é…ç½®
                with gr.TabItem("âš™ï¸ æ¨¡å‹é…ç½®", id="model"):
                    self._create_model_tab()
                
                # æ ‡ç­¾é¡µ4ï¼šè’¸é¦ç”Ÿæˆ
                with gr.TabItem("ğŸ§  è’¸é¦ç”Ÿæˆ", id="distill"):
                    self._create_distill_tab()
                
                # æ ‡ç­¾é¡µ5ï¼šæ•°æ®ç®¡ç†
                with gr.TabItem("ğŸ“Š æ•°æ®ç®¡ç†", id="manage"):
                    self._create_manage_tab()
            
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡æ›´æ–°çŠ¶æ€
            self._setup_auto_refresh(demo)
        
        # å¯åŠ¨æœåŠ¡å™¨
        self.logger.info(f'å¯åŠ¨GradioæœåŠ¡å™¨ï¼Œç«¯å£: {server_port}')
        demo.launch(
            share=share,
            server_port=server_port,
            server_name="0.0.0.0",
            show_error=True
        )
    
    def _get_custom_css(self) -> str:
        """è·å–è‡ªå®šä¹‰CSSæ ·å¼"""
        return """
        .status-running { background-color: #e3f2fd !important; }
        .status-completed { background-color: #e8f5e8 !important; }
        .status-failed { background-color: #ffebee !important; }
        .progress-bar { margin: 10px 0; }
        .task-item { 
            border: 1px solid #ddd; 
            border-radius: 8px; 
            padding: 10px; 
            margin: 5px 0; 
        }
        
        /* æ”¹å–„DataFrameè¡¨æ ¼æ˜¾ç¤º */
        .dataframe {
            overflow-x: auto !important;
            font-size: 14px !important;
            max-height: 600px !important;
            overflow-y: auto !important;
        }
        
        .dataframe table {
            table-layout: fixed !important;
            width: 100% !important;
            border-collapse: collapse !important;
        }
        
        .dataframe td, .dataframe th {
            padding: 8px 12px !important;
            text-align: left !important;
            border: 1px solid #dee2e6 !important;
            word-wrap: break-word !important;
            vertical-align: top !important;
            position: relative !important;
            overflow: hidden !important;
            text-overflow: ellipsis !important;
            white-space: nowrap !important;
        }
        
        /* æ™ºèƒ½åˆ—å®½åˆ†é… - å…¼å®¹æ€§æ›´å¥½çš„å†™æ³• */
        .dataframe colgroup col {
            width: auto !important;
        }
        
        /* JavaScriptåŠ¨æ€è®¾ç½®åˆ—å®½çš„é»˜è®¤è§„åˆ™ */
        .dataframe td, .dataframe th {
            min-width: 100px !important;
            max-width: 400px !important;
        }
        
        /* é¼ æ ‡æ‚¬åœæ—¶æ˜¾ç¤ºå®Œæ•´å†…å®¹ */
        .dataframe td:hover {
            white-space: normal !important;
            overflow: visible !important;
            max-width: none !important;
            z-index: 999 !important;
            background-color: #f8f9fa !important;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15) !important;
        }
        
        /* åŠ¨æ€åˆ—å®½è°ƒæ•´ */
        .equal-width-table td, .equal-width-table th {
            width: auto !important;
        }
        </style>
        
        <script>
        // åŠ¨æ€è°ƒæ•´è¡¨æ ¼åˆ—å®½ - æ™ºèƒ½åˆ†é…
        function adjustTableColumns() {
            const tables = document.querySelectorAll('.dataframe table');
            tables.forEach(table => {
                const rows = table.querySelectorAll('tr');
                if (rows.length === 0) return;
                
                const firstRow = rows[0];
                const cells = firstRow.querySelectorAll('th, td');
                const columnCount = cells.length;
                
                if (columnCount > 0) {
                    // æ™ºèƒ½åˆ†é…åˆ—å®½
                    let widths = [];
                    if (columnCount === 2) {
                        // ä¸¤åˆ—æ—¶ï¼Œå¹³å‡åˆ†é…ï¼Œç¨å¾®å€¾æ–œå‘ç¬¬äºŒåˆ—
                        widths = [45, 55];
                    } else if (columnCount === 3) {
                        // ä¸‰åˆ—æ—¶ï¼Œä¸­é—´åˆ—ç¨å¤§
                        widths = [30, 40, 30];
                    } else if (columnCount >= 4) {
                        // å¤šåˆ—æ—¶ï¼Œå‡åŒ€åˆ†é…ï¼Œä½†ç¡®ä¿æœ€å°å®½åº¦
                        const baseWidth = Math.max(Math.floor(100 / columnCount), 15);
                        widths = Array(columnCount).fill(baseWidth);
                        // è°ƒæ•´æ€»å’Œä¸º100%
                        const totalWidth = widths.reduce((sum, w) => sum + w, 0);
                        if (totalWidth !== 100) {
                            widths[widths.length - 1] += 100 - totalWidth;
                        }
                    } else {
                        // é»˜è®¤å‡åŒ€åˆ†é…
                        const width = Math.floor(100 / columnCount);
                        widths = Array(columnCount).fill(width);
                    }
                    
                    // åº”ç”¨åˆ—å®½
                    cells.forEach((cell, index) => {
                        if (index < widths.length) {
                            cell.style.width = widths[index] + '%';
                            cell.style.minWidth = '150px'; // è®¾ç½®æœ€å°å®½åº¦
                        }
                    });
                    
                    // ä¸ºæ‰€æœ‰è¡Œåº”ç”¨ç›¸åŒçš„åˆ—å®½
                    rows.forEach(row => {
                        const rowCells = row.querySelectorAll('th, td');
                        rowCells.forEach((cell, index) => {
                            if (index < widths.length) {
                                cell.style.width = widths[index] + '%';
                                cell.style.minWidth = '150px';
                            }
                        });
                    });
                }
            });
        }
        
        // ç›‘å¬DOMå˜åŒ–ï¼Œè‡ªåŠ¨è°ƒæ•´è¡¨æ ¼
        const observer = new MutationObserver(function(mutations) {
            mutations.forEach(function(mutation) {
                if (mutation.type === 'childList') {
                    // å»¶è¿Ÿæ‰§è¡Œï¼Œç¡®ä¿DOMå®Œå…¨æ›´æ–°
                    setTimeout(adjustTableColumns, 100);
                }
            });
        });
        
        // å¼€å§‹ç›‘å¬
        observer.observe(document.body, {
            childList: true,
            subtree: true
        });
        
        // é¡µé¢åŠ è½½å®Œæˆåè°ƒæ•´ä¸€æ¬¡
        document.addEventListener('DOMContentLoaded', adjustTableColumns);
        
        // åœ¨æ¸²æŸ“åä¹Ÿè°ƒæ•´ä¸€æ¬¡
        setTimeout(adjustTableColumns, 500);
        </script>
        
        /* æ‚¬åœæ˜¾ç¤ºå®Œæ•´å†…å®¹ */
        .dataframe td:hover {
            overflow: visible !important;
            white-space: normal !important;
            position: relative !important;
            background-color: #fff3cd !important;
            z-index: 10 !important;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1) !important;
            border: 2px solid #ffc107 !important;
            max-width: none !important;
            word-break: break-word !important;
        }
        
        /* æ”¹å–„åˆ—æ ‡é¢˜æ ·å¼ */
        .dataframe th {
            background-color: #e9ecef !important;
            font-weight: bold !important;
            border-bottom: 2px solid #dee2e6 !important;
            position: sticky !important;
            top: 0 !important;
            z-index: 5 !important;
            text-align: center !important;
        }
        
        /* éšè—ç©ºå€¼å•å…ƒæ ¼çš„ç‰¹æ®Šæ ·å¼ */
        .dataframe td:empty,
        .dataframe td[data-value=""],
        .dataframe td[data-value="null"],
        .dataframe td[data-value="None"] {
            background-color: #f8f9fa !important;
            opacity: 0.5 !important;
        }
        
        /* æˆªæ–­æ–‡æœ¬çš„æŒ‡ç¤º */
        .dataframe td[title]:after {
            content: "..." !important;
            color: #6c757d !important;
            font-weight: bold !important;
        }
        
        /* é’ˆå¯¹ä¸åŒåˆ—è®¾ç½®ä¸åŒçš„æœ€å°å®½åº¦ */
        .dataframe th:nth-child(1), .dataframe td:nth-child(1) {
            min-width: 120px !important; /* åç§°åˆ— */
        }
        .dataframe th:nth-child(2), .dataframe td:nth-child(2) {
            min-width: 80px !important;  /* ç±»å‹åˆ— */
        }
        .dataframe th:nth-child(3), .dataframe td:nth-child(3) {
            min-width: 80px !important;  /* å¤§å°åˆ— */
        }
        .dataframe th:nth-child(4), .dataframe td:nth-child(4) {
            min-width: 140px !important; /* æ—¶é—´åˆ— */
        }
        .dataframe th:nth-child(5), .dataframe td:nth-child(5) {
            min-width: 200px !important; /* è·¯å¾„åˆ— */
            max-width: 300px !important;
            overflow: hidden !important;
            text-overflow: ellipsis !important;
        }
        
        /* æ‚¬åœæ˜¾ç¤ºå®Œæ•´å†…å®¹ */
        .dataframe td:hover {
            overflow: visible !important;
            white-space: normal !important;
            position: relative !important;
            background-color: #f8f9fa !important;
            z-index: 10 !important;
        }
        
        /* æ”¹å–„åˆ—æ ‡é¢˜æ ·å¼ */
        .dataframe th {
            background-color: #f8f9fa !important;
            font-weight: bold !important;
            border-bottom: 2px solid #dee2e6 !important;
            position: sticky !important;
            top: 0 !important;
            z-index: 5 !important;
        }
        
        /* æ·»åŠ è¡¨æ ¼å®¹å™¨çš„æ»šåŠ¨æç¤º */
        .dataframe::after {
            content: "æç¤ºï¼šè¡¨æ ¼å¯ä»¥æ°´å¹³æ»šåŠ¨" !important;
            display: block !important;
            font-size: 12px !important;
            color: #6c757d !important;
            text-align: center !important;
            margin-top: 5px !important;
        }
        
        /* é€‰ä¸­è¡Œçš„æ ·å¼ */
        .dataframe tr:hover {
            background-color: #e9ecef !important;
        }
        """
    
    def _create_download_tab(self):
        """åˆ›å»ºæ•°æ®é›†ä¸‹è½½æ ‡ç­¾é¡µ"""
        gr.Markdown("## æ•°æ®é›†ä¸‹è½½ç®¡ç†")
        gr.Markdown("æ”¯æŒä» Hugging Faceã€ModelScope ç­‰å¹³å°ä¸‹è½½æ•°æ®é›†")
        
        with gr.Row():
            with gr.Column(scale=1):
                # ä¸‹è½½é…ç½®åŒºåŸŸ
                gr.Markdown("### ğŸ“ ä¸‹è½½é…ç½®")
                
                source_type = gr.Dropdown(
                    choices=["huggingface", "modelscope", "url"],
                    value="huggingface",
                    label="æ•°æ®æºç±»å‹",
                    info="é€‰æ‹©æ•°æ®é›†æ¥æºå¹³å°"
                )
                
                dataset_name = gr.Textbox(
                    label="æ•°æ®é›†åç§°/URL",
                    placeholder="ä¾‹å¦‚ï¼šsquad æˆ– https://example.com/data.zip",
                    info="è¾“å…¥æ•°æ®é›†åç§°æˆ–ä¸‹è½½é“¾æ¥"
                )
                
                # åˆ†åˆ«é…ç½®ä¸åŒå¹³å°çš„APIå¯†é’¥
                with gr.Row():
                    huggingface_token = gr.Textbox(
                        label="ğŸ¤— Hugging Face Token",
                        type="password",
                        placeholder="è¾“å…¥Hugging Faceè®¿é—®token",
                        info="ç”¨äºè®¿é—®ç§æœ‰Hugging Faceæ•°æ®é›†",
                        value=self._get_saved_token('huggingface')
                    )
                    
                    modelscope_token = gr.Textbox(
                        label="ğŸ¯ ModelScope Token", 
                        type="password",
                        placeholder="è¾“å…¥ModelScopeè®¿é—®token",
                        info="ç”¨äºè®¿é—®ç§æœ‰ModelScopeæ•°æ®é›†",
                        value=self._get_saved_token('modelscope')
                    )
                
                save_dir = gr.Textbox(
                    label="ä¿å­˜ç›®å½•",
                    value=str(self.root_dir / "raw"),
                    info="æ•°æ®é›†ä¿å­˜è·¯å¾„"
                )
                
                with gr.Row():
                    add_task_btn = gr.Button("â• æ·»åŠ ä¸‹è½½ä»»åŠ¡", variant="primary")
                    refresh_status_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", variant="secondary")
            
            with gr.Column(scale=2):
                # ä»»åŠ¡åˆ—è¡¨åŒºåŸŸ
                gr.Markdown("### ğŸ“‹ ä¸‹è½½ä»»åŠ¡åˆ—è¡¨")
                
                # ä»»åŠ¡ä¿¡æ¯æ˜¾ç¤ºè¡¨æ ¼ï¼ˆä»…æ˜¾ç¤ºï¼Œä¸å¯é€‰æ‹©ï¼‰
                task_list = gr.Dataframe(
                    headers=["ä»»åŠ¡ID", "æ•°æ®é›†åç§°", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"],
                    datatype=["str", "str", "str", "str", "str"],
                    label="",
                    interactive=False,
                    wrap=True
                )
                
                # ä»»åŠ¡é€‰æ‹©åŒºåŸŸ
                with gr.Row():
                    with gr.Column(scale=3):
                        gr.Markdown("**ğŸ¯ å¤šä»»åŠ¡é€‰æ‹©**")
                        task_selector = gr.CheckboxGroup(
                            label="é€‰æ‹©è¦æ“ä½œçš„ä»»åŠ¡",
                            choices=[],
                            value=[],
                            interactive=True
                        )
                    with gr.Column(scale=1):
                        # å•ä»»åŠ¡æ“ä½œï¼ˆä»é€‰ä¸­çš„ä»»åŠ¡ä¸­é€‰ç¬¬ä¸€ä¸ªï¼‰
                        gr.Markdown("**å•ä»»åŠ¡æ“ä½œ**")
                        with gr.Row():
                            start_single_btn = gr.Button("â–¶ï¸ å¼€å§‹", size="sm")
                            pause_single_btn = gr.Button("â¸ï¸ æš‚åœ", size="sm")
                            delete_single_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤", size="sm", variant="stop")
                
                with gr.Row():
                    # æ‰¹é‡æ“ä½œæŒ‰é’®
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm")
                    start_all_btn = gr.Button("â–¶ï¸ æ‰¹é‡å¼€å§‹", size="sm", variant="primary")
                    pause_all_btn = gr.Button("â¸ï¸ æ‰¹é‡æš‚åœ", size="sm") 
                    delete_all_btn = gr.Button("ğŸ—‘ï¸ æ‰¹é‡åˆ é™¤", size="sm", variant="stop")
                
                # åˆ é™¤é€‰é¡¹
                with gr.Row():
                    delete_files_checkbox = gr.Checkbox(
                        label="åˆ é™¤ä»»åŠ¡æ—¶åŒæ—¶åˆ é™¤æœ¬åœ°æ–‡ä»¶",
                        value=False,  # é»˜è®¤ä¸åˆ é™¤æ–‡ä»¶
                        info="å‹¾é€‰åå°†åŒæ—¶åˆ é™¤å·²ä¸‹è½½çš„æ•°æ®é›†æ–‡ä»¶"
                    )
        
        # çŠ¶æ€è¾“å‡ºåŒºåŸŸ
        with gr.Row():
            download_status = gr.Textbox(
                label="çŠ¶æ€ä¿¡æ¯",
                lines=3,
                interactive=False,
                show_copy_button=True
            )
        
        # æ·»åŠ éšè—çš„å®šæ—¶å™¨ï¼Œç”¨äºè‡ªåŠ¨åˆ·æ–°ä»»åŠ¡åˆ—è¡¨
        with gr.Row(visible=False):
            auto_refresh_timer = gr.Number(value=0, label="å®šæ—¶å™¨", visible=False)
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.components['download'] = {
            'source_type': source_type,
            'dataset_name': dataset_name,
            'huggingface_token': huggingface_token,
            'modelscope_token': modelscope_token,
            'save_dir': save_dir,
            'task_list': task_list,
            'task_selector': task_selector,
            'delete_files_checkbox': delete_files_checkbox,
            'status': download_status,
            'auto_refresh_timer': auto_refresh_timer
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        add_task_btn.click(
            fn=self._add_download_task,
            inputs=[source_type, dataset_name, huggingface_token, modelscope_token, save_dir],
            outputs=[download_status, task_list, task_selector]
        )
        
        # å·¦ä¾§åˆ·æ–°ï¼ˆé…ç½®åŒºï¼‰
        refresh_status_btn.click(
            fn=self._refresh_download_tasks_and_selector,
            outputs=[task_list, task_selector]
        )
        
        # å³ä¾§åˆ·æ–°ï¼ˆä»»åŠ¡åŒºï¼‰
        try:
            refresh_btn.click(
                fn=self._refresh_download_tasks_and_selector,
                outputs=[task_list, task_selector]
            )
        except Exception:
            pass
        
        # å•ä»»åŠ¡æ“ä½œ
        start_single_btn.click(
            fn=self._start_single_task,
            inputs=[task_selector],
            outputs=[download_status, task_list, task_selector]
        )
        
        pause_single_btn.click(
            fn=self._pause_single_task,
            inputs=[task_selector],
            outputs=[download_status, task_list, task_selector]
        )
        
        delete_single_btn.click(
            fn=self._delete_single_task,
            inputs=[task_selector, delete_files_checkbox],
            outputs=[download_status, task_list, task_selector]
        )
        
        # æ‰¹é‡æ“ä½œ
        start_all_btn.click(
            fn=self._start_multiple_tasks_new,
            inputs=[task_selector],
            outputs=[download_status, task_list, task_selector]
        )
        
        pause_all_btn.click(
            fn=self._pause_multiple_tasks,
            inputs=[task_selector],
            outputs=[download_status, task_list, task_selector]
        )
        
        delete_all_btn.click(
            fn=self._delete_multiple_tasks,
            inputs=[task_selector, delete_files_checkbox],
            outputs=[download_status, task_list, task_selector]
        )
        
        # Tokenè‡ªåŠ¨ä¿å­˜äº‹ä»¶å¤„ç†å™¨
        huggingface_token.change(
            fn=lambda token: self._save_token('huggingface', token) if token.strip() else None,
            inputs=[huggingface_token],
            outputs=[]
        )
        
        modelscope_token.change(
            fn=lambda token: self._save_token('modelscope', token) if token.strip() else None,
            inputs=[modelscope_token],
            outputs=[]
        )
    
    def _add_download_task(self, source_type: str, dataset_name: str, 
                          huggingface_token: str, modelscope_token: str, save_dir: str) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """æ·»åŠ ä¸‹è½½ä»»åŠ¡"""
        try:
            if not dataset_name.strip():
                return "âŒ è¯·è¾“å…¥æ•°æ®é›†åç§°æˆ–URL", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            # æ„å»ºä¸‹è½½å‚æ•°
            params = {
                'source_type': (source_type or '').strip().lower(),
                'dataset_name': dataset_name.strip(),
                'save_dir': (save_dir.strip() if save_dir else str(self.root_dir / "raw"))
            }
            
            # æ ¹æ®source_typeé€‰æ‹©åˆé€‚çš„tokenå¹¶ä¿å­˜
            if params['source_type'] == 'huggingface' and huggingface_token.strip():
                params['token'] = huggingface_token.strip()
                self._save_token('huggingface', huggingface_token.strip())
            elif params['source_type'] == 'modelscope' and modelscope_token.strip():
                params['token'] = modelscope_token.strip()
                self._save_token('modelscope', modelscope_token.strip())
            
            # è°ƒç”¨æ ¸å¿ƒæ¨¡å—æ·»åŠ ä»»åŠ¡ï¼ˆè§£åŒ…å‚æ•°ï¼‰
            task_id = self.dataset_downloader.add_download_task(**params)
            
            return f"âœ… ä¸‹è½½ä»»åŠ¡å·²æ·»åŠ : {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
            
        except Exception as e:
            self.logger.error(f'æ·»åŠ ä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æ·»åŠ ä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _get_task_selector_choices(self) -> gr.CheckboxGroup:
        """è·å–ä»»åŠ¡é€‰æ‹©å™¨çš„é€‰æ‹©åˆ—è¡¨"""
        try:
            tasks = self.dataset_downloader.list_tasks()
            choices = []
            for task in tasks:
                task_id = task.get('task_id', '')
                params = task.get('params', {})
                dataset_name = params.get('dataset_name', '')
                # æ ¼å¼ï¼šä»»åŠ¡ID - æ•°æ®é›†åç§°
                choice_label = f"{task_id} - {dataset_name}"
                choices.append((choice_label, task_id))
            
            return gr.CheckboxGroup(
                label="é€‰æ‹©è¦æ“ä½œçš„ä»»åŠ¡",
                choices=choices,
                value=[],
                interactive=True
            )
        except Exception as e:
            self.logger.error(f'è·å–ä»»åŠ¡é€‰æ‹©å™¨å¤±è´¥: {e}')
            return gr.CheckboxGroup(
                label="é€‰æ‹©è¦æ“ä½œçš„ä»»åŠ¡",
                choices=[],
                value=[],
                interactive=True
            )
    
    def _refresh_download_tasks_and_selector(self) -> Tuple[pd.DataFrame, gr.CheckboxGroup]:
        """åˆ·æ–°ä¸‹è½½ä»»åŠ¡åˆ—è¡¨å’Œä»»åŠ¡é€‰æ‹©å™¨"""
        return self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _refresh_download_tasks(self) -> pd.DataFrame:
        """åˆ·æ–°ä¸‹è½½ä»»åŠ¡åˆ—è¡¨"""
        return self._get_download_tasks_df()
    
    def _start_download_task(self, task_id: str) -> Tuple[str, pd.DataFrame]:
        """å¼€å§‹ä¸‹è½½ä»»åŠ¡"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦å¼€å§‹çš„ä»»åŠ¡", self._get_download_tasks_df()
            
            # UIæ¨¡å¼ï¼šä½¿ç”¨å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ç•Œé¢
            success = self.dataset_downloader.start_task(task_id.strip(), async_mode=True)
            
            if success:
                return f"âœ… ä»»åŠ¡å·²å¼€å§‹: {task_id}", self._get_download_tasks_df()
            else:
                return f"âŒ ä»»åŠ¡å¼€å§‹å¤±è´¥: {task_id}", self._get_download_tasks_df()
                
        except Exception as e:
            self.logger.error(f'å¼€å§‹ä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ å¼€å§‹ä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df()
    
    def _start_multiple_tasks(self, task_ids_str: str) -> Tuple[str, pd.DataFrame]:
        """å¼€å§‹å¤šä¸ªä¸‹è½½ä»»åŠ¡"""
        try:
            self.logger.info(f"æ‰¹é‡å¯åŠ¨è¯·æ±‚ï¼Œè¾“å…¥: '{task_ids_str}'")
            
            # ä¼˜å…ˆä½¿ç”¨å·²é€‰ä»»åŠ¡åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è§£æè¾“å…¥å­—ç¬¦ä¸²
            if self.selected_tasks:
                task_ids = list(self.selected_tasks)
                self.logger.info(f"ä½¿ç”¨å·²é€‰ä»»åŠ¡åˆ—è¡¨: {task_ids}")
            elif task_ids_str.strip():
                # è§£æè¾“å…¥çš„ä»»åŠ¡IDåˆ—è¡¨
                task_ids = [tid.strip() for tid in task_ids_str.split(',') if tid.strip()]
                self.logger.info(f"è§£æè¾“å…¥ä»»åŠ¡IDåˆ—è¡¨: {task_ids}")
            else:
                # å¦‚æœè¾“å…¥ä¸ºç©ºä¸”æ²¡æœ‰é€‰ä¸­ä»»åŠ¡ï¼Œè¿”å›æç¤º
                current_tasks = self.dataset_downloader.list_tasks()
                if current_tasks:
                    return "âŒ è¯·å…ˆç‚¹å‡»ä»»åŠ¡è¡Œé€‰æ‹©è¦å¯åŠ¨çš„ä»»åŠ¡ï¼Œæˆ–åœ¨è¾“å…¥æ¡†ä¸­æ‰‹åŠ¨è¾“å…¥ä»»åŠ¡ID", self._get_download_tasks_df()
                else:
                    return "âŒ æ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡", self._get_download_tasks_df()
            
            if not task_ids:
                return "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„ä»»åŠ¡IDæ ¼å¼ï¼štask1,task2,task3", self._get_download_tasks_df()
            
            # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            available_tasks = {task['task_id'] for task in self.dataset_downloader.list_tasks()}
            valid_tasks = []
            invalid_tasks = []
            
            for task_id in task_ids:
                if task_id in available_tasks:
                    valid_tasks.append(task_id)
                else:
                    invalid_tasks.append(task_id)
            
            if invalid_tasks:
                return f"âŒ ä»¥ä¸‹ä»»åŠ¡IDä¸å­˜åœ¨: {', '.join(invalid_tasks)}\nå¯ç”¨ä»»åŠ¡: {', '.join(available_tasks)}", self._get_download_tasks_df()
            
            success_count = 0
            failed_count = 0
            results = []
            
            for task_id in valid_tasks:
                try:
                    # UIæ¨¡å¼ï¼šä½¿ç”¨å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ç•Œé¢
                    success = self.dataset_downloader.start_task(task_id, async_mode=True)
                    if success:
                        success_count += 1
                        results.append(f"âœ… {task_id}")
                    else:
                        failed_count += 1
                        results.append(f"âŒ {task_id}")
                except Exception as e:
                    failed_count += 1
                    results.append(f"âŒ {task_id}: {str(e)}")
            
            summary = f"æ‰¹é‡å¯åŠ¨å®Œæˆ: {success_count}ä¸ªæˆåŠŸ, {failed_count}ä¸ªå¤±è´¥"
            details = "\n".join(results)
            
            return f"{summary}\n\nè¯¦æƒ…:\n{details}", self._get_download_tasks_df()
                
        except Exception as e:
            self.logger.error(f'æ‰¹é‡å¯åŠ¨ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æ‰¹é‡å¯åŠ¨å¤±è´¥: {str(e)}", self._get_download_tasks_df()
    
    def _pause_download_task(self, task_id: str) -> Tuple[str, pd.DataFrame]:
        """æš‚åœä¸‹è½½ä»»åŠ¡"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦æš‚åœçš„ä»»åŠ¡", self._get_download_tasks_df()
            
            success = self.dataset_downloader.pause_task(task_id.strip())
            
            if success:
                return f"âœ… ä»»åŠ¡å·²æš‚åœ: {task_id}", self._get_download_tasks_df()
            else:
                return f"âŒ ä»»åŠ¡æš‚åœå¤±è´¥: {task_id}", self._get_download_tasks_df()
                
        except Exception as e:
            self.logger.error(f'æš‚åœä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æš‚åœä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df()
    
    def _delete_download_task(self, task_id: str) -> Tuple[str, pd.DataFrame]:
        """åˆ é™¤ä¸‹è½½ä»»åŠ¡"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡", self._get_download_tasks_df()
            
            success = self.dataset_downloader.delete_task(task_id.strip())
            
            if success:
                return f"âœ… ä»»åŠ¡å·²åˆ é™¤: {task_id}", self._get_download_tasks_df()
            else:
                return f"âŒ ä»»åŠ¡åˆ é™¤å¤±è´¥: {task_id}", self._get_download_tasks_df()
                
        except Exception as e:
            self.logger.error(f'åˆ é™¤ä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df()
    
    def _toggle_task_selection(self, evt: gr.SelectData) -> Tuple[str, str]:
        """åˆ‡æ¢ä»»åŠ¡é€‰æ‹©çŠ¶æ€ï¼ˆæ·»åŠ æˆ–ç§»é™¤ï¼‰"""
        # è·å–å½“å‰æ•°æ®æ¡†
        tasks = self.dataset_downloader.list_tasks()
        if not tasks or evt.index[0] >= len(tasks):
            return "", self._format_selected_tasks()
        
        # è·å–é€‰ä¸­çš„ä»»åŠ¡ID
        task = tasks[evt.index[0]]
        task_id = task.get('task_id', '')
        
        if not task_id:
            return "", self._format_selected_tasks()
        
        # åˆ‡æ¢é€‰æ‹©çŠ¶æ€
        if task_id in self.selected_tasks:
            self.selected_tasks.remove(task_id)
            self.logger.info(f'ä»å¤šé€‰åˆ—è¡¨ä¸­ç§»é™¤ä»»åŠ¡: {task_id}')
        else:
            self.selected_tasks.add(task_id)
            self.logger.info(f'æ·»åŠ ä»»åŠ¡åˆ°å¤šé€‰åˆ—è¡¨: {task_id}')
        
        return task_id, self._format_selected_tasks()
    
    def _clear_task_selection(self) -> str:
        """æ¸…ç©ºä»»åŠ¡é€‰æ‹©"""
        self.selected_tasks.clear()
        self.logger.info('æ¸…ç©ºå¤šé€‰ä»»åŠ¡åˆ—è¡¨')
        return ""
    
    def _format_selected_tasks(self) -> str:
        """æ ¼å¼åŒ–å·²é€‰ä»»åŠ¡åˆ—è¡¨æ˜¾ç¤º"""
        if not self.selected_tasks:
            return ""
        return ", ".join(sorted(self.selected_tasks))
    
    def _select_download_task(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©ä¸‹è½½ä»»åŠ¡ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        # è·å–å½“å‰æ•°æ®æ¡†
        tasks = self.dataset_downloader.list_tasks()
        if not tasks or evt.index[0] >= len(tasks):
            return ""
        
        # è¿”å›é€‰ä¸­è¡Œçš„ä»»åŠ¡IDï¼ˆç¬¬ä¸€åˆ—ï¼‰
        task = tasks[evt.index[0]]
        return task.get('task_id', '')
    
    def _get_download_tasks_df(self) -> pd.DataFrame:
        """è·å–ä¸‹è½½ä»»åŠ¡åˆ—è¡¨æ•°æ®æ¡†"""
        try:
            # è·å–æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            tasks = self.dataset_downloader.list_tasks()
            
            if not tasks:
                return pd.DataFrame(columns=["ä»»åŠ¡ID", "æ•°æ®é›†åç§°", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"])
            
            # æ„å»ºæ•°æ®æ¡†
            rows = []
            for task in tasks:
                task_id = task.get('task_id', '')
                params = task.get('params', {})
                progress_info = task.get('progress', {})
                
                dataset_name = params.get('dataset_name', '')
                status = progress_info.get('status', 'unknown')
                progress = progress_info.get('progress', 0)
                start_time = progress_info.get('start_time', '')
                
                # çŠ¶æ€ä¸­æ–‡æ˜ å°„
                status_map = {
                    'pending': 'ç­‰å¾…ä¸­',
                    'running': 'ä¸‹è½½ä¸­',
                    'paused': 'å·²æš‚åœ',
                    'completed': 'å·²å®Œæˆ',
                    'failed': 'å¤±è´¥'
                }
                
                status_cn = status_map.get(status, status)
                progress_str = f"{progress:.1f}%" if isinstance(progress, (int, float)) else "0%"
                
                # æ ¼å¼åŒ–å¼€å§‹æ—¶é—´
                if start_time:
                    try:
                        dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                        start_time_str = dt.strftime('%m-%d %H:%M')
                    except:
                        start_time_str = start_time[:16] if len(start_time) > 16 else start_time
                else:
                    start_time_str = ""
                
                rows.append([
                    task_id,
                    dataset_name,
                    status_cn,
                    progress_str,
                    start_time_str
                ])
            
            return pd.DataFrame(rows, columns=["ä»»åŠ¡ID", "æ•°æ®é›†åç§°", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"])
            
        except Exception as e:
            self.logger.error(f'è·å–ä¸‹è½½ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["ä»»åŠ¡ID", "æ•°æ®é›†åç§°", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"])
    
    # æ–°çš„ä»»åŠ¡æ“ä½œæ–¹æ³•
    def _start_single_task(self, selected_tasks: list) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """å¼€å§‹å•ä¸ªä»»åŠ¡ï¼ˆä»é€‰ä¸­åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªï¼‰"""
        try:
            if not selected_tasks:
                return "âŒ è¯·å…ˆé€‰æ‹©è¦å¼€å§‹çš„ä»»åŠ¡", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            task_id = selected_tasks[0]  # å–ç¬¬ä¸€ä¸ªé€‰ä¸­çš„ä»»åŠ¡
            success = self.dataset_downloader.start_task(task_id, async_mode=True)
            
            if success:
                return f"âœ… ä»»åŠ¡å·²å¼€å§‹: {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
            else:
                return f"âŒ ä»»åŠ¡å¼€å§‹å¤±è´¥: {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
                
        except Exception as e:
            self.logger.error(f'å¼€å§‹å•ä¸ªä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ å¼€å§‹ä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _pause_single_task(self, selected_tasks: list) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """æš‚åœå•ä¸ªä»»åŠ¡ï¼ˆä»é€‰ä¸­åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªï¼‰"""
        try:
            if not selected_tasks:
                return "âŒ è¯·å…ˆé€‰æ‹©è¦æš‚åœçš„ä»»åŠ¡", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            task_id = selected_tasks[0]  # å–ç¬¬ä¸€ä¸ªé€‰ä¸­çš„ä»»åŠ¡
            success = self.dataset_downloader.pause_task(task_id)
            
            if success:
                return f"âœ… ä»»åŠ¡å·²æš‚åœ: {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
            else:
                return f"âŒ ä»»åŠ¡æš‚åœå¤±è´¥: {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
                
        except Exception as e:
            self.logger.error(f'æš‚åœå•ä¸ªä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æš‚åœä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _delete_single_task(self, selected_tasks: list, delete_files: bool = False) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """åˆ é™¤å•ä¸ªä»»åŠ¡ï¼ˆä»é€‰ä¸­åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªï¼‰"""
        try:
            if not selected_tasks:
                return "âŒ è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            task_id = selected_tasks[0]  # å–ç¬¬ä¸€ä¸ªé€‰ä¸­çš„ä»»åŠ¡
            success = self.dataset_downloader.delete_task(task_id, delete_files=delete_files)
            
            if success:
                file_msg = "ï¼ˆåŒ…å«æ–‡ä»¶ï¼‰" if delete_files else "ï¼ˆä¿ç•™æ–‡ä»¶ï¼‰"
                return f"âœ… ä»»åŠ¡å·²åˆ é™¤{file_msg}: {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
            else:
                return f"âŒ ä»»åŠ¡åˆ é™¤å¤±è´¥: {task_id}", self._get_download_tasks_df(), self._get_task_selector_choices()
                
        except Exception as e:
            self.logger.error(f'åˆ é™¤å•ä¸ªä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _start_multiple_tasks_new(self, selected_tasks: list) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """æ‰¹é‡å¼€å§‹å¤šä¸ªä»»åŠ¡"""
        try:
            if not selected_tasks:
                return "âŒ è¯·å…ˆé€‰æ‹©è¦å¼€å§‹çš„ä»»åŠ¡", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            success_count = 0
            failed_count = 0
            results = []
            
            for task_id in selected_tasks:
                try:
                    success = self.dataset_downloader.start_task(task_id, async_mode=True)
                    if success:
                        success_count += 1
                        results.append(f"âœ… {task_id}")
                    else:
                        failed_count += 1
                        results.append(f"âŒ {task_id}")
                except Exception as e:
                    failed_count += 1
                    results.append(f"âŒ {task_id}: {str(e)}")
            
            summary = f"æ‰¹é‡å¼€å§‹å®Œæˆ: {success_count}ä¸ªæˆåŠŸ, {failed_count}ä¸ªå¤±è´¥"
            details = "\n".join(results)
            
            return f"{summary}\n\nè¯¦æƒ…:\n{details}", self._get_download_tasks_df(), self._get_task_selector_choices()
            
        except Exception as e:
            self.logger.error(f'æ‰¹é‡å¼€å§‹ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æ‰¹é‡å¼€å§‹å¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _pause_multiple_tasks(self, selected_tasks: list) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """æ‰¹é‡æš‚åœå¤šä¸ªä»»åŠ¡"""
        try:
            if not selected_tasks:
                return "âŒ è¯·å…ˆé€‰æ‹©è¦æš‚åœçš„ä»»åŠ¡", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            success_count = 0
            failed_count = 0
            results = []
            
            for task_id in selected_tasks:
                try:
                    success = self.dataset_downloader.pause_task(task_id)
                    if success:
                        success_count += 1
                        results.append(f"âœ… {task_id}")
                    else:
                        failed_count += 1
                        results.append(f"âŒ {task_id}")
                except Exception as e:
                    failed_count += 1
                    results.append(f"âŒ {task_id}: {str(e)}")
            
            summary = f"æ‰¹é‡æš‚åœå®Œæˆ: {success_count}ä¸ªæˆåŠŸ, {failed_count}ä¸ªå¤±è´¥"
            details = "\n".join(results)
            
            return f"{summary}\n\nè¯¦æƒ…:\n{details}", self._get_download_tasks_df(), self._get_task_selector_choices()
            
        except Exception as e:
            self.logger.error(f'æ‰¹é‡æš‚åœä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æ‰¹é‡æš‚åœå¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    def _delete_multiple_tasks(self, selected_tasks: list, delete_files: bool = False) -> Tuple[str, pd.DataFrame, gr.CheckboxGroup]:
        """æ‰¹é‡åˆ é™¤å¤šä¸ªä»»åŠ¡"""
        try:
            if not selected_tasks:
                return "âŒ è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡", self._get_download_tasks_df(), self._get_task_selector_choices()
            
            success_count = 0
            failed_count = 0
            results = []
            
            for task_id in selected_tasks:
                try:
                    success = self.dataset_downloader.delete_task(task_id, delete_files=delete_files)
                    if success:
                        success_count += 1
                        results.append(f"âœ… {task_id}")
                    else:
                        failed_count += 1
                        results.append(f"âŒ {task_id}")
                except Exception as e:
                    failed_count += 1
                    results.append(f"âŒ {task_id}: {str(e)}")
            
            file_msg = "ï¼ˆåŒ…å«æ–‡ä»¶ï¼‰" if delete_files else "ï¼ˆä¿ç•™æ–‡ä»¶ï¼‰"
            summary = f"æ‰¹é‡åˆ é™¤å®Œæˆ{file_msg}: {success_count}ä¸ªæˆåŠŸ, {failed_count}ä¸ªå¤±è´¥"
            details = "\n".join(results)
            
            return f"{summary}\n\nè¯¦æƒ…:\n{details}", self._get_download_tasks_df(), self._get_task_selector_choices()
            
        except Exception as e:
            self.logger.error(f'æ‰¹é‡åˆ é™¤ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æ‰¹é‡åˆ é™¤å¤±è´¥: {str(e)}", self._get_download_tasks_df(), self._get_task_selector_choices()
    
    # ä»¥ä¸‹æ˜¯æ—§çš„æ–¹æ³•ï¼ˆä¿ç•™ä»¥é˜²å…¼å®¹æ€§é—®é¢˜ï¼‰
    def _create_process_tab(self):
        """åˆ›å»ºæ•°æ®åŠ å·¥æ ‡ç­¾é¡µ"""
        gr.Markdown("## æ•°æ®åŠ å·¥ç®¡ç†")
        gr.Markdown("æ”¯æŒæ ¼å¼è½¬æ¢ã€å­—æ®µæå–ã€æ•°æ®åˆå¹¶ã€æ•°æ®æ¸…æ´—ç­‰æ“ä½œ")
        
        # åŠŸèƒ½é€‰æ‹©æ ‡ç­¾
        with gr.Tabs():
            # æ ¼å¼è½¬æ¢å­æ ‡ç­¾
            with gr.TabItem("ğŸ”„ æ ¼å¼è½¬æ¢"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ è½¬æ¢é…ç½®")
                        
                        convert_source = gr.File(
                            label="æºæ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json", ".xml", ".md", ".markdown"]
                        )
                        
                        convert_target = gr.Dropdown(
                            choices=["jsonl", "csv", "xlsx", "json", "xml", "markdown"],
                            value="jsonl",
                            label="ç›®æ ‡æ ¼å¼",
                            info="é€‰æ‹©è½¬æ¢åçš„æ ¼å¼"
                        )
                        
                        convert_output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=str(self.root_dir / "processed"),
                            info="è½¬æ¢ç»“æœä¿å­˜è·¯å¾„"
                        )
                        
                        convert_btn = gr.Button("ğŸ”„ å¼€å§‹è½¬æ¢", variant="primary")
                        refresh_convert_btn = gr.Button("ğŸ”„ åˆ·æ–°ä»»åŠ¡", size="sm")
                    
                    with gr.Column(scale=2):
                        gr.Markdown("### ğŸ“‹ è½¬æ¢ä»»åŠ¡åˆ—è¡¨")
                        
                        convert_task_list = gr.Dataframe(
                            headers=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"],
                            datatype=["str", "str", "str", "str", "str", "str"],
                            label="",
                            interactive=False,
                            wrap=True
                        )
                        
                        with gr.Row():
                            selected_convert_task = gr.Textbox(
                                label="é€‰ä¸­ä»»åŠ¡",
                                placeholder="ç‚¹å‡»ä»»åŠ¡è¡Œé€‰æ‹©",
                                interactive=False,
                                scale=2
                            )
                            
                            view_convert_result_btn = gr.Button("ğŸ“ æŸ¥çœ‹ç»“æœ", size="sm", scale=1)
                
                # ä»»åŠ¡è¯¦æƒ…æ˜¾ç¤º
                with gr.Row():
                    convert_detail_status = gr.Textbox(
                        label="ä»»åŠ¡è¯¦æƒ…",
                        lines=5,
                        interactive=False,
                        show_copy_button=True,
                        info="æ˜¾ç¤ºé€‰ä¸­ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯"
                    )
                    
                    convert_status = gr.Textbox(
                        label="è½¬æ¢çŠ¶æ€",
                        lines=5,
                        interactive=False,
                        show_copy_button=True,
                        info="æ˜¾ç¤ºè½¬æ¢ä»»åŠ¡çš„çŠ¶æ€ä¿¡æ¯"
                    )
            
            # å­—æ®µæå–å­æ ‡ç­¾
            with gr.TabItem("ğŸ“Š å­—æ®µæå–"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ æå–é…ç½®")
                        
                        extract_source = gr.File(
                            label="æºæ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json", ".xml", ".md", ".markdown"]
                        )
                        
                        extract_preview_btn = gr.Button("ğŸ‘ï¸ é¢„è§ˆå­—æ®µ", variant="secondary")
                        
                        extract_fields = gr.CheckboxGroup(
                            label="é€‰æ‹©å­—æ®µ",
                            choices=[],
                            value=[],  # ç¡®ä¿åˆå§‹å€¼ä¸ºç©ºåˆ—è¡¨
                            info="é€‰æ‹©è¦æå–çš„å­—æ®µ"
                        )
                        
                        # å­—æ®µé‡å‘½ååŒºåŸŸ
                        gr.Markdown("### ğŸ·ï¸ å­—æ®µé‡å‘½å")
                        field_mapping_df = gr.Dataframe(
                            headers=["åŸå­—æ®µå", "æ–°å­—æ®µå"],
                            datatype=["str", "str"],
                            row_count=0,
                            col_count=(2, "fixed"),
                            interactive=True,
                            label="å­—æ®µæ˜ å°„è¡¨",
                            visible=False  # åˆå§‹éšè—
                        )
                        
                        extract_output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=str(self.root_dir / "processed"),
                            info="æå–ç»“æœä¿å­˜è·¯å¾„"
                        )
                        
                        extract_btn = gr.Button("ğŸ“Š å¼€å§‹æå–", variant="primary")
                    
                    with gr.Column(scale=1):
                        extract_status = gr.Textbox(
                            label="æå–çŠ¶æ€",
                            lines=8,
                            interactive=False,
                            show_copy_button=True
                        )
            
            # æ•°æ®åˆå¹¶å­æ ‡ç­¾
            with gr.TabItem("ğŸ”— æ•°æ®åˆå¹¶"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ åˆå¹¶é…ç½®")
                        
                        # æ–‡ä»¶é€‰æ‹©å’Œç®¡ç†
                        gr.Markdown("#### 1. é€‰æ‹©åˆå¹¶æ–‡ä»¶")
                        merge_file_upload = gr.File(
                            label="æ·»åŠ æ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json", ".md", ".markdown"],
                            file_count="single"
                        )
                        
                        with gr.Row():
                            add_file_btn = gr.Button("â• æ·»åŠ æ–‡ä»¶", size="sm")
                            clear_files_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºåˆ—è¡¨", size="sm")
                        
                        merge_file_list = gr.Textbox(
                            label="å¾…åˆå¹¶æ–‡ä»¶åˆ—è¡¨",
                            lines=6,
                            interactive=False,
                            value="ğŸ“ å¾…åˆå¹¶æ–‡ä»¶åˆ—è¡¨ï¼ˆè‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶ï¼‰:\n\næš‚æ— æ–‡ä»¶",
                            show_copy_button=True
                        )
                        
                        # åˆå¹¶é€‰é¡¹
                        gr.Markdown("#### 2. åˆå¹¶é€‰é¡¹")
                        merge_mode = gr.Radio(
                            choices=["merge", "append"],
                            value="merge",
                            label="åˆå¹¶æ¨¡å¼",
                            info="merge: åˆ›å»ºæ–°æ–‡ä»¶, append: è¿½åŠ åˆ°ç¬¬ä¸€ä¸ªæ–‡ä»¶"
                        )
                        
                        merge_dedup_field = gr.Textbox(
                            label="å»é‡å­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šid æˆ– question",
                            info="æŒ‡å®šç”¨äºå»é‡çš„å­—æ®µå"
                        )
                        
                        merge_output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=str(self.root_dir / "processed"),
                            info="åˆå¹¶ç»“æœä¿å­˜è·¯å¾„"
                        )
                        
                        merge_btn = gr.Button("ğŸ”— å¼€å§‹åˆå¹¶", variant="primary")
                    
                    with gr.Column(scale=1):
                        merge_status = gr.Textbox(
                            label="åˆå¹¶çŠ¶æ€",
                            lines=10,
                            interactive=False,
                            show_copy_button=True
                        )
            
            # æ•°æ®æ¸…æ´—å­æ ‡ç­¾
            with gr.TabItem("ğŸ§¹ æ•°æ®æ¸…æ´—"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ æ¸…æ´—é…ç½®")
                        
                        clean_source = gr.File(
                            label="æºæ–‡ä»¶",
                            file_types=[".jsonl", ".csv", ".xlsx", ".json"]
                        )
                        
                        clean_operations = gr.CheckboxGroup(
                            choices=[
                                ("å»é™¤ç©ºå€¼", "remove_empty"),
                                ("æ¨¡ç³Šå»é‡", "deduplicate"),
                                ("æ•æ„Ÿè¯å¤„ç†", "filter_sensitive"),
                                ("PIIè„±æ•", "pii_desensitize"),
                                ("æ–‡æœ¬æ ‡å‡†åŒ–", "normalize_text")
                            ],
                            label="æ¸…æ´—æ“ä½œ",
                            info="é€‰æ‹©è¦æ‰§è¡Œçš„æ¸…æ´—æ“ä½œ (æ”¯æŒå¤šé€‰)"
                        )
                        
                        clean_empty_fields = gr.Textbox(
                            label="å»ç©ºå­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šquestion,answer",
                            info="æŒ‡å®šæ£€æŸ¥ç©ºå€¼çš„å­—æ®µï¼Œé€—å·åˆ†éš”"
                        )

                        clean_empty_mode = gr.Radio(
                            choices=["any", "all"],
                            value="any",
                            label="ç©ºå€¼ç­–ç•¥",
                            info="any: ä»»ä¸€å­—æ®µä¸ºç©ºå³ä¸¢å¼ƒ; all: æ‰€æœ‰æŒ‡å®šå­—æ®µéƒ½ä¸ºç©ºæ‰ä¸¢å¼ƒ"
                        )
                        
                        clean_dedup_field = gr.Textbox(
                            label="å»é‡å­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šquestion",
                            info="æŒ‡å®šç”¨äºå»é‡çš„å­—æ®µå"
                        )

                        clean_dedup_threshold = gr.Slider(
                            minimum=0.50,
                            maximum=1.00,
                            value=0.95,
                            step=0.01,
                            label="æ¨¡ç³Šå»é‡é˜ˆå€¼",
                            info="ä»…åœ¨é€‰æ‹©æ¨¡ç³Šå»é‡æ—¶ç”Ÿæ•ˆï¼Œæ¨è 0.85~0.97"
                        )
                        
                        clean_sensitive_words = gr.Textbox(
                            label="æ•æ„Ÿè¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šå¯†ç ,èº«ä»½è¯,æ‰‹æœºå·",
                            info="æŒ‡å®šæ•æ„Ÿè¯ï¼Œé€—å·åˆ†éš”"
                        )

                        clean_sensitive_fields = gr.Textbox(
                            label="æ•æ„Ÿè¯æ‰«æå­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šinstruction,output",
                            info="ä»…å¯¹è¿™äº›å­—æ®µæ‰§è¡Œæ•æ„Ÿè¯å¤„ç†ï¼›ç•™ç©ºåˆ™æ‰«ææ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µ"
                        )

                        clean_sensitive_action = gr.Radio(
                            choices=["drop_record", "remove_word", "replace_word"],
                            value="drop_record",
                            label="æ•æ„Ÿè¯åŠ¨ä½œ",
                            info="drop_record: ä¸¢å¼ƒæ•´æ¡è®°å½•; remove_word: åˆ é™¤è¯æœ¬èº«; replace_word: æ›¿æ¢ä¸ºæŒ‡å®šå†…å®¹"
                        )

                        clean_sensitive_replacement = gr.Textbox(
                            label="æ•æ„Ÿè¯æ›¿æ¢æ–‡æœ¬ï¼ˆå½“é€‰æ‹© replace_word æ—¶ï¼‰",
                            value="***",
                            placeholder="ä¾‹å¦‚ï¼š***",
                            info="ä»…åœ¨æ•æ„Ÿè¯åŠ¨ä½œ=replace_word æ—¶ä½¿ç”¨"
                        )

                        clean_sensitive_exclude_fields = gr.Textbox(
                            label="æ•æ„Ÿè¯æ’é™¤å­—æ®µï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šmeta,source",
                            info="è¿™äº›å­—æ®µå°†å¿½ç•¥æ•æ„Ÿè¯å¤„ç†"
                        )

                        clean_sensitive_field_policies = gr.Textbox(
                            label="å­—æ®µçº§ç­–ç•¥ (å¯é€‰)",
                            placeholder="æ ¼å¼: å­—æ®µ:åŠ¨ä½œ[:æ›¿æ¢]; ä¾‹ instruction:replace_word:@@@,note:remove_word",
                            info="è¦†ç›–å…¨å±€åŠ¨ä½œï¼Œå¯é€‰æ›¿æ¢æ–‡æœ¬ã€‚åŠ¨ä½œ=drop_record/remove_word/replace_word"
                        )

                        clean_sensitive_use_regex = gr.Checkbox(
                            label="æ•æ„Ÿè¯ä½¿ç”¨æ­£åˆ™æ¨¡å¼",
                            value=False
                        )

                        clean_sensitive_case_sensitive = gr.Checkbox(
                            label="å¤§å°å†™æ•æ„ŸåŒ¹é…",
                            value=False
                        )

                        with gr.Accordion("ğŸ“˜ æ•æ„Ÿè¯è§„åˆ™è¯´æ˜", open=False):
                            gr.Markdown(
                                """
**åŒ¹é…æ¨¡å¼è¯´æ˜**

1. æ™®é€šæ¨¡å¼ï¼šæŒ‰æä¾›çš„è¯æ¡é€ä¸€ç²¾ç¡®å­ä¸²åŒ¹é…ï¼ˆé»˜è®¤å¿½ç•¥å¤§å°å†™ï¼Œé™¤éå‹¾é€‰å¤§å°å†™æ•æ„Ÿï¼‰ã€‚
2. æ­£åˆ™æ¨¡å¼ï¼šå‹¾é€‰â€œæ•æ„Ÿè¯ä½¿ç”¨æ­£åˆ™æ¨¡å¼â€åï¼Œåˆ—è¡¨ä¸­æ¯ä¸€é¡¹è§†ä¸ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œæ”¯æŒåˆ†ç»„ä¸é‡è¯ã€‚
3. å­—æ®µç™½åå• / æ’é™¤ï¼š
   - â€œæ•æ„Ÿè¯æ‰«æå­—æ®µâ€å¡«å†™åï¼Œä»…è¿™äº›å­—æ®µä¼šè¢«æ£€æµ‹ã€‚
   - â€œæ•æ„Ÿè¯æ’é™¤å­—æ®µâ€ä¼˜å…ˆç”Ÿæ•ˆï¼Œå¯æ’é™¤éƒ¨åˆ†å­—æ®µã€‚
4. å­—æ®µçº§ç­–ç•¥ä¼˜å…ˆçº§ï¼šå­—æ®µç­–ç•¥ > å…¨å±€åŠ¨ä½œã€‚æ ¼å¼: `å­—æ®µ:åŠ¨ä½œ[:æ›¿æ¢]`ï¼›åŠ¨ä½œæ”¯æŒ `drop_record|remove_word|replace_word`ã€‚
5. ç»Ÿè®¡ä¿¡æ¯ï¼šæ¸…æ´—å®ŒæˆåæŠ¥å‘Šä¸­ `sensitive_detail.field_hits` è®°å½•å„å­—æ®µå‘½ä¸­æ¬¡æ•°ï¼Œ`word_hits` è®°å½•å„è¯å‘½ä¸­æ¬¡æ•°ã€‚`unused_parameters` å¯å¸®åŠ©ç¡®è®¤æœªç”Ÿæ•ˆçš„å¤šä½™å‚æ•°ã€‚
6. Drop Record æå‰ï¼šä¸€æ—¦æŸå­—æ®µç­–ç•¥æˆ–å…¨å±€åŠ¨ä½œè§¦å‘ `drop_record` ä¸”åŒ¹é…å‘½ä¸­ï¼Œè¯¥è®°å½•ç«‹å³ä¸¢å¼ƒï¼Œä¸å†ç»§ç»­æ›¿æ¢å…¶å®ƒå­—æ®µã€‚

**ç¤ºä¾‹**
```
æ•æ„Ÿè¯åˆ—è¡¨: å¯†é’¥,å¯†ç 
å­—æ®µçº§ç­–ç•¥: instruction:remove_word,note:replace_word:[SENSITIVE]
```
è¡¨ç¤º instruction åˆ é™¤è¯æœ¬èº«ï¼Œnote ç”¨ [SENSITIVE] æ›¿æ¢ã€‚
                                """
                            )

                        gr.Markdown("### ğŸ” æ•æ„Ÿè¯è¯•è¿è¡Œ (ä¸è½åœ°æ–‡ä»¶)")
                        sensitive_preview_text = gr.Textbox(
                            label="é¢„è§ˆè¾“å…¥æ–‡æœ¬",
                            placeholder="åœ¨è¿™é‡Œç²˜è´´ä¸€æ®µæ–‡æœ¬ï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æŸ¥çœ‹å¤„ç†æ•ˆæœ",
                            lines=3
                        )
                        sensitive_preview_btn = gr.Button("â–¶ è¯•è¿è¡Œé¢„è§ˆ", size="sm")
                        sensitive_preview_result = gr.Textbox(
                            label="é¢„è§ˆç»“æœ",
                            lines=5,
                            interactive=False,
                            show_copy_button=True
                        )

                        pii_enable = gr.CheckboxGroup(
                            choices=["id_card", "phone", "email", "bank_card", "ip", "passport"],
                            label="å¯ç”¨çš„ PII ç±»å‹",
                            info="é€‰æ‹©éœ€è¦åŒ¹é…å¹¶è„±æ•çš„ä¸ªäººä¿¡æ¯ç±»å‹"
                        )

                        pii_repl_default = gr.Textbox(
                            label="PII é»˜è®¤æ›¿æ¢æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼š<PII>",
                            info="æœªä¸ºåˆ†ç±»å•ç‹¬æŒ‡å®šæ—¶ä½¿ç”¨"
                        )

                        pii_repl_map = gr.Textbox(
                            label="PII åˆ†ç±»æ›¿æ¢ (å¯é€‰)",
                            placeholder="æ ¼å¼: ç±»å‹:æ›¿æ¢æ–‡æœ¬, ä¾‹å¦‚ id_card:<ID>,phone:<TEL>",
                            info="æŒ‰é€—å·åˆ†éš”çš„ é”®:å€¼ åˆ—è¡¨"
                        )

                        normalize_modes = gr.CheckboxGroup(
                            choices=["unicode_nfc", "fullwidth", "lowercase", "collapse_newlines"],
                            label="æ–‡æœ¬æ ‡å‡†åŒ–æ¨¡å¼",
                            info="å¤šé€‰ç»„åˆï¼Œå¯¹æ–‡æœ¬åšç»Ÿä¸€å¤„ç†ï¼ˆç‚¹å‡»ä¸‹æ–¹è¯´æ˜è·å–è¯¦ç»†å·®å¼‚ï¼‰"
                        )

                        with gr.Accordion("ğŸ“˜ æ–‡æœ¬æ ‡å‡†åŒ–è¯´æ˜", open=False):
                            gr.Markdown(
                                """
**å„é€‰é¡¹å«ä¹‰ä¸åœºæ™¯**

1. `unicode_nfc` ç»Ÿä¸€ç­‰ä»·å­—ç¬¦çš„å†…éƒ¨è¡¨ç¤ºï¼ˆNFC è§„èŒƒåŒ–ï¼‰ã€‚
    - è§£å†³ï¼šåŒæ ·æ˜¾ç¤ºçš„å­—ç¬¦å› ä¸ºåˆ†è§£/ç»„åˆå½¢å¼ä¸åŒå¯¼è‡´åŒ¹é…/å»é‡å¤±è´¥ã€‚
    - ä¾‹ï¼š`e + Ì` -> `Ã©`ã€‚

2. `fullwidth` å…¨è§’è½¬åŠè§’ï¼ˆåªä½œç”¨äºå­—æ¯ / æ•°å­— / å¸¸è§è‹±æ–‡æ ‡ç‚¹ï¼‰ã€‚
    - è§£å†³ï¼šè¾“å…¥æ³•å…¨è§’æ¨¡å¼ / ç½‘é¡µå¤åˆ¶å¯¼è‡´çš„ï¼¡ï¼¢ï¼£ï¼‘ï¼’ï¼“ï¼Œé¿å…åŒ¹é…å¤±è´¥ã€‚
    - ä¾‹ï¼š`ï¼¡ï¼¢ï¼£ï¼‘ï¼’ï¼“ï¼Œï¼ï¼` -> `ABC123,./`ã€‚

3. `lowercase` æ‰€æœ‰å­—æ¯è½¬å°å†™ã€‚
    - é€‚åˆï¼šåç»­åŒ¹é…/å»é‡ä¸å…³å¿ƒå¤§å°å†™ï¼ˆå¦‚è‹±æ–‡æ™®é€šæè¿°ã€æ ‡ç­¾ï¼‰ã€‚
    - ä¸æ¨èï¼šåŒºåˆ†å¤§å°å†™æœ‰æ„ä¹‰ï¼ˆä¸“æœ‰åè¯ã€ä»£ç ç‰‡æ®µã€å˜é‡ã€æƒ…æ„Ÿå¼ºè°ƒï¼‰ã€‚

4. `collapse_newlines` æŠ˜å å¤šä½™ç©ºè¡Œï¼Œé¿å…å¤§æ®µç©ºç™½ã€‚
    - å¤„ç†ï¼šå°†è¿ç»­çš„ç©ºè¡Œæ”¶ç¼©ä¸º 1 è¡Œï¼Œå¹¶æ¸…ç†å¤šä½™ç©ºç™½ï¼›å¯å‡å°‘ token / å™ªå£°ã€‚
    - ä¿ç•™ï¼šæ­£å¸¸æ®µè½çš„å•ä¸ªæ¢è¡Œã€‚

**æ‰§è¡Œé¡ºåºï¼ˆå½“å‰å®ç°ï¼‰** å…ˆåšç©ºç™½æŠ˜å ï¼Œå†æŒ‰æ‰€é€‰æ¨¡å¼åº”ç”¨ï¼ˆNFC â†’ å…¨è§’ â†’ å°å†™ â†’ ç©ºè¡ŒæŠ˜å ï¼‰ã€‚
å¦‚éœ€æ›´ç²¾ç»†é¡ºåºæˆ–å¢åŠ â€œä¿ç•™å¤§å°å†™é‡è¦å­—æ®µâ€ç™½åå•ï¼Œå¯åç»­æ‰©å±•ã€‚
"""
                            )
                        
                        clean_btn = gr.Button("ğŸ§¹ å¼€å§‹æ¸…æ´—", variant="primary")
                    
                    with gr.Column(scale=1):
                        clean_status = gr.Textbox(
                            label="æ¸…æ´—çŠ¶æ€",
                            lines=8,
                            interactive=False,
                            show_copy_button=True
                        )
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.components['process'] = {
            'convert_source': convert_source,
            'convert_target': convert_target,
            'convert_output_dir': convert_output_dir,
            'convert_status': convert_status,
            'convert_task_list': convert_task_list,
            'selected_convert_task': selected_convert_task,
            'convert_detail_status': convert_detail_status,
            'extract_source': extract_source,
            'extract_fields': extract_fields,
            'extract_output_dir': extract_output_dir,
            'extract_status': extract_status,
            'merge_file_upload': merge_file_upload,
            'add_file_btn': add_file_btn,
            'clear_files_btn': clear_files_btn,
            'merge_file_list': merge_file_list,
            'merge_mode': merge_mode,
            'merge_dedup_field': merge_dedup_field,
            'merge_output_dir': merge_output_dir,
            'merge_status': merge_status,
            'clean_source': clean_source,
            'clean_operations': clean_operations,
            'clean_empty_fields': clean_empty_fields,
            'clean_empty_mode': clean_empty_mode,
            'clean_dedup_field': clean_dedup_field,
            'clean_dedup_threshold': clean_dedup_threshold,
            'clean_sensitive_words': clean_sensitive_words,
            'clean_sensitive_fields': clean_sensitive_fields,
            'clean_sensitive_action': clean_sensitive_action,
            'clean_sensitive_replacement': clean_sensitive_replacement,
            'clean_sensitive_exclude_fields': clean_sensitive_exclude_fields,
            'clean_sensitive_field_policies': clean_sensitive_field_policies,
            'clean_sensitive_use_regex': clean_sensitive_use_regex,
            'clean_sensitive_case_sensitive': clean_sensitive_case_sensitive,
            'sensitive_preview_text': sensitive_preview_text,
            'sensitive_preview_btn': sensitive_preview_btn,
            'sensitive_preview_result': sensitive_preview_result,
            'pii_enable': pii_enable,
            'pii_repl_default': pii_repl_default,
            'pii_repl_map': pii_repl_map,
            'normalize_modes': normalize_modes,
            'clean_status': clean_status
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        convert_btn.click(
            fn=self._start_format_convert,
            inputs=[convert_source, convert_target, convert_output_dir],
            outputs=[convert_status]
        )
        
        # æ–°å¢ï¼šå¼‚æ­¥ä»»åŠ¡ç®¡ç†äº‹ä»¶
        refresh_convert_btn.click(
            fn=self._get_convert_tasks_df,
            outputs=[convert_task_list]
        )
        
        convert_task_list.select(
            fn=self._select_convert_task,
            outputs=[selected_convert_task]
        )
        
        view_convert_result_btn.click(
            fn=self._view_convert_result,
            inputs=[selected_convert_task],
            outputs=[convert_detail_status]
        )
        
        extract_preview_btn.click(
            fn=self._preview_extract_fields,
            inputs=[extract_source],
            outputs=[extract_fields]
        )
        
        # å½“æ–‡ä»¶ä¸Šä¼ æ—¶è‡ªåŠ¨é‡ç½®å­—æ®µé€‰æ‹©
        extract_source.change(
            fn=self._reset_field_selection,
            inputs=[],
            outputs=[extract_fields, field_mapping_df]
        )
        
        # å­—æ®µé€‰æ‹©å˜åŒ–æ—¶æ›´æ–°æ˜ å°„è¡¨
        extract_fields.change(
            fn=self._update_field_mapping,
            inputs=[extract_fields],
            outputs=[field_mapping_df]
        )
        
        extract_btn.click(
            fn=self._start_field_extract_with_progress,
            inputs=[extract_source, extract_fields, field_mapping_df, extract_output_dir],
            outputs=[extract_status]
        )
        
        # æ–‡ä»¶ç®¡ç†äº‹ä»¶ç»‘å®š
        add_file_btn.click(
            fn=self._add_merge_file,
            inputs=[merge_file_upload],
            outputs=[merge_file_upload, merge_file_list]
        )
        
        clear_files_btn.click(
            fn=self._clear_merge_files,
            inputs=[],
            outputs=[merge_file_list]
        )
        
        merge_btn.click(
            fn=self._start_data_merge,
            inputs=[merge_mode, merge_dedup_field, merge_output_dir],
            outputs=[merge_status]
        )

        # æ¸…æ´—ä»»åŠ¡å¯åŠ¨æŒ‰é’®äº‹ä»¶ç»‘å®š
        clean_btn.click(
            fn=self._start_data_clean,
            inputs=[clean_source, clean_operations, clean_empty_fields,
                clean_empty_mode, clean_dedup_field, clean_dedup_threshold,
                clean_sensitive_words, clean_sensitive_action, clean_sensitive_replacement,
                clean_sensitive_fields, clean_sensitive_exclude_fields, clean_sensitive_field_policies,
                clean_sensitive_use_regex, clean_sensitive_case_sensitive,
                pii_enable, pii_repl_default, pii_repl_map,
                normalize_modes],
            outputs=[clean_status]
        )

        # æ•æ„Ÿè¯è¯•è¿è¡Œç»‘å®š
        sensitive_preview_btn.click(
            fn=self._preview_sensitive_processing,
            inputs=[sensitive_preview_text, clean_sensitive_words, clean_sensitive_action, clean_sensitive_replacement,
                    clean_sensitive_fields, clean_sensitive_exclude_fields, clean_sensitive_field_policies,
                    clean_sensitive_use_regex, clean_sensitive_case_sensitive],
            outputs=[sensitive_preview_result]
        )
    
    def _create_model_tab(self):
        """åˆ›å»ºæ¨¡å‹é…ç½®æ ‡ç­¾é¡µ"""
        gr.Markdown("## æ¨¡å‹é…ç½®ç®¡ç†")
        gr.Markdown("æ”¯æŒ vLLMã€OpenAIã€SGlangã€Ollama ç­‰å¤šç§æ¨¡å‹ç±»å‹")
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ¨¡å‹é…ç½®åŒºåŸŸ
                gr.Markdown("### â• æ·»åŠ æ–°æ¨¡å‹")
                
                model_name = gr.Textbox(
                    label="æ¨¡å‹åç§°",
                    placeholder="ä¾‹å¦‚ï¼šgpt-4 æˆ– local-llama",
                    info="ä¸ºæ¨¡å‹è®¾ç½®ä¸€ä¸ªå”¯ä¸€æ ‡è¯†åç§°"
                )
                
                model_type = gr.Dropdown(
                    choices=["vllm", "openai", "sglang", "ollama"],
                    value="openai",
                    label="æ¨¡å‹ç±»å‹",
                    info="é€‰æ‹©æ¨¡å‹çš„éƒ¨ç½²ç±»å‹"
                )
                
                model_url = gr.Textbox(
                    label="æ¨¡å‹URL",
                    placeholder="ä¾‹å¦‚ï¼šhttp://localhost:8000/v1 æˆ– https://api.openai.com/v1",
                    info="æ¨¡å‹æœåŠ¡çš„APIåœ°å€"
                )
                
                model_api_key = gr.Textbox(
                    label="APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰",
                    type="password",
                    placeholder="è¾“å…¥APIå¯†é’¥",
                    info="æŸäº›æ¨¡å‹éœ€è¦APIå¯†é’¥è®¤è¯"
                )
                
                model_model_name = gr.Textbox(
                    label="å®é™…æ¨¡å‹å",
                    placeholder="ä¾‹å¦‚ï¼šgpt-4 æˆ– llama-2-7b",
                    info="APIè°ƒç”¨æ—¶ä½¿ç”¨çš„æ¨¡å‹åç§°"
                )
                
                with gr.Row():
                    add_model_btn = gr.Button("â• æ·»åŠ æ¨¡å‹", variant="primary")
                    test_all_btn = gr.Button("ğŸ”§ æµ‹è¯•æ‰€æœ‰æ¨¡å‹", variant="secondary")
            
            with gr.Column(scale=2):
                # æ¨¡å‹åˆ—è¡¨åŒºåŸŸ
                gr.Markdown("### ğŸ“‹ å·²é…ç½®æ¨¡å‹åˆ—è¡¨")
                
                model_list = gr.Dataframe(
                    headers=["æ¨¡å‹åç§°", "ç±»å‹", "çŠ¶æ€", "URL", "å“åº”æ—¶é—´", "æ“ä½œ"],
                    datatype=["str", "str", "str", "str", "str", "str"],
                    label="",
                    interactive=False,
                    wrap=True
                )
                
                with gr.Row():
                    test_model_btn = gr.Button("ğŸ”§ æµ‹è¯•æ¨¡å‹", size="sm")
                    delete_model_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤æ¨¡å‹", size="sm", variant="stop")
                    refresh_model_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm")
                
                selected_model_name = gr.Textbox(
                    label="é€‰ä¸­æ¨¡å‹",
                    placeholder="ç‚¹å‡»æ¨¡å‹è¡Œé€‰æ‹©",
                    interactive=False
                )
        
        # çŠ¶æ€è¾“å‡ºåŒºåŸŸ
        with gr.Row():
            model_status = gr.Textbox(
                label="çŠ¶æ€ä¿¡æ¯",
                lines=4,
                interactive=False,
                show_copy_button=True
            )
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.components['model'] = {
            'model_name': model_name,
            'model_type': model_type,
            'model_url': model_url,
            'model_api_key': model_api_key,
            'model_model_name': model_model_name,
            'model_list': model_list,
            'selected_model_name': selected_model_name,
            'status': model_status
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        add_model_btn.click(
            fn=self._add_model,
            inputs=[model_name, model_type, model_url, model_api_key, model_model_name],
            outputs=[model_status, model_list]
        )
        
        test_all_btn.click(
            fn=self._test_all_models,
            outputs=[model_status, model_list]
        )
        
        test_model_btn.click(
            fn=self._test_model,
            inputs=[selected_model_name],
            outputs=[model_status, model_list]
        )
        
        delete_model_btn.click(
            fn=self._delete_model,
            inputs=[selected_model_name],
            outputs=[model_status, model_list]
        )
        
        refresh_model_btn.click(
            fn=self._refresh_models,
            outputs=[model_list]
        )
        
        # æ¨¡å‹åˆ—è¡¨ç‚¹å‡»äº‹ä»¶
        model_list.select(
            fn=self._select_model,
            outputs=[selected_model_name]
        )
    
    def _create_distill_tab(self):
        """åˆ›å»ºè’¸é¦ç”Ÿæˆæ ‡ç­¾é¡µ"""
        gr.Markdown("## è’¸é¦ç”Ÿæˆç®¡ç†")
        gr.Markdown("åŸºäºAIæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®")
        
        with gr.Row():
            with gr.Column(scale=1):
                # è’¸é¦é…ç½®åŒºåŸŸ
                gr.Markdown("### ğŸ“ ç”Ÿæˆé…ç½®")
                
                distill_source = gr.File(
                    label="æºæ•°æ®æ–‡ä»¶",
                    file_types=[".jsonl", ".json"]
                )
                
                distill_strategy = gr.Dropdown(
                    choices=[
                        ("æ•°æ®æ‰©å……", "expand"),
                        ("å†…å®¹å¢å¼º", "enhance"),
                        ("æ–‡æœ¬æ”¹å†™", "paraphrase"),
                        ("åˆ†ç±»æ ‡æ³¨", "classify_label"),
                        ("ä»Qç”ŸA", "q_to_a"),
                        ("è‡ªå®šä¹‰", "custom")
                    ],
                    value="expand",
                    label="ç”Ÿæˆç­–ç•¥",
                    info="é€‰æ‹©æ•°æ®ç”Ÿæˆçš„ç­–ç•¥ç±»å‹"
                )
                
                distill_model = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=[],
                    info="é€‰æ‹©ç”¨äºç”Ÿæˆçš„AIæ¨¡å‹"
                )
                
                refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨", size="sm")
                
                distill_count = gr.Slider(
                    minimum=1,
                    maximum=50,
                    value=5,
                    step=1,
                    label="ç”Ÿæˆæ•°é‡",
                    info="æ¯ä¸ªè¾“å…¥æ ·æœ¬ç”Ÿæˆçš„æ•°é‡"
                )
                
                distill_temperature = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=0.7,
                    step=0.1,
                    label="æ¸©åº¦å‚æ•°",
                    info="æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœº"
                )
                
                distill_max_tokens = gr.Slider(
                    minimum=100,
                    maximum=4000,
                    value=2048,
                    step=100,
                    label="æœ€å¤§Tokenæ•°",
                    info="é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆå°†æ ¹æ®æ¨¡å‹ç±»å‹åŠ¨æ€è°ƒæ•´ä¸Šé™ï¼‰"
                )

                distill_top_p = gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.05,
                    label="top_p",
                    info="é‡‡æ ·æˆªæ–­æ¦‚ç‡"
                )

                distill_top_k = gr.Slider(
                    minimum=0,
                    maximum=200,
                    value=0,
                    step=1,
                    label="top_k (0 è¡¨ç¤ºæœªå¯ç”¨)",
                    info="top-k é‡‡æ ·ï¼Œ0 è¡¨ç¤ºå…³é—­"
                )

                with gr.Accordion("âš™ï¸ å¹¶å‘ä¸æ€§èƒ½", open=False):
                    concurrency_workers = gr.Slider(
                        minimum=1,
                        maximum=128,
                        value=8,
                        step=1,
                        label="å¹¶å‘åº¦ï¼ˆåŒæ—¶è¯·æ±‚æ•°ï¼‰",
                        info="æ ¹æ®æ¨¡å‹ååä¸é™æµè°ƒæ•´ï¼›JSONL å¤§ä»»åŠ¡å»ºè®® 8~64 ä¹‹é—´"
                    )
                    fsync_interval_slider = gr.Slider(
                        minimum=1,
                        maximum=1000,
                        value=50,
                        step=1,
                        label="å†™ç›˜åŒæ­¥é—´éš”ï¼ˆè¡Œï¼‰",
                        info="æ¯å¤šå°‘è¡Œè°ƒç”¨ä¸€æ¬¡ fsyncï¼Œè¶Šå°è¶Šå®‰å…¨ï¼Œè¶Šå¤§ååè¶Šé«˜"
                    )
                    checkpoint_interval_slider = gr.Slider(
                        minimum=10,
                        maximum=5000,
                        value=100,
                        step=10,
                        label="æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”ï¼ˆè¡Œï¼‰",
                        info="æ¯å¤šå°‘è¾“å…¥è¡Œä¿å­˜ä¸€æ¬¡ checkpoint.json"
                    )
                    inflight_multiplier_slider = gr.Slider(
                        minimum=1,
                        maximum=20,
                        value=4,
                        step=1,
                        label="åœ¨é€”ä»»åŠ¡å€æ•°",
                        info="æœ€å¤§åœ¨é€”ä»»åŠ¡æ•° = å¹¶å‘åº¦ * å€æ•°ï¼ˆæµå¼JSONLæ¨¡å¼ï¼‰"
                    )
                    unordered_write_checkbox = gr.Checkbox(
                        label="æ— åºå†™å…¥ï¼ˆå®Œæˆå³å†™ï¼‰",
                        value=False,
                        info="æé«˜ååé‡ï¼Œæ”¾å¼ƒä¸¥æ ¼æŒ‰è¾“å…¥é¡ºåºå†™å‡º"
                    )
                    rate_limit_rps_number = gr.Number(
                        label="é™æµï¼šæ¯ç§’è¯·æ±‚æ•°ï¼ˆå¯ç©ºï¼‰",
                        value=None,
                        precision=2,
                        interactive=True
                    )
                    max_backoff_number = gr.Number(
                        label="æœ€å¤§é€€é¿ç§’æ•°",
                        value=8.0,
                        precision=2,
                        interactive=True
                    )
                
                # ç­–ç•¥è¯´æ˜ä¸æç¤ºè¯/å­—æ®µ/å‚æ•°
                with gr.Accordion("ğŸ“˜ ç­–ç•¥è¯´æ˜ä¸æç¤ºè¯", open=True):
                    strategy_desc = gr.Markdown("*é€‰æ‹©ç­–ç•¥åæ˜¾ç¤ºè¯´æ˜ä¸å‚æ•°æç¤º*")

                    system_prompt_box = gr.Textbox(
                        label="System æç¤ºè¯",
                        lines=3,
                        placeholder="å¯é€‰ï¼šç”¨äºçº¦æŸæ•´ä½“é£æ ¼ã€ç¦åˆ™ç­‰"
                    )
                    q_prompt_box = gr.Textbox(
                        label="Q æç¤ºè¯ï¼ˆä»… q_to_a/customï¼‰",
                        lines=2,
                        visible=False,
                        placeholder="ç”¨äºå¼•å¯¼é—®é¢˜çš„æ„é€ æˆ–é€‰å–è¦ç‚¹"
                    )
                    a_prompt_box = gr.Textbox(
                        label="A æç¤ºè¯ï¼ˆä»… q_to_a/customï¼‰",
                        lines=2,
                        visible=False,
                        placeholder="ç”¨äºçº¦æŸç­”æ¡ˆçš„é£æ ¼ä¸ç»“æ„"
                    )

                with gr.Accordion("ğŸ§© å­—æ®µä¸ç›®æ ‡è¾“å‡º", open=False):
                    with gr.Row():
                        dataset_fields_box = gr.CheckboxGroup(
                            label="ä»æ•°æ®æ–‡ä»¶ä¸­æ£€æµ‹åˆ°çš„å­—æ®µ",
                            choices=[],
                            value=[],
                            interactive=True,
                            info="é€‰æ‹©éœ€è¦å‚ä¸ç”Ÿæˆ/æ”¹å†™çš„å­—æ®µï¼ˆå°†åŒæ­¥åˆ°ä¸‹æ–¹æ–‡æœ¬æ¡†ï¼‰"
                        )
                    selected_fields_input = gr.Textbox(
                        label="é€‰å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼‰",
                        placeholder="ä¾‹å¦‚ï¼šinstruction,output æˆ– question,answer"
                    )
                    q_field_name_input = gr.Textbox(
                        label="Q å­—æ®µåï¼ˆè¾“å‡ºæ•°æ®ä¸­ä½¿ç”¨ï¼‰",
                        value="instruction",
                        placeholder="é»˜è®¤ instructionï¼Œå¯è‡ªå®šä¹‰ä¸º question ç­‰"
                    )
                    label_set_input = gr.Textbox(
                        label="æ ‡ç­¾é›†åˆï¼ˆä»…åˆ†ç±»æ ‡æ³¨ï¼Œé€—å·åˆ†éš”ï¼‰",
                        visible=False,
                        placeholder="ä¾‹å¦‚ï¼šæ­£å‘,è´Ÿå‘,ä¸­ç«‹"
                    )
                    target_field_input = gr.Textbox(
                        label="A å­—æ®µåï¼ˆç›®æ ‡å­—æ®µï¼‰",
                        value="output",
                        placeholder="ç”Ÿæˆå†…å®¹å†™å…¥çš„å­—æ®µåï¼Œé»˜è®¤ output"
                    )
                
                with gr.Row():
                    start_distill_btn = gr.Button("ğŸ§  å¼€å§‹ç”Ÿæˆ", variant="primary")
                    pause_distill_btn = gr.Button("â¸ï¸ æš‚åœç”Ÿæˆ", variant="secondary")
            
            with gr.Column(scale=2):
                # ä»»åŠ¡çŠ¶æ€åŒºåŸŸ
                gr.Markdown("### ğŸ“Š ç”Ÿæˆè¿›åº¦")
                
                distill_progress = gr.Progress()
                
                distill_task_list = gr.Dataframe(
                    headers=["ä»»åŠ¡ID", "ç­–ç•¥", "æ¨¡å‹", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"],
                    datatype=["str", "str", "str", "str", "str", "str"],
                    label="è’¸é¦ä»»åŠ¡åˆ—è¡¨",
                    interactive=False,
                    wrap=True
                )
                
                with gr.Row():
                    refresh_distill_btn = gr.Button("ğŸ”„ åˆ·æ–°ä»»åŠ¡", size="sm")
                    view_report_btn = gr.Button("ğŸ“‹ æŸ¥çœ‹æŠ¥å‘Š", size="sm")
                    resume_task_btn = gr.Button("â¯ï¸ æ¢å¤ä»»åŠ¡", size="sm")
                
                selected_distill_task = gr.Textbox(
                    label="é€‰ä¸­ä»»åŠ¡ID",
                    placeholder="ç‚¹å‡»ä»»åŠ¡è¡Œé€‰æ‹©",
                    interactive=False
                )

                with gr.Accordion("â¯ï¸ æ¢å¤ä¸è¦†ç›–å‚æ•°", open=False):
                    resume_model_override = gr.Dropdown(label="è¦†ç›–æ¨¡å‹ï¼ˆå¯é€‰ï¼‰", choices=[], interactive=True)
                    resume_workers = gr.Slider(minimum=1, maximum=128, value=8, step=1, label="å¹¶å‘åº¦ï¼ˆè¦†ç›–å¯é€‰ï¼‰")
                    resume_temp = gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label="æ¸©åº¦ï¼ˆè¦†ç›–å¯é€‰ï¼‰")
                    resume_max_tokens = gr.Slider(minimum=100, maximum=4000, value=2048, step=100, label="æœ€å¤§Tokenï¼ˆè¦†ç›–å¯é€‰ï¼‰")
                    resume_top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label="top_pï¼ˆè¦†ç›–å¯é€‰ï¼‰")
                    resume_top_k = gr.Slider(minimum=0, maximum=200, value=0, step=1, label="top_kï¼ˆè¦†ç›–å¯é€‰ï¼Œ0=ä¸å˜ï¼‰")
                    resume_rate_limit_rps = gr.Number(label="é™æµRPSï¼ˆè¦†ç›–ï¼‰", value=None, precision=2)
                    resume_max_backoff = gr.Number(label="æœ€å¤§é€€é¿ï¼ˆè¦†ç›–ï¼‰", value=None, precision=2)
                    resume_as_new_checkbox = gr.Checkbox(label="å…‹éš†ä¸ºæ–°ä»»åŠ¡ç»§ç»­ï¼ˆresume-as-newï¼‰", value=False)
        
        # çŠ¶æ€è¾“å‡ºåŒºåŸŸ
        with gr.Row():
            distill_status = gr.Textbox(
                label="çŠ¶æ€ä¿¡æ¯",
                lines=5,
                interactive=False,
                show_copy_button=True
            )
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.components['distill'] = {
            'source': distill_source,
            'strategy': distill_strategy,
            'model': distill_model,
            'count': distill_count,
            'temperature': distill_temperature,
            'max_tokens': distill_max_tokens,
            'top_p': distill_top_p,
            'top_k': distill_top_k,
            'strategy_desc': strategy_desc,
            'system_prompt': system_prompt_box,
            'q_prompt': q_prompt_box,
            'a_prompt': a_prompt_box,
            'dataset_fields': dataset_fields_box,
            'selected_fields': selected_fields_input,
            'q_field_name': q_field_name_input,
            'label_set': label_set_input,
            'target_field': target_field_input,
            'task_list': distill_task_list,
            'selected_task': selected_distill_task,
            'status': distill_status,
            'unordered_write': unordered_write_checkbox,
            'rate_limit_rps': rate_limit_rps_number,
            'max_backoff': max_backoff_number
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        refresh_models_btn.click(
            fn=self._refresh_distill_models,
            outputs=[distill_model]
        )

        # æ¢å¤å‚æ•°æ¨¡å‹ä¸‹æ‹‰åŒæ­¥
        refresh_models_btn.click(
            fn=self._refresh_distill_models,
            outputs=[resume_model_override]
        )

        # æ¨¡å‹å˜æ›´æ—¶ï¼ŒåŠ¨æ€è°ƒæ•´æœ€å¤§tokenä¸Šé™
        distill_model.change(
            fn=self._on_distill_model_change,
            inputs=[distill_model],
            outputs=[distill_max_tokens]
        )
        
        start_distill_btn.click(
            fn=self._start_distill_generation,
            inputs=[distill_source, distill_strategy, distill_model, distill_count,
                   distill_temperature, distill_max_tokens, distill_top_p, distill_top_k,
                   concurrency_workers, fsync_interval_slider, checkpoint_interval_slider, inflight_multiplier_slider,
                   unordered_write_checkbox, rate_limit_rps_number, max_backoff_number,
                   system_prompt_box, q_prompt_box, a_prompt_box,
                   selected_fields_input, q_field_name_input, label_set_input, target_field_input],
            outputs=[distill_status, distill_task_list]
        )
        
        pause_distill_btn.click(
            fn=self._pause_distill_generation,
            inputs=[selected_distill_task],
            outputs=[distill_status, distill_task_list]
        )
        
        refresh_distill_btn.click(
            fn=self._refresh_distill_tasks,
            outputs=[distill_task_list]
        )
        
        view_report_btn.click(
            fn=self._view_distill_report,
            inputs=[selected_distill_task],
            outputs=[distill_status]
        )

        resume_task_btn.click(
            fn=self._resume_distill_task,
            inputs=[selected_distill_task, resume_model_override, resume_workers, resume_temp, resume_max_tokens, resume_top_p, resume_top_k, resume_rate_limit_rps, resume_max_backoff, resume_as_new_checkbox],
            outputs=[distill_status, distill_task_list]
        )

        # é€‰æ‹©æ–‡ä»¶åï¼Œè‡ªåŠ¨æ‰«æå­—æ®µå¹¶å¡«å……å¤é€‰æ¡†
        distill_source.change(
            fn=self._on_distill_source_change,
            inputs=[distill_source],
            outputs=[dataset_fields_box, selected_fields_input]
        )

        # å‹¾é€‰å­—æ®µæ—¶ï¼ŒåŒæ­¥åˆ°æ–‡æœ¬æ¡†
        dataset_fields_box.change(
            fn=self._sync_selected_fields_text,
            inputs=[dataset_fields_box],
            outputs=[selected_fields_input]
        )
        
        # ä»»åŠ¡åˆ—è¡¨ç‚¹å‡»äº‹ä»¶
        distill_task_list.select(
            fn=self._select_distill_task,
            outputs=[selected_distill_task]
        )

        # ç­–ç•¥åˆ‡æ¢æ—¶æ›´æ–°è¯´æ˜ä¸å¯è§æ€§
        distill_strategy.change(
            fn=self._on_strategy_change,
            inputs=[distill_strategy],
            outputs=[strategy_desc, distill_count, q_prompt_box, a_prompt_box, label_set_input]
        )
    
    def _create_manage_tab(self):
        """åˆ›å»ºæ•°æ®ç®¡ç†æ ‡ç­¾é¡µ"""
        gr.Markdown("## æ•°æ®ç®¡ç†ä¸­å¿ƒ")
        gr.Markdown("ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®é›†ï¼Œæ”¯æŒæŒ‰ç±»å‹æŸ¥çœ‹å’Œé¢„è§ˆæ“ä½œ")
        
        # ç­›é€‰å’Œæœç´¢åŒºåŸŸ
        with gr.Row():
            data_type_filter = gr.Dropdown(
                choices=["å…¨éƒ¨", "åŸå§‹æ•°æ®", "å¤„ç†æ•°æ®", "è’¸é¦æ•°æ®"],
                value="å…¨éƒ¨",
                label="æ•°æ®ç±»å‹ç­›é€‰",
                info="æŒ‰æ•°æ®ç±»å‹ç­›é€‰æ˜¾ç¤º",
                scale=2
            )
            
            dataset_name_search = gr.Textbox(
                label="æ•°æ®é›†åç§°æœç´¢",
                placeholder="è¾“å…¥æ•°æ®é›†åç§°å…³é”®è¯",
                info="æ¨¡ç³Šæœç´¢æ•°æ®é›†åç§°",
                scale=2
            )
            
            search_dataset_btn = gr.Button("ğŸ” æœç´¢", size="sm", scale=1)
        
        # æ•°æ®é›†åˆ—è¡¨åŒºåŸŸ
        gr.Markdown("### ğŸ“‚ æ•°æ®é›†åˆ—è¡¨")
        gr.Markdown("ğŸ’¡ **æ“ä½œæç¤º**ï¼šç‚¹å‡»è¡¨æ ¼è¡Œé€‰æ‹©æ•°æ®é›† â€¢ è¡¨æ ¼æ”¯æŒæ°´å¹³æ»šåŠ¨æŸ¥çœ‹å®Œæ•´å†…å®¹ â€¢ æ‚¬åœå•å…ƒæ ¼æ˜¾ç¤ºå®Œæ•´æ–‡æœ¬")
        gr.Markdown("âš ï¸ **åˆ é™¤è­¦å‘Š**ï¼šåˆ é™¤æ•°æ®ä¼šæ°¸ä¹…åˆ é™¤æ•°æ®é›†æ–‡ä»¶æˆ–æ•´ä¸ªæ•°æ®é›†ç›®å½•ï¼Œè¯·è°¨æ…æ“ä½œï¼")
                
        dataset_list = gr.Dataframe(
            headers=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"],
            datatype=["str", "str", "str", "str", "str"],
            label="",
            interactive=False,
            wrap=True
        )
        
        # æ“ä½œæŒ‰é’®åŒºåŸŸ - æ”¾åœ¨åˆ—è¡¨ä¸‹æ–¹
        with gr.Row():
            refresh_data_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm")
            preview_data_btn = gr.Button("ğŸ‘ï¸ é¢„è§ˆæ•°æ®", size="sm")
            delete_data_btn = gr.Button("ğŸ—‘ï¸ å±é™©åˆ é™¤", size="sm", variant="stop")
        
        selected_dataset = gr.Textbox(
            label="é€‰ä¸­æ•°æ®é›†",
            placeholder="ç‚¹å‡»æ•°æ®é›†è¡Œé€‰æ‹©",
            interactive=False
        )
        
        # æ•°æ®é¢„è§ˆåŒºåŸŸ - æ”¹ä¸ºä¸‹æ–¹å®Œæ•´åŒºåŸŸ
        gr.Markdown("### ğŸ‘ï¸ æ•°æ®é¢„è§ˆ")
        
        with gr.Row():
            with gr.Column(scale=1):
                # é¢„è§ˆæ§åˆ¶é€‰é¡¹
                preview_rows = gr.Slider(
                    minimum=10,
                    maximum=1000,
                    value=100,
                    step=10,
                    label="é¢„è§ˆè¡Œæ•°",
                    info="è®¾ç½®è¦é¢„è§ˆçš„æ•°æ®è¡Œæ•°"
                )
                
                auto_preview = gr.Checkbox(
                    label="è‡ªåŠ¨é¢„è§ˆ",
                    value=True,
                    info="é€‰æ‹©æ•°æ®é›†æ—¶è‡ªåŠ¨é¢„è§ˆ"
                )
                
                text_truncation = gr.Checkbox(
                    label="æ–‡æœ¬æˆªæ–­",
                    value=True,
                    info="å¯¹é•¿æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æˆªæ–­"
                )
                
                max_text_length = gr.Slider(
                    minimum=50,
                    maximum=1000,
                    value=300,
                    step=50,
                    label="æ–‡æœ¬æˆªæ–­é•¿åº¦",
                    info="è¶…è¿‡æ­¤é•¿åº¦çš„æ–‡æœ¬å°†è¢«æˆªæ–­"
                )
            
            with gr.Column(scale=2):
                # å­—æ®µé€‰æ‹©åŒºåŸŸ
                gr.Markdown("#### ğŸ“‹ å­—æ®µé€‰æ‹©")
                gr.Markdown("ğŸ’¡ é€‰æ‹©è¦æ˜¾ç¤ºçš„å­—æ®µï¼Œå®æ—¶æ›´æ–°é¢„è§ˆè¡¨æ ¼")
                
                with gr.Row():
                    select_all_fields = gr.Button("å…¨é€‰", size="sm", variant="secondary")
                    clear_all_fields = gr.Button("æ¸…é™¤", size="sm", variant="secondary")
                    select_common_fields = gr.Button("å¸¸ç”¨å­—æ®µ", size="sm", variant="primary")
                
                # å­—æ®µé€‰æ‹©æ¡† - è¿™é‡Œæ˜¯æ ¸å¿ƒç»„ä»¶
                field_selector = gr.CheckboxGroup(
                    label="å¯ç”¨å­—æ®µ",
                    choices=[],
                    value=[],
                    interactive=True,
                    info="é€‰æ‹©è¦åœ¨é¢„è§ˆä¸­æ˜¾ç¤ºçš„å­—æ®µ"
                )
        
        # æ•°æ®é›†ä¿¡æ¯æ˜¾ç¤º
        with gr.Row():
            dataset_info = gr.Markdown("*é€‰æ‹©æ•°æ®é›†ä»¥æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯*")
        
        # é¢„è§ˆç»“æœæ˜¾ç¤º - æ”¹è¿›ä¸ºæ›´å¤§çš„æ˜¾ç¤ºåŒºåŸŸ
        result_display = gr.Dataframe(
            label="æ•°æ®å†…å®¹é¢„è§ˆ",
            interactive=False,
            wrap=True
        )
        
        # å±•å¼€/æŠ˜å æ–‡æœ¬æ§åˆ¶
        with gr.Row():
            with gr.Column():
                expanded_text = gr.Textbox(
                    label="å®Œæ•´æ–‡æœ¬å†…å®¹",
                    lines=10,
                    interactive=False,
                    visible=False,
                    info="ç‚¹å‡»é¢„è§ˆè¡¨æ ¼ä¸­çš„æˆªæ–­æ–‡æœ¬å¯åœ¨æ­¤å¤„æŸ¥çœ‹å®Œæ•´å†…å®¹"
                )
                expand_text_btn = gr.Button("æŸ¥çœ‹å®Œæ•´æ–‡æœ¬", visible=False, size="sm")
        
        # æ“ä½œçŠ¶æ€æ˜¾ç¤ºåŒºåŸŸ - ç§»åˆ°æœ€ä¸‹é¢
        with gr.Row():
            data_status = gr.Textbox(
                label="æ“ä½œçŠ¶æ€",
                lines=3,
                interactive=False,
                show_copy_button=True,
                info="æ˜¾ç¤ºæ•°æ®æ“ä½œçš„çŠ¶æ€ä¿¡æ¯"
            )
        
        # å­˜å‚¨ç»„ä»¶å¼•ç”¨
        self.components['manage'] = {
            'data_type_filter': data_type_filter,
            'dataset_name_search': dataset_name_search,
            'search_dataset_btn': search_dataset_btn,
            'dataset_list': dataset_list,
            'selected_dataset': selected_dataset,
            'preview_rows': preview_rows,
            'auto_preview': auto_preview,
            'text_truncation': text_truncation,
            'max_text_length': max_text_length,
            'field_selector': field_selector,
            'select_all_fields': select_all_fields,
            'clear_all_fields': clear_all_fields,
            'select_common_fields': select_common_fields,
            'dataset_info': dataset_info,
            'result_display': result_display,
            'data_status': data_status,
            'expanded_text': expanded_text,
            'expand_text_btn': expand_text_btn
        }
        
        # ç¼“å­˜å½“å‰æ•°æ®é›†çš„æ•°æ®å’Œå­—æ®µä¿¡æ¯ï¼Œé¿å…é‡å¤è¯»å–
        self.current_dataset_cache = {
            'path': None,
            'data': None,
            'fields': [],
            'original_preview': None
        }
        
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
        data_type_filter.change(
            fn=self._filter_datasets,
            inputs=[data_type_filter, dataset_name_search],
            outputs=[dataset_list]
        )
        
        search_dataset_btn.click(
            fn=self._filter_datasets,
            inputs=[data_type_filter, dataset_name_search],
            outputs=[dataset_list]
        )
        
        refresh_data_btn.click(
            fn=self._refresh_datasets,
            inputs=[data_type_filter, dataset_name_search],
            outputs=[dataset_list]
        )
        
        preview_data_btn.click(
            fn=self._preview_dataset_with_field_filter,
            inputs=[selected_dataset, preview_rows, text_truncation, max_text_length, field_selector],
            outputs=[result_display, data_status]
        )
        
        delete_data_btn.click(
            fn=self._delete_dataset,
            inputs=[selected_dataset, data_type_filter],
            outputs=[data_status, dataset_list, selected_dataset, dataset_info, result_display]
        )
        
        # æ•°æ®é›†åˆ—è¡¨ç‚¹å‡»äº‹ä»¶ - æ”¯æŒè‡ªåŠ¨é¢„è§ˆå’Œå­—æ®µåŠ è½½
        dataset_list.select(
            fn=self._select_dataset_with_auto_preview,
            inputs=[auto_preview, preview_rows, text_truncation, max_text_length],
            outputs=[selected_dataset, dataset_info, result_display, field_selector, data_status]
        )
        
        # å­—æ®µé€‰æ‹©å™¨å˜åŒ–äº‹ä»¶ - å®æ—¶æ›´æ–°é¢„è§ˆ
        field_selector.change(
            fn=self._update_preview_by_fields,
            inputs=[field_selector, preview_rows, text_truncation, max_text_length],
            outputs=[result_display, data_status]
        )
        
        # å­—æ®µé€‰æ‹©æŒ‰é’®äº‹ä»¶
        select_all_fields.click(
            fn=self._select_all_dataset_fields,
            outputs=[field_selector]
        )
        
        clear_all_fields.click(
            fn=self._clear_all_dataset_fields,
            outputs=[field_selector]
        )
        
        select_common_fields.click(
            fn=self._select_common_dataset_fields,
            outputs=[field_selector]
        )
    
    def _start_format_convert_async(self, source_file, target_format: str, output_dir: str) -> Tuple[pd.DataFrame, str]:
        """å¼€å§‹æ ¼å¼è½¬æ¢ - å¼‚æ­¥ç‰ˆæœ¬"""
        try:
            if source_file is None:
                return pd.DataFrame(), "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"
            
            source_path = source_file.name
            if not os.path.exists(source_path):
                return pd.DataFrame(), "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # åˆ›å»ºè½¬æ¢ä»»åŠ¡ - æ”¹è¿›ï¼šä¸ä½¿ç”¨å­ç›®å½•ï¼Œç›´æ¥è¾“å‡ºåˆ°æŒ‡å®šç›®å½•
            task_id = convert_format(
                source_path=source_path,
                target_format=target_format,
                output_dir=output_dir,
                use_subdirectory=False  # ç›´æ¥è¾“å‡ºåˆ°æŒ‡å®šç›®å½•ï¼Œé¿å…å¤æ‚è·¯å¾„
            )
            
            # å¯åŠ¨ä»»åŠ¡
            success = start_convert(task_id)
            if not success:
                return pd.DataFrame(), f"âŒ å¯åŠ¨è½¬æ¢ä»»åŠ¡å¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}"
            
            # è¿”å›æ›´æ–°åçš„ä»»åŠ¡åˆ—è¡¨
            task_df = self._get_convert_tasks_df()
            
            return task_df, f"âœ… è½¬æ¢ä»»åŠ¡å·²å¯åŠ¨ï¼\nä»»åŠ¡ID: {task_id}\næ–‡ä»¶: {os.path.basename(source_path)}\nç›®æ ‡æ ¼å¼: {target_format.upper()}"
            
        except Exception as e:
            self.logger.error(f'å¯åŠ¨æ ¼å¼è½¬æ¢å¤±è´¥: {e}')
            return pd.DataFrame(), f"âŒ å¯åŠ¨è½¬æ¢å¤±è´¥: {str(e)}"
    
    def _get_convert_tasks_df(self) -> pd.DataFrame:
        """è·å–è½¬æ¢ä»»åŠ¡åˆ—è¡¨"""
        try:
            tasks = list_converts()
            if not tasks:
                return pd.DataFrame(columns=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"])
            
            task_data = []
            for task in tasks:
                task_id = task.get('task_id', 'N/A')
                source_path = task.get('source_path', '')
                source_file = os.path.basename(source_path) if source_path else 'N/A'
                target_format = task.get('target_format', 'N/A').upper()
                status = task.get('status', 'unknown')
                progress = f"{task.get('progress', 0)}%"
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                if status == 'completed':
                    # æ ¹æ®æ–°çš„å‘½åè§„åˆ™æ„å»ºæ–‡ä»¶å
                    source_stem = Path(source_path).stem if source_path else 'unknown'
                    source_ext = Path(source_path).suffix.replace('.', '') if source_path else ''
                    target_ext = 'md' if target_format.lower() == 'markdown' else target_format.lower()
                    output_filename = f"{source_stem}_{source_ext}2{target_ext}.{target_ext}"
                else:
                    output_filename = "è½¬æ¢ä¸­..."
                
                task_data.append([
                    task_id[:15] + "..." if len(task_id) > 15 else task_id,
                    source_file,
                    target_format,
                    status,
                    progress,
                    output_filename
                ])
            
            return pd.DataFrame(task_data, columns=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"])
            
        except Exception as e:
            self.logger.error(f'è·å–è½¬æ¢ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["ä»»åŠ¡ID", "æºæ–‡ä»¶", "ç›®æ ‡æ ¼å¼", "çŠ¶æ€", "è¿›åº¦", "è¾“å‡ºæ–‡ä»¶"])
    
    def _select_convert_task(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©è½¬æ¢ä»»åŠ¡"""
        try:
            row_data = evt.row_value
            if row_data and len(row_data) >= 1:
                return row_data[0]  # è¿”å›ä»»åŠ¡ID
            return ""
        except Exception as e:
            self.logger.error(f'é€‰æ‹©è½¬æ¢ä»»åŠ¡å¤±è´¥: {e}')
            return ""
    
    def _view_convert_result(self, task_id: str) -> str:
        """æŸ¥çœ‹è½¬æ¢ç»“æœè¯¦æƒ…"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„ä»»åŠ¡"
            
            # è·å–å®Œæ•´ä»»åŠ¡IDï¼ˆå¦‚æœè¢«æˆªæ–­ï¼‰
            tasks = list_converts()
            full_task_id = None
            for task in tasks:
                if task.get('task_id', '').startswith(task_id.replace('...', '')):
                    full_task_id = task.get('task_id')
                    break
            
            if not full_task_id:
                return f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}"
            
            progress = get_convert_progress(full_task_id)
            status = progress.get('status', 'unknown')
            
            if status == 'completed':
                # ä»ä»»åŠ¡è¿›åº¦ä¸­è·å–å®é™…çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
                output_file_path = progress.get('output_file')
                
                if output_file_path and Path(output_file_path).exists():
                    output_file = Path(output_file_path)
                    file_size = output_file.stat().st_size / (1024 * 1024)  # MB
                    return f"""âœ… è½¬æ¢å®Œæˆï¼
ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file.name}
ğŸ“‚ å®Œæ•´è·¯å¾„: {output_file}
ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB
ğŸ“ˆ å¤„ç†è¡Œæ•°: {progress.get('processed_rows', 'N/A')}
ğŸ• ä»»åŠ¡ID: {full_task_id}

ğŸ’¡ æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥ç›´æ¥æ‰“å¼€ä½¿ç”¨"""
                else:
                    # å¦‚æœè¿›åº¦ä¸­æ²¡æœ‰è¾“å‡ºè·¯å¾„ï¼Œå°è¯•æ¨æµ‹ï¼ˆå‘åå…¼å®¹ï¼‰
                    source_path = ""
                    target_format = ""
                    for task in tasks:
                        if task.get('task_id') == full_task_id:
                            source_path = task.get('source_path', '')
                            target_format = task.get('target_format', '')
                            break
                    
                    if source_path and target_format:
                        # å°è¯•ä¸¤ç§å¯èƒ½çš„è·¯å¾„ï¼šæœ‰å­ç›®å½•å’Œæ— å­ç›®å½•
                        source_stem = Path(source_path).stem
                        source_ext = Path(source_path).suffix.replace('.', '')
                        target_ext = 'md' if target_format.lower() == 'markdown' else target_format.lower()
                        output_filename = f"{source_stem}_{source_ext}2{target_ext}.{target_ext}"
                        
                        # ä¼˜å…ˆæ£€æŸ¥æœ‰ä»»åŠ¡IDå­ç›®å½•çš„è·¯å¾„
                        output_dir_with_id = Path(self.root_dir / "processed") / full_task_id
                        output_file_with_id = output_dir_with_id / output_filename
                        
                        # å…¶æ¬¡æ£€æŸ¥æ— å­ç›®å½•çš„è·¯å¾„
                        output_dir_direct = Path(self.root_dir / "processed")
                        output_file_direct = output_dir_direct / output_filename
                        
                        if output_file_with_id.exists():
                            output_file = output_file_with_id
                            file_size = output_file.stat().st_size / (1024 * 1024)  # MB
                            return f"""âœ… è½¬æ¢å®Œæˆï¼
ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_filename}
ğŸ“‚ å®Œæ•´è·¯å¾„: {output_file}
ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB
ğŸ“ˆ å¤„ç†è¡Œæ•°: {progress.get('processed_rows', 'N/A')}
ğŸ• ä»»åŠ¡ID: {full_task_id}

ğŸ’¡ æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥ç›´æ¥æ‰“å¼€ä½¿ç”¨"""
                        elif output_file_direct.exists():
                            output_file = output_file_direct
                            file_size = output_file.stat().st_size / (1024 * 1024)  # MB
                            return f"""âœ… è½¬æ¢å®Œæˆï¼
ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_filename}
ğŸ“‚ å®Œæ•´è·¯å¾„: {output_file}
ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB
ğŸ“ˆ å¤„ç†è¡Œæ•°: {progress.get('processed_rows', 'N/A')}
ğŸ• ä»»åŠ¡ID: {full_task_id}

ğŸ’¡ æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥ç›´æ¥æ‰“å¼€ä½¿ç”¨"""
                        else:
                            return f"""âŒ è½¬æ¢å®Œæˆä½†æ–‡ä»¶ä¸å­˜åœ¨
æ£€æŸ¥è·¯å¾„:
1. {output_file_with_id}
2. {output_file_direct}
ä»»åŠ¡ID: {full_task_id}"""
                    else:
                        return "âŒ æ— æ³•è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯"
            
            elif status == 'failed':
                error_msg = progress.get('error_msg', 'æœªçŸ¥é”™è¯¯')
                return f"âŒ è½¬æ¢å¤±è´¥\né”™è¯¯: {error_msg}\nä»»åŠ¡ID: {full_task_id}"
            
            elif status in ['pending', 'running']:
                progress_pct = progress.get('progress', 0)
                processed = progress.get('processed_rows', 0)
                speed = progress.get('speed', 'è®¡ç®—ä¸­')
                eta = progress.get('eta', 'æœªçŸ¥')
                
                return f"""ğŸ”„ è½¬æ¢è¿›è¡Œä¸­...
ğŸ“Š è¿›åº¦: {progress_pct}%
ğŸ“ˆ å·²å¤„ç†: {processed} è¡Œ
âš¡ é€Ÿåº¦: {speed}
â±ï¸ é¢„è®¡å‰©ä½™: {eta}
ğŸ†” ä»»åŠ¡ID: {full_task_id}"""
            
            else:
                return f"â“ æœªçŸ¥çŠ¶æ€: {status}\nä»»åŠ¡ID: {full_task_id}"
                
        except Exception as e:
            self.logger.error(f'æŸ¥çœ‹è½¬æ¢ç»“æœå¤±è´¥: {e}')
            return f"âŒ æŸ¥çœ‹ç»“æœå¤±è´¥: {str(e)}"
    
    def _start_format_convert(self, source_file, target_format: str, output_dir: str) -> str:
        """å¼€å§‹æ ¼å¼è½¬æ¢ - åŒæ­¥ç‰ˆæœ¬ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        task_df, message = self._start_format_convert_async(source_file, target_format, output_dir)
        return message
        """å¼€å§‹æ ¼å¼è½¬æ¢ - ä¿®å¤ç‰ˆ2"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"
            
            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # åˆ›å»ºè½¬æ¢ä»»åŠ¡
            task_id = convert_format(
                source_path=source_path,
                target_format=target_format,
                output_dir=output_dir
            )
            
            # å¯åŠ¨ä»»åŠ¡
            success = start_convert(task_id)
            if not success:
                return f"âŒ å¯åŠ¨è½¬æ¢ä»»åŠ¡å¤±è´¥ï¼Œä»»åŠ¡ID: {task_id}"
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼ˆç®€å•çš„åŒæ­¥ç­‰å¾…ï¼‰
            max_wait = 120  # æœ€å¤šç­‰å¾…120ç§’ï¼ˆå¤§æ–‡ä»¶éœ€è¦æ›´å¤šæ—¶é—´ï¼‰
            wait_time = 0
            
            while wait_time < max_wait:
                progress = get_convert_progress(task_id)
                status = progress.get('status', 'unknown')
                
                if status == 'completed':
                    # æ ¹æ®ä»»åŠ¡IDæ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
                    expected_output_dir = Path(output_dir) / task_id
                    expected_output_file = expected_output_dir / f"converted.{target_format}"
                    
                    if expected_output_file.exists():
                        return f"""âœ… æ ¼å¼è½¬æ¢å®Œæˆï¼
ğŸ“ è¾“å‡ºæ–‡ä»¶: converted.{target_format}
ğŸ“‚ ä¿å­˜ä½ç½®: {expected_output_file}
ğŸ“Š å¤„ç†è¡Œæ•°: {progress.get('processed_rows', 'N/A')}
ğŸ• è½¬æ¢æ—¶é—´: {progress.get('duration', 'N/A')}
ğŸ“‹ ä»»åŠ¡ID: {task_id}

ğŸ’¡ æç¤ºï¼šæ–‡ä»¶å·²ä¿å­˜åˆ° {expected_output_file}"""
                    else:
                        # å°è¯•æŸ¥æ‰¾è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
                        if expected_output_dir.exists():
                            output_files = list(expected_output_dir.glob(f"*.{target_format}"))
                            if output_files:
                                actual_file = output_files[0]
                                return f"""âœ… æ ¼å¼è½¬æ¢å®Œæˆï¼
ğŸ“ è¾“å‡ºæ–‡ä»¶: {actual_file.name}
ğŸ“‚ ä¿å­˜ä½ç½®: {actual_file}
ğŸ“Š å¤„ç†è¡Œæ•°: {progress.get('processed_rows', 'N/A')}
ğŸ• è½¬æ¢æ—¶é—´: {progress.get('duration', 'N/A')}
ï¿½ ä»»åŠ¡ID: {task_id}"""
                        
                        return f"âŒ è½¬æ¢å®Œæˆä½†æ‰¾ä¸åˆ°è¾“å‡ºæ–‡ä»¶\né¢„æœŸä½ç½®: {expected_output_file}\nä»»åŠ¡ID: {task_id}"
                        
                elif status == 'failed':
                    error_msg = progress.get('error_msg', 'æœªçŸ¥é”™è¯¯')
                    return f"âŒ è½¬æ¢å¤±è´¥: {error_msg}\nä»»åŠ¡ID: {task_id}"
                
                elif status in ['pending', 'running']:
                    # æ˜¾ç¤ºè¿›åº¦
                    progress_pct = progress.get('progress', 0)
                    processed = progress.get('processed_rows', 0)
                    if wait_time % 5 == 0:  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡UIåé¦ˆ
                        self.logger.info(f"è½¬æ¢è¿›åº¦: {progress_pct}%, å·²å¤„ç†: {processed} è¡Œ")
                    time.sleep(1)
                    wait_time += 1
                else:
                    return f"âŒ æœªçŸ¥ä»»åŠ¡çŠ¶æ€: {status}\nä»»åŠ¡ID: {task_id}"
            
            return f"â³ è½¬æ¢è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ä»»åŠ¡çŠ¶æ€\nä»»åŠ¡ID: {task_id}\né¢„æœŸè¾“å‡ºç›®å½•: {Path(output_dir) / task_id}"
            
        except Exception as e:
            self.logger.error(f'æ ¼å¼è½¬æ¢å¤±è´¥: {e}')
            return f"âŒ è½¬æ¢å¤±è´¥: {str(e)}"
    
    def _reset_field_selection(self):
        """é‡ç½®å­—æ®µé€‰æ‹©çŠ¶æ€"""
        self.logger.info("ğŸ”„ é‡ç½®å­—æ®µé€‰æ‹©çŠ¶æ€")
        return (
            gr.CheckboxGroup(choices=[], value=[], label="é€‰æ‹©å­—æ®µ", info="è¯·å…ˆé¢„è§ˆå­—æ®µ"),
            gr.Dataframe(
                headers=["åŸå­—æ®µå", "æ–°å­—æ®µå"],
                datatype=["str", "str"],
                row_count=0,
                col_count=(2, "fixed"),
                interactive=True,
                label="å­—æ®µæ˜ å°„è¡¨",
                visible=False
            )
        )
    
    def _preview_extract_fields(self, source_file) -> gr.CheckboxGroup:
        """é¢„è§ˆå¯æå–çš„å­—æ®µ"""
        # é‡ç½®çŠ¶æ€ï¼Œæ¸…é™¤ä¹‹å‰çš„é€‰æ‹©
        self.logger.info(f"é¢„è§ˆå­—æ®µï¼Œæ–‡ä»¶: {source_file.name if source_file else None}")
        try:
            if source_file is None:
                return gr.CheckboxGroup(choices=[], value=[], label="é€‰æ‹©å­—æ®µ")
            
            source_path = source_file.name
            if not os.path.exists(source_path):
                return gr.CheckboxGroup(choices=[], value=[], label="é€‰æ‹©å­—æ®µ - æ–‡ä»¶ä¸å­˜åœ¨")
            
            # ç›´æ¥è·å–å­—æ®µåç§°åˆ—è¡¨
            field_names = self._get_simple_field_names(source_path)
            
            if not field_names:
                return gr.CheckboxGroup(choices=[], value=[], label="é€‰æ‹©å­—æ®µ - æœªæ‰¾åˆ°å­—æ®µ")
            
            # æ˜ç¡®é‡ç½®é€‰æ‹©çŠ¶æ€
            return gr.CheckboxGroup(
                choices=field_names, 
                value=[],  # æ˜ç¡®è®¾ç½®ä¸ºç©ºåˆ—è¡¨
                label=f"é€‰æ‹©å­—æ®µ (å…±{len(field_names)}ä¸ªå­—æ®µ)",
                info="é€‰æ‹©è¦æå–çš„å­—æ®µ"
            )
            
        except Exception as e:
            self.logger.error(f'é¢„è§ˆå­—æ®µå¤±è´¥: {e}')
            return gr.CheckboxGroup(
                choices=[], 
                value=[],
                label=f"é€‰æ‹©å­—æ®µ - é”™è¯¯: {str(e)}"
            )
    
    def _get_simple_field_names(self, file_path: str) -> List[str]:
        """è·å–æ–‡ä»¶çš„å­—æ®µåç§°åˆ—è¡¨ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰"""
        try:
            # ä½¿ç”¨é€šç”¨å­—æ®µæå–å™¨è·å–å­—æ®µå
            return get_field_names_universal(file_path)
        except Exception as e:
            self.logger.error(f'è·å–å­—æ®µåç§°å¤±è´¥: {e}')
            return []
    
    def _start_field_extract(self, source_file, fields: List[str], output_dir: str) -> str:
        """å¼€å§‹å­—æ®µæå–ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"

            if not fields:
                return "âŒ è¯·é€‰æ‹©è¦æå–çš„å­—æ®µ"

            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"

            try:
                # ä½¿ç”¨é€šç”¨å­—æ®µæå–å™¨
                self.logger.info(f"å¼€å§‹å­—æ®µæå–: {fields}")
                result_path = extract_fields_universal(
                    source_path=source_path,
                    fields=fields,
                    output_dir=output_dir or str(self.root_dir / 'processed')
                )

                if result_path and os.path.exists(result_path):
                    file_size = os.path.getsize(result_path)
                    self.logger.info(f"å­—æ®µæå–å®Œæˆ: {result_path}")
                    return f"âœ… å­—æ®µæå–å®Œæˆï¼\\næå–å­—æ®µ: {', '.join(fields)}\\nè¾“å‡ºæ–‡ä»¶: {result_path}\\næ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚"
                else:
                    return f"âŒ å­—æ®µæå–å¤±è´¥\\nè¯·æ£€æŸ¥æºæ–‡ä»¶æ ¼å¼å’Œé€‰æ‹©çš„å­—æ®µ"

            except Exception as e:
                self.logger.error(f'å­—æ®µæå–å¼‚å¸¸: {e}')
                return f"âŒ æå–å¼‚å¸¸: {str(e)}"
            
        except Exception as e:
            self.logger.error(f'å­—æ®µæå–å¤±è´¥: {e}')
            return f"âŒ æå–å¤±è´¥: {str(e)}"
    
    def _update_field_mapping(self, selected_fields) -> gr.Dataframe:
        """æ ¹æ®é€‰æ‹©çš„å­—æ®µæ›´æ–°å­—æ®µæ˜ å°„è¡¨"""
        try:
            if not selected_fields:
                return gr.Dataframe(
                    value=[],
                    headers=["åŸå­—æ®µå", "æ–°å­—æ®µå"],
                    visible=False
                )
            
            # åˆ›å»ºæ˜ å°„è¡¨æ•°æ®ï¼Œé»˜è®¤æ–°å­—æ®µåå’ŒåŸå­—æ®µåç›¸åŒ
            mapping_data = [[field, field] for field in selected_fields]
            
            return gr.Dataframe(
                value=mapping_data,
                headers=["åŸå­—æ®µå", "æ–°å­—æ®µå"],
                datatype=["str", "str"],
                interactive=True,
                visible=True
            )
            
        except Exception as e:
            self.logger.error(f'æ›´æ–°å­—æ®µæ˜ å°„å¤±è´¥: {e}')
            return gr.Dataframe(
                value=[],
                headers=["åŸå­—æ®µå", "æ–°å­—æ®µå"],
                visible=False
            )
    
    def _start_field_extract_with_mapping(self, source_file, fields, field_mapping_df, output_dir: str) -> str:
        """å¼€å§‹å­—æ®µæå–ï¼ˆæ”¯æŒå­—æ®µé‡å‘½åå’Œè¿›åº¦æ˜¾ç¤ºï¼‰"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"

            if not fields or len(fields) == 0:
                return "âŒ è¯·å…ˆé¢„è§ˆå­—æ®µå¹¶é€‰æ‹©è¦æå–çš„å­—æ®µ"

            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"

            # åˆ›å»ºè¿›åº¦çŠ¶æ€å˜é‡
            progress_status = {"message": "ğŸš€ å‡†å¤‡å¼€å§‹å­—æ®µæå–...", "percent": 0}
            
            def progress_callback(message, percent):
                """è¿›åº¦å›è°ƒå‡½æ•°"""
                progress_status["message"] = message
                progress_status["percent"] = percent
                self.logger.info(f"æå–è¿›åº¦: {percent}% - {message}")

            try:
                # å¤„ç†å­—æ®µæ˜ å°„
                field_mapping = {}
                
                # å®‰å…¨å¤„ç†field_mapping_dfï¼Œå¯èƒ½æ˜¯DataFrameã€åˆ—è¡¨æˆ–None
                mapping_data = []
                if field_mapping_df is not None:
                    try:
                        # å¦‚æœæ˜¯pandas DataFrame
                        if hasattr(field_mapping_df, 'values'):
                            mapping_data = field_mapping_df.values.tolist()
                        # å¦‚æœæ˜¯åˆ—è¡¨
                        elif isinstance(field_mapping_df, list):
                            mapping_data = field_mapping_df
                        # å¦‚æœæ˜¯å…¶ä»–å¯è¿­ä»£å¯¹è±¡
                        elif hasattr(field_mapping_df, '__iter__'):
                            mapping_data = list(field_mapping_df)
                        
                        self.logger.info(f"æ˜ å°„æ•°æ®ç±»å‹: {type(field_mapping_df)}")
                        self.logger.info(f"å¤„ç†åçš„æ˜ å°„æ•°æ®: {mapping_data}")
                        
                    except Exception as e:
                        self.logger.error(f"å¤„ç†æ˜ å°„æ•°æ®å¤±è´¥: {e}")
                        mapping_data = []
                
                # è§£ææ˜ å°„æ•°æ®
                if mapping_data:
                    for row in mapping_data:
                        try:
                            if isinstance(row, (list, tuple)) and len(row) >= 2:
                                original_field = str(row[0]).strip() if row[0] else ""
                                new_field = str(row[1]).strip() if row[1] else ""
                                
                                if original_field and new_field:
                                    field_mapping[original_field] = new_field
                                    self.logger.info(f"å­—æ®µæ˜ å°„: {original_field} -> {new_field}")
                        except Exception as e:
                            self.logger.error(f"å¤„ç†æ˜ å°„è¡Œå¤±è´¥: {row}, é”™è¯¯: {e}")
                            continue

                # ä½¿ç”¨é€šç”¨å­—æ®µæå–å™¨ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
                self.logger.info(f"ğŸ” å­—æ®µæå–è°ƒè¯•ä¿¡æ¯:")
                self.logger.info(f"   æºæ–‡ä»¶: {source_path}")
                self.logger.info(f"   é€‰ä¸­å­—æ®µåˆ—è¡¨: {fields}")
                self.logger.info(f"   å­—æ®µç±»å‹: {type(fields)}")
                self.logger.info(f"   å­—æ®µé•¿åº¦: {len(fields) if fields else 0}")
                self.logger.info(f"   å­—æ®µæ˜ å°„: {field_mapping}")
                
                result_path = extract_fields_universal(
                    source_path=source_path,
                    fields=fields,
                    output_dir=output_dir or str(self.root_dir / 'processed'),
                    field_mapping=field_mapping,
                    progress_callback=progress_callback
                )

                if result_path and os.path.exists(result_path):
                    file_size = os.path.getsize(result_path)
                    self.logger.info(f"å­—æ®µæå–å®Œæˆ: {result_path}")
                    
                    # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
                    mapping_info = ""
                    if field_mapping:
                        mapping_list = [f"{k} -> {v}" for k, v in field_mapping.items()]
                        mapping_info = f"\\nå­—æ®µæ˜ å°„: {', '.join(mapping_list)}"
                    
                    return f"âœ… å­—æ®µæå–å®Œæˆï¼\\næå–å­—æ®µ: {', '.join(fields)}{mapping_info}\\nè¾“å‡ºæ–‡ä»¶: {result_path}\\næ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚"
                else:
                    return f"âŒ å­—æ®µæå–å¤±è´¥\\nè¯·æ£€æŸ¥æºæ–‡ä»¶æ ¼å¼å’Œé€‰æ‹©çš„å­—æ®µ"

            except Exception as e:
                self.logger.error(f'å­—æ®µæå–å¼‚å¸¸: {e}')
                return f"âŒ æå–å¼‚å¸¸: {str(e)}"
            
        except Exception as e:
            self.logger.error(f'å­—æ®µæå–å¤±è´¥: {e}')
            return f"âŒ æå–å¤±è´¥: {str(e)}"
    
    def _start_field_extract_with_progress(self, source_file, fields, field_mapping_df, output_dir: str) -> str:
        """å¼€å§‹å­—æ®µæå–ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
        import time
        
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"

            if not fields:
                return "âŒ è¯·é€‰æ‹©è¦æå–çš„å­—æ®µ"

            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"

            # è¿›åº¦ä¿¡æ¯æ”¶é›†
            progress_log = []
            
            def progress_callback(message, percent):
                """è¿›åº¦å›è°ƒå‡½æ•°"""
                timestamp = time.strftime("%H:%M:%S")
                progress_info = f"[{timestamp}] {percent:3.0f}% - {message}"
                progress_log.append(progress_info)
                self.logger.info(progress_info)

            try:
                # å¼€å§‹æå–å‰çš„å‡†å¤‡å·¥ä½œ
                progress_callback("ğŸš€ å¼€å§‹åˆå§‹åŒ–å­—æ®µæå–ä»»åŠ¡...", 0)
                
                # å¤„ç†å­—æ®µæ˜ å°„
                field_mapping = {}
                mapping_data = []
                
                if field_mapping_df is not None:
                    try:
                        if hasattr(field_mapping_df, 'values'):
                            mapping_data = field_mapping_df.values.tolist()
                        elif isinstance(field_mapping_df, list):
                            mapping_data = field_mapping_df
                        elif hasattr(field_mapping_df, '__iter__'):
                            mapping_data = list(field_mapping_df)
                        
                        progress_callback("âœ… å­—æ®µæ˜ å°„æ•°æ®å¤„ç†å®Œæˆ", 5)
                        
                    except Exception as e:
                        progress_callback(f"âš ï¸ æ˜ å°„æ•°æ®å¤„ç†å¼‚å¸¸: {e}", 5)
                        mapping_data = []
                
                # è§£ææ˜ å°„æ•°æ®
                if mapping_data:
                    for row in mapping_data:
                        try:
                            if isinstance(row, (list, tuple)) and len(row) >= 2:
                                original_field = str(row[0]).strip() if row[0] else ""
                                new_field = str(row[1]).strip() if row[1] else ""
                                
                                if original_field and new_field:
                                    field_mapping[original_field] = new_field
                        except Exception as e:
                            continue
                    
                    if field_mapping:
                        progress_callback(f"ğŸ·ï¸ å­—æ®µé‡å‘½åæ˜ å°„å·²è®¾ç½®: {len(field_mapping)} ä¸ªå­—æ®µ", 8)

                # ä½¿ç”¨é€šç”¨å­—æ®µæå–å™¨ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
                progress_callback("ğŸ”„ å¯åŠ¨å­—æ®µæå–å¼•æ“...", 10)
                
                result_path = extract_fields_universal(
                    source_path=source_path,
                    fields=fields,
                    output_dir=output_dir or str(self.root_dir / 'processed'),
                    field_mapping=field_mapping,
                    progress_callback=progress_callback
                )

                if result_path and os.path.exists(result_path):
                    file_size = os.path.getsize(result_path)
                    
                    # æ„å»ºè¯¦ç»†çš„ç»“æœæŠ¥å‘Š
                    mapping_info = ""
                    if field_mapping:
                        mapping_list = [f"{k} â†’ {v}" for k, v in field_mapping.items()]
                        mapping_info = f"\\nğŸ“‹ å­—æ®µæ˜ å°„: {', '.join(mapping_list)}"
                    
                    # åˆå¹¶è¿›åº¦æ—¥å¿—
                    progress_summary = "\\n".join(progress_log)
                    
                    final_result = f"""âœ… å­—æ®µæå–ä»»åŠ¡å®Œæˆï¼

ğŸ“Š æå–è¯¦æƒ…:
â€¢ é€‰æ‹©å­—æ®µ: {', '.join(fields)}{mapping_info}
â€¢ è¾“å‡ºæ–‡ä»¶: {result_path}  
â€¢ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚

ğŸ“ˆ æ‰§è¡Œæ—¥å¿—:
{progress_summary}

ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼"""
                    
                    self.logger.info(f"å­—æ®µæå–å®Œæˆ: {result_path}")
                    return final_result
                else:
                    error_summary = "\\n".join(progress_log)
                    return f"""âŒ å­—æ®µæå–å¤±è´¥

æ‰§è¡Œæ—¥å¿—:
{error_summary}

è¯·æ£€æŸ¥æºæ–‡ä»¶æ ¼å¼å’Œé€‰æ‹©çš„å­—æ®µ"""

            except Exception as e:
                error_summary = "\\n".join(progress_log)
                self.logger.error(f'å­—æ®µæå–å¼‚å¸¸: {e}')
                return f"""âŒ æå–è¿‡ç¨‹å¼‚å¸¸: {str(e)}

æ‰§è¡Œæ—¥å¿—:
{error_summary}"""
            
        except Exception as e:
            self.logger.error(f'å­—æ®µæå–å¤±è´¥: {e}')
            return f"âŒ æå–å¤±è´¥: {str(e)}"
    
    def _add_merge_file(self, file) -> tuple:
        """æ·»åŠ æ–‡ä»¶åˆ°åˆå¹¶åˆ—è¡¨"""
        if file is None:
            return None, self._get_merge_file_list_display()
        
        file_path = file.name
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if file_path in self.merge_file_paths:
            message = f"âš ï¸ æ–‡ä»¶å·²åœ¨åˆ—è¡¨ä¸­: {os.path.basename(file_path)}"
            return None, message + "\n\n" + self._get_merge_file_list_display()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            message = f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            return None, message + "\n\n" + self._get_merge_file_list_display()
        
        # æ·»åŠ æ–‡ä»¶åˆ°åˆ—è¡¨
        self.merge_file_paths.append(file_path)
        
        message = f"âœ… å·²æ·»åŠ æ–‡ä»¶: {os.path.basename(file_path)}"
        return None, message + "\n\n" + self._get_merge_file_list_display()
    
    def _clear_merge_files(self) -> str:
        """æ¸…ç©ºåˆå¹¶æ–‡ä»¶åˆ—è¡¨"""
        self.merge_file_paths = []
        return "ğŸ—‘ï¸ æ–‡ä»¶åˆ—è¡¨å·²æ¸…ç©º\n\n" + self._get_merge_file_list_display()
    
    def _get_merge_file_list_display(self) -> str:
        """è·å–æ–‡ä»¶åˆ—è¡¨æ˜¾ç¤ºå†…å®¹"""
        if not self.merge_file_paths:
            return "ğŸ“ å¾…åˆå¹¶æ–‡ä»¶åˆ—è¡¨ï¼ˆè‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶ï¼‰:\n\næš‚æ— æ–‡ä»¶"
        
        display_text = f"ğŸ“ å¾…åˆå¹¶æ–‡ä»¶åˆ—è¡¨ï¼ˆå…± {len(self.merge_file_paths)} ä¸ªæ–‡ä»¶ï¼‰:\n\n"
        for i, path in enumerate(self.merge_file_paths, 1):
            file_name = os.path.basename(path)
            file_size = os.path.getsize(path) if os.path.exists(path) else 0
            size_mb = file_size / (1024 * 1024)
            display_text += f"{i}. {file_name} ({size_mb:.2f} MB)\n"
            display_text += f"   è·¯å¾„: {path}\n\n"
        
        return display_text.strip()

    def _start_data_merge(self, mode: str, dedup_field: str, output_dir: str) -> str:
        """å¼€å§‹æ•°æ®åˆå¹¶"""
        try:
            if len(self.merge_file_paths) < 2:
                return "âŒ è¯·æ·»åŠ è‡³å°‘2ä¸ªæ–‡ä»¶è¿›è¡Œåˆå¹¶"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for path in self.merge_file_paths:
                if not os.path.exists(path):
                    return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}"
            
            # è°ƒç”¨æ•°æ®åˆå¹¶æ¨¡å—
            try:
                result_path = merge_data(
                    source_paths=self.merge_file_paths,
                    mode=mode,
                    output_dir=output_dir,
                    dedup_field=dedup_field.strip() if dedup_field.strip() else None
                )
            except Exception as merge_error:
                # æ•è·åˆå¹¶è¿‡ç¨‹ä¸­çš„å…·ä½“é”™è¯¯
                error_msg = f"âŒ åˆå¹¶è¿‡ç¨‹å¤±è´¥: {str(merge_error)}"
                self.logger.error(f'åˆå¹¶è¿‡ç¨‹å¼‚å¸¸: {merge_error}')
                return error_msg
            
            if result_path:
                # è·å–è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
                output_size = os.path.getsize(result_path) if os.path.exists(result_path) else 0
                output_size_mb = output_size / (1024 * 1024)
                
                merge_info = f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼\n\n"
                merge_info += f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {os.path.basename(result_path)}\n"
                merge_info += f"ğŸ“ å®Œæ•´è·¯å¾„: {result_path}\n"
                merge_info += f"ğŸ“Š æ–‡ä»¶å¤§å°: {output_size_mb:.2f} MB\n"
                merge_info += f"ğŸ”— åˆå¹¶æ¨¡å¼: {mode}\n"
                merge_info += f"ğŸ“ æºæ–‡ä»¶æ•°é‡: {len(self.merge_file_paths)}\n"
                
                if dedup_field.strip():
                    merge_info += f"ğŸ” å»é‡å­—æ®µ: {dedup_field.strip()}\n"
                
                # æ˜¾ç¤ºåˆå¹¶çš„æ–‡ä»¶åˆ—è¡¨
                merge_info += f"\nï¿½ å·²åˆå¹¶çš„æºæ–‡ä»¶:\n"
                for i, path in enumerate(self.merge_file_paths, 1):
                    file_size = os.path.getsize(path) if os.path.exists(path) else 0
                    file_size_mb = file_size / (1024 * 1024)
                    merge_info += f"  {i}. {os.path.basename(path)} ({file_size_mb:.2f} MB)\n"
                
                merge_info += f"\nğŸ’¡ æç¤º: åˆå¹¶åçš„æ–‡ä»¶å·²ä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼"
                
                return merge_info
            else:
                # result_path ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²çš„æƒ…å†µ
                debug_info = f"âŒ åˆå¹¶å¤±è´¥: åˆå¹¶å‡½æ•°è¿”å›äº†ç©ºç»“æœ\n\n"
                debug_info += f"è°ƒè¯•ä¿¡æ¯:\n"
                debug_info += f"- æºæ–‡ä»¶æ•°é‡: {len(self.merge_file_paths)}\n"
                debug_info += f"- åˆå¹¶æ¨¡å¼: {mode}\n"
                debug_info += f"- è¾“å‡ºç›®å½•: {output_dir}\n"
                debug_info += f"- å»é‡å­—æ®µ: {dedup_field.strip() if dedup_field.strip() else 'æ— '}\n"
                debug_info += f"- æºæ–‡ä»¶åˆ—è¡¨:\n"
                for i, path in enumerate(self.merge_file_paths, 1):
                    exists = os.path.exists(path)
                    debug_info += f"  {i}. {path} ({'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'})\n"
                
                self.logger.error(f'åˆå¹¶è¿”å›ç©ºç»“æœ - æ–‡ä»¶: {self.merge_file_paths}, æ¨¡å¼: {mode}')
                return debug_info
            
        except Exception as e:
            self.logger.error(f'æ•°æ®åˆå¹¶å¤±è´¥: {e}')
            return f"âŒ åˆå¹¶å¤±è´¥: {str(e)}"
    
    def _start_data_clean(self, source_file, operations: List[str], empty_fields: str,
                          empty_mode: str, dedup_field: str, dedup_threshold: float,
                          sensitive_words: str, sensitive_action: str, sensitive_replacement: str,
                          sensitive_fields: str, sensitive_exclude_fields: str, sensitive_field_policies: str,
                          sensitive_use_regex: bool, sensitive_case_sensitive: bool,
                          pii_enable: List[str], pii_repl_default: str, pii_repl_map: str,
                          normalize_modes: List[str]) -> str:
        """å¼€å§‹æ•°æ®æ¸…æ´—ï¼ˆæ–°ç‰ˆå‚æ•°è”åŠ¨ï¼‰"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ–‡ä»¶"
            if not operations:
                return "âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ¸…æ´—æ“ä½œ"
            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨"

            params: Dict[str, Any] = {
                'source_path': source_path,
                'operations': operations
            }
            # å»ç©ºå­—æ®µä¸ç­–ç•¥ (ä»…å½“é€‰æ‹©äº† remove_empty)
            if 'remove_empty' in operations:
                if empty_fields and empty_fields.strip():
                    params['remove_empty_fields'] = [f.strip() for f in empty_fields.split(',') if f.strip()]
                if empty_mode:
                    params['empty_mode'] = empty_mode

            # æ¨¡ç³Šå»é‡
            if 'deduplicate' in operations and dedup_field and dedup_field.strip():
                params['dedup_field'] = dedup_field.strip()
                if dedup_threshold:
                    params['dedup_threshold'] = float(dedup_threshold)

            # æ•æ„Ÿè¯
            if 'filter_sensitive' in operations:
                if sensitive_words and sensitive_words.strip():
                    params['sensitive_words'] = [w.strip() for w in sensitive_words.split(',') if w.strip()]
                params['sensitive_action'] = sensitive_action or 'drop_record'
                if sensitive_action == 'replace_word':
                    params['sensitive_replacement'] = sensitive_replacement or '***'
                if sensitive_fields and sensitive_fields.strip():
                    params['sensitive_fields'] = [f.strip() for f in sensitive_fields.split(',') if f.strip()]
                if sensitive_exclude_fields and sensitive_exclude_fields.strip():
                    params['sensitive_exclude_fields'] = [f.strip() for f in sensitive_exclude_fields.split(',') if f.strip()]
                if sensitive_field_policies and sensitive_field_policies.strip():
                    # è§£æ: å­—æ®µ:åŠ¨ä½œ[:æ›¿æ¢]
                    mapping = {}
                    for seg in sensitive_field_policies.split(','):
                        seg = seg.strip()
                        if not seg:
                            continue
                        parts = seg.split(':')
                        if len(parts) >= 2:
                            field = parts[0].strip()
                            act = parts[1].strip()
                            repl = None
                            if len(parts) >= 3:
                                repl = ':'.join(parts[2:]).strip()  # æ”¯æŒæ›¿æ¢æ–‡æœ¬ä¸­å†å«å†’å·
                            if field:
                                mapping[field] = (act, repl)
                    if mapping:
                        params['sensitive_field_policies_parsed'] = mapping
                        params['sensitive_field_policies'] = sensitive_field_policies
                if sensitive_use_regex:
                    params['sensitive_use_regex'] = True
                if sensitive_case_sensitive:
                    params['sensitive_case_sensitive'] = True

            # PII è„±æ•
            if 'pii_desensitize' in operations:
                if pii_enable:
                    params['pii_enable'] = pii_enable
                repl_map: Dict[str, str] = {}
                if pii_repl_default and pii_repl_default.strip():
                    repl_map['default'] = pii_repl_default.strip()
                if pii_repl_map and pii_repl_map.strip():
                    # è§£æ  id_card:<ID>,phone:<TEL>
                    parts = [p.strip() for p in pii_repl_map.split(',') if p.strip()]
                    for p in parts:
                        if ':' in p:
                            k, v = p.split(':', 1)
                            if k.strip() and v.strip():
                                repl_map[k.strip()] = v.strip()
                if repl_map:
                    params['pii_replacements'] = repl_map

            # æ–‡æœ¬æ ‡å‡†åŒ–
            if 'normalize_text' in operations and normalize_modes:
                params['normalize_modes'] = normalize_modes

            # æ—§ç‰ˆå­—æ®µè„±æ•å·²ç§»é™¤ï¼Œä¸å†é€ä¼ 

            task_id = data_cleaner.start_clean(params)
            user_params = {k: v for k, v in params.items() if k not in ['source_path', 'operations']}
            # è¿›ä¸€æ­¥åªå±•ç¤ºå½“å‰æ“ä½œçœŸæ­£ç›¸å…³çš„å‚æ•°ï¼Œå‡å°‘æ··æ·†
            op_related_keys = set()
            if 'remove_empty' in operations:
                op_related_keys.update({'remove_empty_fields', 'empty_mode'})
            if 'deduplicate' in operations:
                op_related_keys.update({'dedup_field', 'dedup_threshold'})
            if 'filter_sensitive' in operations:
                op_related_keys.update({'sensitive_words', 'sensitive_action', 'sensitive_replacement', 'sensitive_fields',
                                        'sensitive_exclude_fields', 'sensitive_field_policies', 'sensitive_use_regex',
                                        'sensitive_case_sensitive'})
            if 'pii_desensitize' in operations:
                op_related_keys.update({'pii_enable', 'pii_replacements'})
            if 'normalize_text' in operations:
                op_related_keys.update({'normalize_modes'})
            # desensitize å·²åºŸå¼ƒ
            filtered_params = {k: user_params[k] for k in user_params if k in op_related_keys}
            return ("âœ… æ•°æ®æ¸…æ´—ä»»åŠ¡å·²å¯åŠ¨ï¼\n"
                    f"ä»»åŠ¡ID: {task_id}\n"
                    f"æ¸…æ´—æ“ä½œ: {', '.join(operations)}\n"
                    f"æœ‰æ•ˆå‚æ•°: {json.dumps(filtered_params, ensure_ascii=False)}")
        except Exception as e:
            self.logger.error(f'æ•°æ®æ¸…æ´—å¤±è´¥: {e}')
            return f"âŒ æ¸…æ´—ä»»åŠ¡å¯åŠ¨å¤±è´¥: {e}"

    def _preview_sensitive_processing(self, text: str, sensitive_words: str, action: str, replacement: str,
                                      sensitive_fields: str, sensitive_exclude_fields: str, field_policies: str,
                                      use_regex: bool, case_sensitive: bool) -> str:
        """å¯¹å•æ¡æ–‡æœ¬è¿›è¡Œæ•æ„Ÿè¯å¤„ç†é¢„è§ˆï¼Œä¸å†™å…¥æ–‡ä»¶ã€‚

        é€»è¾‘ï¼šæ„é€ ä¸€ä¸ªä¸´æ—¶æ•°æ® dictï¼ŒåªåŒ…å«ä¸€ä¸ªå­—æ®µ previewï¼›æ ¹æ®ç”¨æˆ·å‚æ•°æ‰§è¡Œ _process_sensitiveï¼Œè¿”å›å‰åå¯¹æ¯”ä¸å‘½ä¸­ç»Ÿè®¡ã€‚
        å­—æ®µçº§ç­–ç•¥å¦‚æœåŒ…å« preview:åŠ¨ä½œ[:æ›¿æ¢] ä¼šç”Ÿæ•ˆã€‚
        """
        try:
            from .data_cleaner import data_cleaner as _dc  # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯
            if not text or not text.strip():
                return "âš ï¸ è¯·è¾“å…¥è¦é¢„è§ˆçš„æ–‡æœ¬"
            words = [w.strip() for w in (sensitive_words or '').split(',') if w.strip()] or _dc.default_sensitive_words
            data = {'preview': text}
            params = {}
            allowed = [f.strip() for f in sensitive_fields.split(',') if f.strip()] if sensitive_fields else None
            exclude = [f.strip() for f in sensitive_exclude_fields.split(',') if f.strip()] if sensitive_exclude_fields else None
            mapping = {}
            if field_policies and field_policies.strip():
                for seg in field_policies.split(','):
                    seg = seg.strip()
                    if not seg:
                        continue
                    parts = seg.split(':')
                    if len(parts) >= 2:
                        fld = parts[0].strip()
                        act = parts[1].strip()
                        repl = None
                        if len(parts) >= 3:
                            repl = ':'.join(parts[2:]).strip()
                        mapping[fld] = (act, repl)
            # ç»Ÿè®¡å®¹å™¨æ¨¡æ‹Ÿ
            stats = {'sensitive_detail': {'field_hits': {}, 'word_hits': {}}}
            hit, modified, dropped = _dc._process_sensitive(
                data,
                words,
                action or 'drop_record',
                replacement or '***',
                allowed,
                exclude,
                mapping,
                bool(use_regex),
                bool(case_sensitive),
                stats
            )
            if dropped:
                return "ğŸ›‘ ç»“æœ: è®°å½•å°†è¢«ä¸¢å¼ƒ (drop_record è§¦å‘)\n\nåŸæ–‡æœ¬:\n" + text
            new_text = data.get('preview', '')
            if not hit:
                return "âœ… æœªå‘½ä¸­ä»»ä½•æ•æ„Ÿè¯\n\nåŸæ–‡æœ¬:\n" + text
            detail = stats['sensitive_detail']
            return (
                "ğŸ¯ å‘½ä¸­æ•æ„Ÿè¯é¢„è§ˆ\n" +
                f"åŠ¨ä½œ: {action}\n\n" +
                "åŸæ–‡æœ¬:\n" + text + "\n\n" +
                "å¤„ç†å:\n" + new_text + "\n\n" +
                "å­—æ®µå‘½ä¸­ç»Ÿè®¡:" + json.dumps(detail['field_hits'], ensure_ascii=False) + "\n" +
                "è¯å‘½ä¸­ç»Ÿè®¡:" + json.dumps(detail['word_hits'], ensure_ascii=False)
            )
        except Exception as e:
            self.logger.error(f'æ•æ„Ÿè¯è¯•è¿è¡Œå¤±è´¥: {e}')
            return f"âŒ è¯•è¿è¡Œå¤±è´¥: {e}"
    def _add_model(self, name: str, model_type: str, url: str, 
                  api_key: str, model_name: str) -> Tuple[str, pd.DataFrame]:
        """æ·»åŠ æ–°æ¨¡å‹"""
        try:
            if not name.strip():
                return "âŒ è¯·è¾“å…¥æ¨¡å‹åç§°", self._get_models_df()
            
            if not url.strip():
                return "âŒ è¯·è¾“å…¥æ¨¡å‹URL", self._get_models_df()
            
            if not model_name.strip():
                return "âŒ è¯·è¾“å…¥å®é™…æ¨¡å‹å", self._get_models_df()
            
            # é‡åé¢„æ£€æŸ¥ï¼Œæä¾›æ›´å‹å¥½çš„é”™è¯¯æç¤º
            existing = model_manager.get_all_models()
            if name.strip() in existing:
                return f"âŒ æ¨¡å‹åç§°å·²å­˜åœ¨ï¼Œè¯·æ›´æ¢åç§°æˆ–å…ˆåˆ é™¤: {name}", self._get_models_df()

            # æ„å»ºæ¨¡å‹é…ç½®
            model_info = {
                'name': name.strip(),
                'type': model_type,
                'url': url.strip(),
                'model_name': model_name.strip()
            }
            
            if api_key.strip():
                model_info['api_key'] = api_key.strip()
            
            # è°ƒç”¨æ¨¡å‹ç®¡ç†æ¨¡å—
            success = model_manager.add_model(model_info)
            
            if success:
                return f"âœ… æ¨¡å‹å·²æ·»åŠ : {name}", self._get_models_df()
            else:
                return f"âŒ æ¨¡å‹æ·»åŠ å¤±è´¥: {name}", self._get_models_df()
                
        except Exception as e:
            self.logger.error(f'æ·»åŠ æ¨¡å‹å¤±è´¥: {e}')
            return f"âŒ æ·»åŠ å¤±è´¥: {str(e)}", self._get_models_df()
    
    def _test_all_models(self) -> Tuple[str, pd.DataFrame]:
        """æµ‹è¯•æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…å«ç¦»çº¿/æœªçŸ¥çŠ¶æ€ï¼‰"""
        try:
            # è·å–å…¨éƒ¨æ¨¡å‹ï¼ˆåç§° -> é…ç½®ï¼‰
            all_models = model_manager.get_all_models()
            names = list(all_models.keys())
            if not names:
                return "âŒ æ²¡æœ‰å·²æ³¨å†Œçš„æ¨¡å‹", self._get_models_df()

            results = []
            for model_name in names:
                try:
                    result = model_manager.test_model(model_name)
                    if result.get('success'):
                        rt = result.get('response_time', 0)
                        status = f"âœ… æ­£å¸¸ ({rt:.2f}ms)"
                    else:
                        status = f"âŒ å¤±è´¥: {result.get('error_msg') or result.get('error', 'Unknown error')}"
                    results.append(f"æ¨¡å‹ {model_name}: {status}")
                except Exception as e:
                    results.append(f"æ¨¡å‹ {model_name}: âŒ å¼‚å¸¸: {str(e)}")

            return f"ğŸ”§ æµ‹è¯•å®Œæˆ:\n" + "\n".join(results), self._get_models_df()

        except Exception as e:
            self.logger.error(f'æµ‹è¯•æ‰€æœ‰æ¨¡å‹å¤±è´¥: {e}')
            return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}", self._get_models_df()
    
    def _test_model(self, model_name: str) -> Tuple[str, pd.DataFrame]:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        try:
            if not model_name.strip():
                return "âŒ è¯·é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹", self._get_models_df()
            
            result = model_manager.test_model(model_name.strip())
            
            if result.get('success'):
                response_time = result.get('response_time', 0)
                return f"âœ… æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸï¼\nå“åº”æ—¶é—´: {response_time:.2f}s", self._get_models_df()
            else:
                error_msg = result.get('error', 'Unknown error')
                return f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {error_msg}", self._get_models_df()
                
        except Exception as e:
            self.logger.error(f'æµ‹è¯•æ¨¡å‹å¤±è´¥: {e}')
            return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}", self._get_models_df()
    
    def _delete_model(self, model_name: str) -> Tuple[str, pd.DataFrame]:
        """åˆ é™¤æ¨¡å‹"""
        try:
            if not model_name.strip():
                return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¨¡å‹", self._get_models_df()
            
            success = model_manager.delete_model(model_name.strip())
            
            if success:
                return f"âœ… æ¨¡å‹å·²åˆ é™¤: {model_name}", self._get_models_df()
            else:
                return f"âŒ æ¨¡å‹åˆ é™¤å¤±è´¥: {model_name}", self._get_models_df()
                
        except Exception as e:
            self.logger.error(f'åˆ é™¤æ¨¡å‹å¤±è´¥: {e}')
            return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}", self._get_models_df()
    
    def _refresh_models(self) -> pd.DataFrame:
        """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
        return self._get_models_df()
    
    def _select_model(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©æ¨¡å‹"""
        if evt.index[1] == 0:  # ç‚¹å‡»æ¨¡å‹åç§°åˆ—
            return evt.value
        return ""
    
    def _get_models_df(self) -> pd.DataFrame:
        """è·å–æ¨¡å‹åˆ—è¡¨æ•°æ®æ¡†"""
        try:
            # ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹é…ç½®ï¼Œé¿å…é€ä¸ªå®æ—¶è¿æµ‹å¯¼è‡´é˜»å¡
            all_models = model_manager.get_all_models()  # { name: config }

            if not all_models:
                return pd.DataFrame(columns=["æ¨¡å‹åç§°", "ç±»å‹", "çŠ¶æ€", "URL", "å“åº”æ—¶é—´", "æ“ä½œ"])

            # æ„å»ºæ•°æ®æ¡†ï¼ˆä½¿ç”¨å·²è®°å½•çš„ status/response_timeï¼‰
            rows = []
            for name, cfg in all_models.items():
                model_type = (cfg.get('type') or '').upper()
                url = cfg.get('url', '')
                status_val = cfg.get('status', 'unknown')
                rt_ms = cfg.get('response_time', 0)

                if status_val == 'online':
                    status = "âœ… æ­£å¸¸"
                elif status_val == 'offline':
                    status = "âŒ ç¦»çº¿"
                elif status_val == 'error':
                    status = "âš ï¸ é”™è¯¯"
                else:
                    status = "â“ æœªçŸ¥"

                # å±•ç¤ºä¸ºæ¯«ç§’ï¼Œé¿å…è¯¯è§£ä¸ºç§’
                response_time = f"{rt_ms:.2f}ms" if rt_ms else "-"

                display_url = url[:50] + "..." if len(url) > 50 else url

                rows.append([
                    name,
                    model_type,
                    status,
                    display_url,
                    response_time,
                    "ç‚¹å‡»é€‰æ‹©"
                ])

            return pd.DataFrame(rows, columns=["æ¨¡å‹åç§°", "ç±»å‹", "çŠ¶æ€", "URL", "å“åº”æ—¶é—´", "æ“ä½œ"])
            
        except Exception as e:
            self.logger.error(f'è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["æ¨¡å‹åç§°", "ç±»å‹", "çŠ¶æ€", "URL", "å“åº”æ—¶é—´", "æ“ä½œ"])
    
    def _refresh_distill_models(self) -> gr.Dropdown:
        """åˆ·æ–°è’¸é¦æ¨¡å‹åˆ—è¡¨"""
        try:
            models = model_manager.get_active_models()
            model_choices = [model.get('name', '') for model in models if model.get('name')]
            
            return gr.Dropdown(choices=model_choices, value=model_choices[0] if model_choices else None)
            
        except Exception as e:
            self.logger.error(f'åˆ·æ–°æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}')
            return gr.Dropdown(choices=[])
    
    def _start_distill_generation(self,
                                 source_file,
                                 strategy: str,
                                 model_name: str,
                                 count: int,
                                 temperature: float,
                                 max_tokens: int,
                                 top_p: float,
                                 top_k: int,
                                 max_workers: int,
                                 fsync_interval: int,
                                 checkpoint_interval: int,
                                 inflight_multiplier: int,
                                 unordered_write: bool,
                                 rate_limit_rps: Optional[float],
                                 max_backoff: Optional[float],
                                 system_prompt: str,
                                 q_prompt: str,
                                 a_prompt: str,
                                 selected_fields: str,
                                 q_field_name: str,
                                 label_set: str,
                                 target_field: str) -> Tuple[str, pd.DataFrame]:
        """å¼€å§‹è’¸é¦ç”Ÿæˆï¼ˆå¯¹é½æ–°ç‰ˆç­–ç•¥ä¸å‚æ•°ï¼‰"""
        try:
            if source_file is None:
                return "âŒ è¯·é€‰æ‹©æºæ•°æ®æ–‡ä»¶", self._get_distill_tasks_df()
            
            if not model_name:
                return "âŒ è¯·é€‰æ‹©ç”Ÿæˆæ¨¡å‹", self._get_distill_tasks_df()
            
            source_path = source_file.name
            if not os.path.exists(source_path):
                return "âŒ æºæ–‡ä»¶ä¸å­˜åœ¨", self._get_distill_tasks_df()
            
            # æ ¡éªŒæ ¼å¼
            ext = os.path.splitext(source_path)[1].lower()
            if ext not in [".jsonl", ".json"]:
                return "âŒ ä»…æ”¯æŒ .jsonl æˆ– .json æºæ•°æ®æ–‡ä»¶", self._get_distill_tasks_df()
            
            # æ„å»ºç”Ÿæˆå‚æ•°
            params = {
                'strategy': strategy,
                'model_id': model_name,
                'input_file': source_path,
                'temperature': float(temperature),
                'max_tokens': int(max_tokens),
                'top_p': float(top_p),
                'max_workers': int(max_workers),
                'fsync_interval': int(fsync_interval),
                'checkpoint_interval': int(checkpoint_interval),
                'inflight_multiplier': int(inflight_multiplier)
            }

            # generation_count ä»…å¯¹éœ€è¦çš„ç­–ç•¥ç”Ÿæ•ˆ
            strategies_need_count = {'expand', 'paraphrase', 'q_to_a'}
            if strategy in strategies_need_count:
                params['generation_count'] = int(count)

            # é‡‡æ ·å‚æ•°
            if isinstance(top_k, (int, float)) and int(top_k) > 0:
                params['top_k'] = int(top_k)

            # å¹¶å‘/é²æ£’æ€§
            if unordered_write:
                params['unordered_write'] = True
            if rate_limit_rps is not None and str(rate_limit_rps).strip() != "":
                try:
                    params['rate_limit_rps'] = float(rate_limit_rps)
                except Exception:
                    pass
            if max_backoff is not None and str(max_backoff).strip() != "":
                try:
                    params['max_backoff'] = float(max_backoff)
                except Exception:
                    pass

            # æç¤ºè¯
            if system_prompt and system_prompt.strip():
                params['system_prompt'] = system_prompt.strip()
            if q_prompt and q_prompt.strip():
                params['q_prompt'] = q_prompt.strip()
            if a_prompt and a_prompt.strip():
                params['a_prompt'] = a_prompt.strip()

            # å­—æ®µ/æ ‡ç­¾/ç›®æ ‡
            if selected_fields and selected_fields.strip():
                params['selected_fields'] = selected_fields.strip()
            # Q å­—æ®µåï¼ˆè¾“å‡ºåï¼‰é»˜è®¤ instruction
            q_out = (q_field_name or '').strip() or 'instruction'
            params['q_field_name'] = q_out
            if label_set and label_set.strip():
                params['label_set'] = label_set.strip()
            # A å­—æ®µåï¼ˆç›®æ ‡å­—æ®µï¼‰é»˜è®¤ output
            tgt = (target_field or '').strip() or 'output'
            params['target_field'] = tgt
            
            # è°ƒç”¨è’¸é¦ç”Ÿæˆæ¨¡å—
            task_id = distill_generator.start_generation(params)
            
            return f"âœ… è’¸é¦ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨ï¼\nä»»åŠ¡ID: {task_id}\nç­–ç•¥: {strategy}\næ¨¡å‹: {model_name}", self._get_distill_tasks_df()
            
        except Exception as e:
            self.logger.error(f'å¯åŠ¨è’¸é¦ç”Ÿæˆå¤±è´¥: {e}')
            return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}", self._get_distill_tasks_df()

    def _on_strategy_change(self, strategy: str):
        """ç­–ç•¥åˆ‡æ¢æ—¶ï¼Œæ›´æ–°è¯´æ˜ä¸æ§ä»¶å¯è§æ€§"""
        try:
            # è·å–ç­–ç•¥æè¿°
            desc = distill_generator.get_strategy_description(strategy)
            if 'error' in desc:
                md = f"âŒ æœªçŸ¥ç­–ç•¥: {strategy}"
            else:
                name = desc.get('name', strategy)
                description = desc.get('description', '')
                req = desc.get('required_params', [])
                opt = desc.get('optional_params', [])
                req_str = ', '.join(req) if req else 'æ— '
                opt_str = ', '.join(opt) if opt else 'æ— '
                md = f"### {name}\n\n{description}\n\n- å¿…éœ€å‚æ•°ï¼š{req_str}\n- å¯é€‰å‚æ•°ï¼š{opt_str}"

            # æ§ä»¶å¯è§æ€§
            need_count = strategy in {'expand', 'paraphrase', 'q_to_a'}
            show_q_a = strategy in {'q_to_a', 'custom'}
            show_label = strategy == 'classify_label'

            return (
                md,
                gr.update(visible=need_count),
                gr.update(visible=show_q_a),
                gr.update(visible=show_q_a),
                gr.update(visible=show_label)
            )
        except Exception as e:
            self.logger.error(f'ç­–ç•¥åˆ‡æ¢æ›´æ–°å¤±è´¥: {e}')
            # å¤±è´¥æ—¶é»˜è®¤å…¨éƒ¨éšè—å¯é€‰æ§ä»¶
            return (
                f"âŒ æ›´æ–°ç­–ç•¥è¯´æ˜å¤±è´¥: {e}",
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False)
            )
    
    def _pause_distill_generation(self, task_id: str) -> Tuple[str, pd.DataFrame]:
        """æš‚åœè’¸é¦ç”Ÿæˆ"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦æš‚åœçš„ä»»åŠ¡", self._get_distill_tasks_df()
            
            # è¿™é‡Œç”±äºdistill_generatoræ²¡æœ‰pauseæ–¹æ³•ï¼Œæˆ‘ä»¬æ›´æ–°çŠ¶æ€
            state_manager.update_state(task_id.strip(), 'status', 'paused')
            
            return f"âœ… ä»»åŠ¡å·²æš‚åœ: {task_id}", self._get_distill_tasks_df()
            
        except Exception as e:
            self.logger.error(f'æš‚åœè’¸é¦ç”Ÿæˆå¤±è´¥: {e}')
            return f"âŒ æš‚åœå¤±è´¥: {str(e)}", self._get_distill_tasks_df()
    
    def _refresh_distill_tasks(self) -> pd.DataFrame:
        """åˆ·æ–°è’¸é¦ä»»åŠ¡åˆ—è¡¨"""
        return self._get_distill_tasks_df()

    def _on_distill_model_change(self, model_name: str):
        """æ ¹æ®æ¨¡å‹ç±»å‹åŠ¨æ€è°ƒæ•´ max_tokens ä¸Šé™ä¸æç¤º
        è§„åˆ™ï¼ˆä¿å®ˆï¼‰ï¼š
        - OPENAI: ä¸Šé™ 8192ï¼ˆé¿å… 400 é”™ï¼‰
        - VLLM/OLLAMA/SGLANG: ä¸Šé™ 200000ï¼ˆå¯è°ƒé«˜ï¼Œè§†æ¨¡å‹ä¸éƒ¨ç½²ï¼‰
        - å…¶ä»–/æœªçŸ¥: ä¸Šé™ 4000ï¼ˆé»˜è®¤ï¼‰
        """
        try:
            if not model_name:
                return gr.update()
            # è·å–è¯¥æ¨¡å‹é…ç½®
            all_models = model_manager.get_all_models()
            cfg = all_models.get(model_name) if isinstance(all_models, dict) else None
            mtype = (cfg.get('type') or '').upper() if cfg else ''

            # è®¾å®šä¸Šé™ä¸è¯´æ˜
            if mtype == 'OPENAI':
                max_cap = 8192
                info = "æœ€å¤§Tokenæ•°ï¼ˆOPENAI å…¼å®¹ï¼šå»ºè®®<=8192ï¼Œè¶…å‡ºå¯èƒ½æŠ¥é”™ï¼‰"
            elif mtype in ('VLLM', 'OLLAMA', 'SGLANG'):
                max_cap = 200000
                info = "æœ€å¤§Tokenæ•°ï¼ˆæœ¬åœ°/å…¼å®¹åç«¯ï¼šå·²æ”¾å®½è‡³ 200000ï¼Œè¯·æŒ‰æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶åˆç†è®¾ç½®ï¼‰"
            else:
                max_cap = 4000
                info = "æœ€å¤§Tokenæ•°ï¼ˆæœªçŸ¥åç«¯ï¼šä¿æŒé»˜è®¤ä¸Šé™ 4000ï¼‰"

            return gr.update(maximum=max_cap, info=info)
        except Exception:
            return gr.update()
    
    def _view_distill_report(self, task_id: str) -> str:
        """æŸ¥çœ‹è’¸é¦æŠ¥å‘Š"""
        try:
            if not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„ä»»åŠ¡"
            
            report = distill_generator.get_quality_report(task_id.strip())
            
            if 'error' in report:
                return f"âŒ è·å–æŠ¥å‘Šå¤±è´¥: {report['error']}"
            
            # æ ¼å¼åŒ–æŠ¥å‘Šä¿¡æ¯
            metrics = report.get('quality_metrics', {})
            report_text = f"""ğŸ“‹ ä»»åŠ¡ {task_id} è´¨é‡æŠ¥å‘Š

ğŸ¯ åŸºæœ¬ä¿¡æ¯:
  ç­–ç•¥: {report.get('generation_strategy', 'Unknown')}
  æ¨¡å‹: {report.get('model_id', 'Unknown')}

ğŸ“Š ç”Ÿæˆç»Ÿè®¡:
  è¾“å…¥é¡¹ç›®æ•°: {metrics.get('total_input_items', 0)}
  ç”Ÿæˆé¡¹ç›®æ•°: {metrics.get('total_generated_items', 0)}
  è´¨é‡é€šè¿‡æ•°: {metrics.get('quality_passed_items', 0)}
  è´¨é‡é€šè¿‡ç‡: {metrics.get('quality_pass_rate', 0):.1f}%
  ç”ŸæˆæˆåŠŸç‡: {metrics.get('generation_success_rate', 0):.1f}%
  å¹³å‡ç”Ÿæˆå€æ•°: {metrics.get('average_generations_per_input', 0):.1f}

â° ç”Ÿæˆæ—¶é—´: {report.get('generated_time', 'Unknown')}
"""
            
            return report_text
            
        except Exception as e:
            self.logger.error(f'æŸ¥çœ‹è’¸é¦æŠ¥å‘Šå¤±è´¥: {e}')
            return f"âŒ æŸ¥çœ‹æŠ¥å‘Šå¤±è´¥: {str(e)}"

    def _resume_distill_task(self, task_id: str, model_override: Optional[str], workers: int, temperature: float, max_tokens: int, top_p: float, top_k: int, rate_limit_rps: Optional[float], max_backoff: Optional[float], resume_as_new: bool) -> Tuple[str, pd.DataFrame]:
        """æ¢å¤è’¸é¦ä»»åŠ¡ï¼Œæ”¯æŒè¦†ç›–éƒ¨åˆ†å‚æ•°ï¼ˆæ¨¡å‹/å¹¶å‘/é‡‡æ ·ï¼‰ã€‚"""
        try:
            if not task_id or not task_id.strip():
                return "âŒ è¯·é€‰æ‹©è¦æ¢å¤çš„ä»»åŠ¡", self._get_distill_tasks_df()

            overrides = {}
            if model_override:
                overrides['model_id'] = model_override
            if workers and int(workers) > 0:
                overrides['max_workers'] = int(workers)
            if temperature:
                overrides['temperature'] = float(temperature)
            if max_tokens:
                overrides['max_tokens'] = int(max_tokens)
            if top_p:
                overrides['top_p'] = float(top_p)
            if isinstance(top_k, (int, float)) and int(top_k) > 0:
                overrides['top_k'] = int(top_k)
            if rate_limit_rps is not None and str(rate_limit_rps).strip() != "":
                try:
                    overrides['rate_limit_rps'] = float(rate_limit_rps)
                except Exception:
                    pass
            if max_backoff is not None and str(max_backoff).strip() != "":
                try:
                    overrides['max_backoff'] = float(max_backoff)
                except Exception:
                    pass
            if bool(resume_as_new):
                overrides['resume_as_new'] = True

            distill_generator.resume_generation(task_id.strip(), overrides or None)
            return f"âœ… å·²è§¦å‘æ¢å¤: {task_id}", self._get_distill_tasks_df()
        except Exception as e:
            self.logger.error(f'æ¢å¤ä»»åŠ¡å¤±è´¥: {e}')
            return f"âŒ æ¢å¤å¤±è´¥: {str(e)}", self._get_distill_tasks_df()

    def _on_distill_source_change(self, source_file):
        """é€‰æ‹©æºæ•°æ®æ–‡ä»¶åï¼Œæ‰«æè‹¥å¹²è¡Œæ¨æ–­å­—æ®µåˆ—è¡¨ï¼Œå¡«å……å­—æ®µå¤é€‰æ¡†å¹¶åŒæ­¥æ–‡æœ¬æ¡†"""
        try:
            if source_file is None:
                return gr.update(choices=[], value=[]), ""
            path = source_file.name
            if not os.path.exists(path):
                return gr.update(choices=[], value=[]), ""
            ext = os.path.splitext(path)[1].lower()
            fields = set()
            limit = 100  # é‡‡æ ·å‰100è¡Œ
            if ext == '.jsonl':
                with open(path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= limit:
                            break
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            obj = json.loads(line)
                            if isinstance(obj, dict):
                                fields.update(obj.keys())
                        except Exception:
                            continue
            elif ext == '.json':
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        fields.update(data.keys())
                    elif isinstance(data, list):
                        for i, obj in enumerate(data[:limit]):
                            if isinstance(obj, dict):
                                fields.update(obj.keys())
            else:
                return gr.update(choices=[], value=[]), ""

            choices = sorted(list(fields))
            # é»˜è®¤é€‰æ‹©æ¨æµ‹åˆ°çš„å¸¸ç”¨å­—æ®µä¹‹ä¸€
            defaults = []
            for cand in ['instruction', 'input', 'prompt', 'question', 'query']:
                if cand in fields:
                    defaults.append(cand)
                    break
            return gr.update(choices=choices, value=defaults), (", ".join(defaults) if defaults else "")
        except Exception:
            return gr.update(choices=[], value=[]), ""

    def _sync_selected_fields_text(self, selected_list: list) -> str:
        """å°†å¤é€‰æ¡†é€‰æ‹©åŒæ­¥åˆ°æ–‡æœ¬æ¡†ï¼ˆé€—å·åˆ†éš”ï¼‰"""
        try:
            if not selected_list:
                return ""
            return ",".join(selected_list)
        except Exception:
            return ""
    
    def _select_distill_task(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©è’¸é¦ä»»åŠ¡"""
        if evt.index[1] == 0:  # ç‚¹å‡»ä»»åŠ¡IDåˆ—
            return evt.value
        return ""
    
    def _get_distill_tasks_df(self) -> pd.DataFrame:
        """è·å–è’¸é¦ä»»åŠ¡åˆ—è¡¨æ•°æ®æ¡†"""
        try:
            # è·å–æ‰€æœ‰è’¸é¦ä»»åŠ¡ï¼ˆæŒ‰ä»»åŠ¡ç±»å‹ DISTILL è¿‡æ»¤ï¼‰
            tasks = state_manager.list_tasks(task_type=TaskType.DISTILL)
            
            if not tasks:
                return pd.DataFrame(columns=["ä»»åŠ¡ID", "ç­–ç•¥", "æ¨¡å‹", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"])
            
            # æ„å»ºæ•°æ®æ¡†
            rows = []
            for task in tasks:
                task_id = task.get('task_id', '')
                params = task.get('params', {})
                strategy = params.get('strategy', '')
                model_id = params.get('model_id', '')
                status = task.get('status', 'unknown')
                progress = task.get('progress', 0)
                start_time = task.get('start_time', '')
                
                # çŠ¶æ€ä¸­æ–‡æ˜ å°„
                status_map = {
                    'pending': 'ç­‰å¾…ä¸­',
                    'running': 'ç”Ÿæˆä¸­',
                    'paused': 'å·²æš‚åœ',
                    'completed': 'å·²å®Œæˆ',
                    'failed': 'å¤±è´¥'
                }
                
                status_cn = status_map.get(status, status)
                progress_str = f"{progress:.1f}%" if isinstance(progress, (int, float)) else "0%"
                
                # ç­–ç•¥ä¸­æ–‡æ˜ å°„
                strategy_map = {
                    'expand': 'æ•°æ®æ‰©å……',
                    'enhance': 'å†…å®¹å¢å¼º',
                    'paraphrase': 'æ–‡æœ¬æ”¹å†™',
                    'classify_label': 'åˆ†ç±»æ ‡æ³¨',
                    'q_to_a': 'ä»Qç”ŸA',
                    'custom': 'è‡ªå®šä¹‰'
                }
                
                strategy_cn = strategy_map.get(strategy, strategy)
                
                # æ ¼å¼åŒ–å¼€å§‹æ—¶é—´
                if start_time:
                    try:
                        dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                        start_time_str = dt.strftime('%m-%d %H:%M')
                    except:
                        start_time_str = start_time[:16] if len(start_time) > 16 else start_time
                else:
                    start_time_str = ""
                
                rows.append([
                    task_id,
                    strategy_cn,
                    model_id,
                    status_cn,
                    progress_str,
                    start_time_str
                ])
            
            return pd.DataFrame(rows, columns=["ä»»åŠ¡ID", "ç­–ç•¥", "æ¨¡å‹", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"])
            
        except Exception as e:
            self.logger.error(f'è·å–è’¸é¦ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["ä»»åŠ¡ID", "ç­–ç•¥", "æ¨¡å‹", "çŠ¶æ€", "è¿›åº¦", "å¼€å§‹æ—¶é—´"])
    
    def _filter_datasets(self, data_type: str, search_name: str = "") -> pd.DataFrame:
        """æ ¹æ®æ•°æ®ç±»å‹å’Œåç§°ç­›é€‰æ•°æ®é›†"""
        return self._get_datasets_df(data_type, search_name)
    
    def _refresh_datasets(self, data_type: str, search_name: str = "") -> pd.DataFrame:
        """åˆ·æ–°æ•°æ®é›†åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            self.logger.info(f"å¼€å§‹åˆ·æ–°æ•°æ®é›†åˆ—è¡¨: ç±»å‹={data_type}, æœç´¢={search_name}")
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ¥é¿å…é•¿æ—¶é—´é˜»å¡
            import concurrent.futures
            import threading
            
            def get_datasets_with_timeout():
                return self._get_datasets_df(data_type, search_name)
            
            try:
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œï¼Œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(get_datasets_with_timeout)
                    
                    try:
                        # è®¾ç½®30ç§’è¶…æ—¶
                        result = future.result(timeout=30)
                        self.logger.info(f"æ•°æ®é›†åˆ—è¡¨åˆ·æ–°å®Œæˆ: {len(result)} ä¸ªæ•°æ®é›†")
                        return result
                    except concurrent.futures.TimeoutError:
                        self.logger.error("æ•°æ®é›†åˆ—è¡¨åˆ·æ–°è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                        return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
                        
            except Exception as e:
                self.logger.error(f"æ‰§è¡Œæ•°æ®é›†åˆ·æ–°æ—¶å‡ºé”™: {e}")
                return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
            
        except Exception as e:
            self.logger.error(f'åˆ·æ–°æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}')
            # è¿”å›ç©ºçš„DataFrameï¼Œä½†åŒ…å«æ­£ç¡®çš„åˆ—å
            return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
    
    def _preview_dataset(self, dataset_path: str, rows: int, 
                        enable_truncation: bool = True, max_text_length: int = 300,
                        show_metadata: bool = True, show_stats: bool = True,
                        smart_columns: bool = True, show_all_columns: bool = False,
                        column_info_display: bool = False) -> Tuple[pd.DataFrame, str]:
        """é¢„è§ˆæ•°æ®é›† - ä½¿ç”¨æ–°çš„å¢å¼ºé¢„è§ˆå™¨"""
        try:
            if not dataset_path.strip():
                return pd.DataFrame(), "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            if not os.path.exists(dataset_path):
                return pd.DataFrame(), "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨"
            
            # æ›´æ–°é¢„è§ˆå™¨é…ç½®
            self.dataset_previewer.config.max_rows = rows
            self.dataset_previewer.config.enable_truncation = enable_truncation
            self.dataset_previewer.config.max_text_length = max_text_length
            self.dataset_previewer.config.show_stats = show_stats
            self.dataset_previewer.config.include_metadata = show_metadata
            self.dataset_previewer.config.smart_columns = smart_columns
            self.dataset_previewer.config.show_all_columns = show_all_columns
            
            # ä½¿ç”¨å¢å¼ºé¢„è§ˆå™¨é¢„è§ˆæ•°æ®
            preview_result = self.dataset_previewer.preview_dataset(dataset_path, rows)
            
            if not preview_result.success:
                return pd.DataFrame(), f"âŒ é¢„è§ˆå¤±è´¥: {preview_result.error_message}"
            
            if not preview_result.data:
                return pd.DataFrame(), "âŒ æ•°æ®é›†ä¸ºç©º"
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(preview_result.data)
            
            # æ„å»ºè¯¦ç»†çŠ¶æ€ä¿¡æ¯
            status_parts = [f"âœ… é¢„è§ˆæˆåŠŸï¼"]
            
            # åŸºç¡€ä¿¡æ¯
            status_parts.append(f"ğŸ“ è·¯å¾„: {dataset_path}")
            status_parts.append(f"ğŸ“Š æ ¼å¼: {preview_result.format.value.upper()}")
            status_parts.append(f"ğŸ“ˆ æ€»è¡Œæ•°: {preview_result.total_rows:,}")
            status_parts.append(f"ğŸ‘€ é¢„è§ˆè¡Œæ•°: {len(preview_result.data)}")
            status_parts.append(f"ğŸ“‚ æ–‡ä»¶æ•°: {preview_result.total_files}")
            
            # æ–‡ä»¶ä¿¡æ¯
            if preview_result.files:
                file_info = preview_result.files[0]  # ä¸»æ–‡ä»¶
                status_parts.append(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {self._format_size(file_info.size)}")
                if file_info.columns:
                    status_parts.append(f"ğŸ“‹ æ€»åˆ—æ•°: {len(file_info.columns)}")
                    # æ˜¾ç¤ºå½“å‰æ˜¾ç¤ºçš„åˆ—æ•°
                    if preview_result.data:
                        displayed_cols = len(preview_result.data[0].keys())
                        status_parts.append(f"ğŸ‘ï¸ æ˜¾ç¤ºåˆ—æ•°: {displayed_cols}")
                    
                    column_names = list(preview_result.data[0].keys())[:5] if preview_result.data else []
                    if len(column_names) > 5:
                        column_names.append("...")
                    status_parts.append(f"ğŸ”¤ å½“å‰åˆ—å: {', '.join(column_names)}")
            
            # éšè—åˆ—ä¿¡æ¯
            if preview_result.hidden_columns:
                status_parts.append(f"ğŸ™ˆ éšè—åˆ—æ•°: {len(preview_result.hidden_columns)}")
                hidden_names = preview_result.hidden_columns[:3]
                if len(preview_result.hidden_columns) > 3:
                    hidden_names.append("...")
                status_parts.append(f"ğŸ”’ éšè—åˆ—å: {', '.join(hidden_names)}")
            
            # æˆªæ–­ä¿¡æ¯
            if preview_result.truncated_fields:
                status_parts.append(f"âœ‚ï¸ æˆªæ–­å­—æ®µ: {', '.join(preview_result.truncated_fields)}")
            
            # åˆ—ç»Ÿè®¡ä¿¡æ¯
            if column_info_display and preview_result.column_info:
                status_parts.append("\nğŸ“Š åˆ—ç»Ÿè®¡ä¿¡æ¯:")
                for col, stats in list(preview_result.column_info.items())[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—
                    null_rate = stats.get('null_rate', 0) * 100
                    unique_count = stats.get('unique_count', 0)
                    avg_length = stats.get('avg_length', 0)
                    status_parts.append(f"  â€¢ {col}: ç©ºå€¼{null_rate:.1f}%, å”¯ä¸€å€¼{unique_count}, å¹³å‡é•¿åº¦{avg_length:.1f}")
            
            # å…ƒæ•°æ®ä¿¡æ¯
            if show_metadata and preview_result.metadata:
                metadata = preview_result.metadata
                if 'column_types' in metadata:
                    type_summary = {}
                    for col, types in metadata['column_types'].items():
                        for t in types:
                            type_summary[t] = type_summary.get(t, 0) + 1
                    
                    type_str = ", ".join([f"{t}: {count}" for t, count in type_summary.items()])
                    status_parts.append(f"ğŸ”¢ æ•°æ®ç±»å‹: {type_str}")
            
            status_msg = "\n".join(status_parts)
            
            return df, status_msg
            
        except Exception as e:
            self.logger.error(f'é¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}')
            return pd.DataFrame(), f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f}TB"
    
    def _preview_dataset_enhanced(self, dataset_path: str, rows: int, 
                                 enable_truncation: bool, max_text_length: int,
                                 show_metadata: bool, show_stats: bool,
                                 smart_columns: bool, show_all_columns: bool,
                                 column_info_display: bool) -> Tuple[str, pd.DataFrame, str]:
        """å¢å¼ºé¢„è§ˆæ•°æ®é›†ï¼ˆé€šè¿‡é¢„è§ˆæŒ‰é’®è§¦å‘ï¼‰"""
        try:
            if not dataset_path.strip():
                return "*è¯·å…ˆé€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†*", pd.DataFrame(), "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            # è°ƒç”¨å¢å¼ºé¢„è§ˆåŠŸèƒ½
            preview_df, status_msg = self._preview_dataset(
                dataset_path, rows, enable_truncation, max_text_length, 
                show_metadata, show_stats, smart_columns, show_all_columns, column_info_display
            )
            
            # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
            dataset_info = self._generate_dataset_info(dataset_path, preview_df, status_msg)
            
            return dataset_info, preview_df, status_msg
            
        except Exception as e:
            self.logger.error(f'å¢å¼ºé¢„è§ˆæ•°æ®é›†å¤±è´¥: {e}')
            error_msg = f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
            return "*é¢„è§ˆå¤±è´¥*", pd.DataFrame(), error_msg
    
    def _generate_dataset_info(self, dataset_path: str, preview_df: pd.DataFrame, status_msg: str) -> str:
        """ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯Markdown"""
        try:
            if preview_df.empty:
                return "*æš‚æ— æ•°æ®é›†ä¿¡æ¯*"
            
            # è·å–æ•°æ®é›†æ‘˜è¦
            summary = self.dataset_previewer.get_dataset_summary(dataset_path)
            
            if 'error' in summary:
                return f"*æ•°æ®é›†ä¿¡æ¯è·å–å¤±è´¥: {summary['error']}*"
            
            info_parts = [
                f"### ğŸ“Š æ•°æ®é›†è¯¦æƒ…",
                f"**åç§°**: {summary.get('name', 'Unknown')}",
                f"**ç±»å‹**: {summary.get('type', 'Unknown')}",
                f"**æ ¼å¼**: {summary.get('format', 'Unknown').upper()}"
            ]
            
            if summary.get('size_human'):
                info_parts.append(f"**å¤§å°**: {summary['size_human']}")
            
            if summary.get('estimated_rows'):
                info_parts.append(f"**é¢„ä¼°è¡Œæ•°**: {summary['estimated_rows']:,}")
            
            if summary.get('file_count'):
                info_parts.append(f"**æ–‡ä»¶æ•°é‡**: {summary['file_count']}")
            
            if summary.get('formats'):
                info_parts.append(f"**åŒ…å«æ ¼å¼**: {', '.join(summary['formats'])}")
            
            # é¢„è§ˆæ•°æ®ä¿¡æ¯
            if not preview_df.empty:
                info_parts.extend([
                    "",
                    f"### ğŸ“ˆ é¢„è§ˆç»Ÿè®¡",
                    f"**é¢„è§ˆè¡Œæ•°**: {len(preview_df)}",
                    f"**åˆ—æ•°**: {len(preview_df.columns)}",
                    f"**åˆ—å**: {', '.join(preview_df.columns.tolist()[:5])}{'...' if len(preview_df.columns) > 5 else ''}"
                ])
            
            return "\n".join(info_parts)
            
        except Exception as e:
            self.logger.error(f'ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}')
            return f"*æ•°æ®é›†ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {str(e)}*"
    
    def _delete_dataset(self, dataset_path: str, data_type: str) -> Tuple[str, pd.DataFrame, str, str, pd.DataFrame]:
        """åˆ é™¤æ•°æ®é›†"""
        try:
            print(f"[DEBUG] åˆ é™¤è¯·æ±‚: path={dataset_path}, type={data_type}")
            
            if not dataset_path.strip():
                return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ•°æ®é›†", pd.DataFrame(), "", "", pd.DataFrame()
            
            # æ£€æŸ¥è·¯å¾„ç±»å‹
            from pathlib import Path
            path_obj = Path(dataset_path)
            if path_obj.is_file():
                path_type = "æ–‡ä»¶"
            elif path_obj.is_dir():
                path_type = "æ•°æ®é›†ç›®å½•"
            else:
                path_type = "è·¯å¾„"
            
            # è°ƒç”¨æ•°æ®ç®¡ç†æ¨¡å—åˆ é™¤æ•°æ®
            print(f"[DEBUG] å‡†å¤‡åˆ é™¤{path_type}: {dataset_path}")
            success = data_manager.delete_data(dataset_path)
            print(f"[DEBUG] åˆ é™¤ç»“æœ: {success}")
            
            if success:
                # åˆ é™¤æˆåŠŸååˆ·æ–°å½“å‰é€‰ä¸­çš„æ•°æ®ç±»å‹åˆ—è¡¨
                print(f"[DEBUG] åˆ·æ–°æ•°æ®ç±»å‹: {data_type}")
                refreshed_df = self._get_datasets_df(data_type)
                print(f"[DEBUG] åˆ·æ–°åæ•°æ®é›†æ•°é‡: {len(refreshed_df)}")
                
                # æ¸…ç©ºé€‰ä¸­çŠ¶æ€å’Œæ˜¾ç¤ºå†…å®¹
                return (f"âœ… åˆ é™¤{path_type}å®Œæˆï¼\n{path_type}: {dataset_path}", 
                       refreshed_df, 
                       "",  # æ¸…ç©ºselected_dataset
                       "",  # æ¸…ç©ºdataset_info
                       pd.DataFrame())  # æ¸…ç©ºresult_displayç”¨ç©ºDataFrame
            else:
                return f"âŒ åˆ é™¤{path_type}å¤±è´¥ï¼šå¯èƒ½æ–‡ä»¶ä¸å­˜åœ¨æˆ–æƒé™ä¸è¶³", pd.DataFrame(), dataset_path, "", pd.DataFrame()
                
        except Exception as e:
            print(f"[DEBUG] åˆ é™¤å¼‚å¸¸: {e}")
            self.logger.error(f'åˆ é™¤æ•°æ®é›†å¤±è´¥: {e}')
            return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}", pd.DataFrame(), dataset_path, "", pd.DataFrame()
    
    def _get_storage_stats(self) -> str:
        """è·å–å­˜å‚¨ç»Ÿè®¡"""
        try:
            # è°ƒç”¨æ•°æ®ç®¡ç†æ¨¡å—è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = data_manager.get_storage_statistics()
            
            # æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯
            stats_text = f"""ğŸ“Š å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯

ğŸ“ æ•°æ®ç±»å‹ç»Ÿè®¡:
  åŸå§‹æ•°æ®: {stats.get('raw_count', 0)} ä¸ªæ–‡ä»¶
  å¤„ç†æ•°æ®: {stats.get('processed_count', 0)} ä¸ªæ–‡ä»¶  
  è’¸é¦æ•°æ®: {stats.get('distilled_count', 0)} ä¸ªæ–‡ä»¶
  å¤‡ä»½æ•°æ®: {stats.get('backup_count', 0)} ä¸ªæ–‡ä»¶

ğŸ’¾ å­˜å‚¨ç©ºé—´:
  æ€»å¤§å°: {stats.get('total_size_mb', 0):.1f} MB
  åŸå§‹æ•°æ®: {stats.get('raw_size_mb', 0):.1f} MB
  å¤„ç†æ•°æ®: {stats.get('processed_size_mb', 0):.1f} MB
  è’¸é¦æ•°æ®: {stats.get('distilled_size_mb', 0):.1f} MB

ğŸ“ˆ æœ€æ–°æ´»åŠ¨:
  æœ€æ–°æ–‡ä»¶: {stats.get('latest_file', 'N/A')}
  åˆ›å»ºæ—¶é—´: {stats.get('latest_time', 'N/A')}
"""
            
            return stats_text
            
        except Exception as e:
            self.logger.error(f'è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}')
            return f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"
    
    def _select_dataset(self, evt: gr.SelectData) -> str:
        """é€‰æ‹©æ•°æ®é›† - æ”¯æŒç‚¹å‡»ä»»æ„åˆ—"""
        try:
            # è·å–å½“å‰è¡Œçš„æ‰€æœ‰æ•°æ®
            row_data = evt.row_value
            if row_data and len(row_data) >= 5:
                # è¿”å›è·¯å¾„åˆ—ï¼ˆç¬¬5åˆ—ï¼Œç´¢å¼•4ï¼‰çš„å€¼
                return row_data[4]
            return ""
        except Exception as e:
            self.logger.error(f'é€‰æ‹©æ•°æ®é›†å¤±è´¥: {e}')
            return ""
    
    def _select_dataset_with_enhanced_preview(self, evt: gr.SelectData, auto_preview: bool, preview_rows: int,
                                            enable_truncation: bool, max_text_length: int,
                                            show_metadata: bool, show_stats: bool,
                                            smart_columns: bool, show_all_columns: bool,
                                            column_info_display: bool) -> Tuple[str, str, pd.DataFrame, str]:
        """é€‰æ‹©æ•°æ®é›†å¹¶ä½¿ç”¨å¢å¼ºé¢„è§ˆ"""
        try:
            # è·å–å½“å‰è¡Œçš„æ‰€æœ‰æ•°æ®
            row_data = evt.row_value
            if not row_data or len(row_data) < 5:
                return "", "*è¯·é€‰æ‹©æœ‰æ•ˆçš„æ•°æ®é›†*", pd.DataFrame(), ""
            
            # æå–æ•°æ®é›†ä¿¡æ¯
            dataset_name = row_data[0]
            dataset_type = row_data[1]
            dataset_size = row_data[2]
            dataset_time = row_data[3]
            dataset_path = row_data[4]
            
            # å¦‚æœå¯ç”¨è‡ªåŠ¨é¢„è§ˆï¼Œåˆ™æ‰§è¡Œå¢å¼ºé¢„è§ˆ
            if auto_preview and dataset_path:
                preview_df, status_msg = self._preview_dataset(
                    dataset_path, preview_rows, enable_truncation, max_text_length, 
                    show_metadata, show_stats, smart_columns, show_all_columns, column_info_display
                )
                dataset_info = self._generate_dataset_info(dataset_path, preview_df, status_msg)
                return dataset_path, dataset_info, preview_df, status_msg
            else:
                # æ„å»ºåŸºç¡€æ•°æ®é›†ä¿¡æ¯æ˜¾ç¤º
                info_text = f"""**ğŸ“Š æ•°æ®é›†ä¿¡æ¯**
- **åç§°**: {dataset_name}
- **ç±»å‹**: {dataset_type}
- **å¤§å°**: {dataset_size}
- **åˆ›å»ºæ—¶é—´**: {dataset_time}
- **è·¯å¾„**: `{os.path.basename(dataset_path)}`"""
                
                return dataset_path, info_text, pd.DataFrame(), "âœ… æ•°æ®é›†å·²é€‰æ‹©ï¼Œç‚¹å‡»é¢„è§ˆæŒ‰é’®æŸ¥çœ‹æ•°æ®å†…å®¹"
                
        except Exception as e:
            self.logger.error(f'é€‰æ‹©æ•°æ®é›†å¤±è´¥: {e}')
            return "", f"âŒ é€‰æ‹©å¤±è´¥: {str(e)}", pd.DataFrame(), ""
    
    def _get_datasets_df(self, data_type: str = "å…¨éƒ¨", search_name: str = "") -> pd.DataFrame:
        """è·å–æ•°æ®é›†åˆ—è¡¨æ•°æ®æ¡†"""
        try:
            # æ•°æ®ç±»å‹æ˜ å°„
            type_map = {
                "å…¨éƒ¨": None,
                "åŸå§‹æ•°æ®": "raw",
                "å¤„ç†æ•°æ®": "processed", 
                "è’¸é¦æ•°æ®": "distilled"
            }
            
            filter_type = type_map.get(data_type)
            
            # è°ƒç”¨æ•°æ®ç®¡ç†æ¨¡å—è·å–æ•°æ®é›†åˆ—è¡¨
            datasets = data_manager.list_datasets(data_type=filter_type)
            
            if not datasets:
                return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
            
            # è¿‡æ»¤å’Œä¼˜åŒ–æ•°æ®é›†åˆ—è¡¨
            filtered_datasets = self._filter_datasets_for_display(datasets)
            
            # æ ¹æ®åç§°æœç´¢è¿‡æ»¤
            if search_name and search_name.strip():
                search_keyword = search_name.strip().lower()
                filtered_datasets = [
                    ds for ds in filtered_datasets 
                    if search_keyword in ds.get('display_name', '').lower() or
                       search_keyword in ds.get('path', '').lower()
                ]
            
            # æ„å»ºæ•°æ®æ¡†
            rows = []
            for dataset in filtered_datasets:
                file_path = dataset.get('path', '')
                
                # ä½¿ç”¨æ–°çš„åˆ†å±‚æ˜¾ç¤ºé€»è¾‘
                if 'display_name' in dataset:
                    # ä½¿ç”¨åˆ†å±‚æ˜¾ç¤ºåç§°
                    display_name = dataset['display_name']
                    # åˆ†å±‚æ¡ç›®ä½¿ç”¨ provider ä½œä¸ºç±»å‹æ˜¾ç¤ºï¼ˆä¾‹å¦‚ï¼šè’¸é¦æ•°æ®/å¤„ç†æ•°æ®/æä¾›å•†åï¼‰
                    data_type_cn = dataset.get('provider') or dataset.get('type', 'æœªçŸ¥')
                    
                    # æ ¼å¼åŒ–å¤§å°æ˜¾ç¤º
                    size_mb = dataset.get('size_mb', 0)
                    if size_mb > 1024:
                        size_display = f"{size_mb/1024:.1f} GB"
                    elif size_mb > 0:
                        size_display = f"{size_mb:.1f} MB"
                    else:
                        size_display = "è®¡ç®—ä¸­..."
                    
                    # æ–‡ä»¶æ•°é‡ä¿¡æ¯
                    file_count = dataset.get('file_count', 0)
                    if file_count > 1:
                        size_display += f" ({file_count}ä¸ªæ–‡ä»¶)"
                    
                else:
                    # å…¼å®¹æ—§æ ¼å¼ - æ£€æŸ¥æ˜¯å¦ä¸ºæ±‡æ€»æ¡ç›®
                    if dataset.get('is_summary', False):
                        # æ•°æ®é›†æ±‡æ€»æ¡ç›®çš„ç‰¹æ®Šå¤„ç†
                        dataset_name = dataset.get('dataset_name', '')
                        file_count = dataset.get('file_count', 0)
                        organized_count = dataset.get('organized_count', 0)
                        cache_count = dataset.get('cache_count', 0)
                        
                        # æ˜¾ç¤ºåç§°åŒ…å«ç»Ÿè®¡ä¿¡æ¯
                        display_name = f"ğŸ“ {dataset_name} ({file_count}ä¸ªæ–‡ä»¶)"
                        if organized_count > 0:
                            display_name += f" âœ¨{organized_count}æ•´ç†"
                        if cache_count > 0:
                            display_name += f" ğŸ“¦{cache_count}ç¼“å­˜"
                        
                        data_type_cn = "æ•°æ®é›†æ±‡æ€»"
                        
                    else:
                        # æ™®é€šæ–‡ä»¶æ¡ç›®
                        file_name = os.path.basename(file_path)
                        display_name = file_name
                        
                        # ç¡®å®šæ•°æ®ç±»å‹
                        path_norm = (file_path or '').replace('\\', '/')
                        if '/raw/' in path_norm:
                            if '/organized_files/' in path_norm:
                                data_type_cn = "åŸå§‹æ•°æ®(æ•´ç†)"
                            elif '/cache/' in path_norm:
                                data_type_cn = "åŸå§‹æ•°æ®(ç¼“å­˜)"
                            else:
                                data_type_cn = "åŸå§‹æ•°æ®"
                        elif '/distilled/' in path_norm:
                            data_type_cn = "è’¸é¦æ•°æ®"
                        elif '/processed/' in path_norm:
                            data_type_cn = "å¤„ç†æ•°æ®"
                        else:
                            data_type_cn = "å…¶ä»–"
                    
                    # æ–‡ä»¶å¤§å° - æ”¹è¿›æ˜¾ç¤ºæ ¼å¼
                    size_mb = dataset.get('size_mb', 0)
                    if size_mb > 1024:
                        size_display = f"{size_mb/1024:.1f} GB"
                    elif size_mb > 1:
                        size_display = f"{size_mb:.1f} MB"
                    elif size_mb > 0:
                        size_display = f"{size_mb*1024:.0f} KB"
                    else:
                        size_display = "è®¡ç®—ä¸­..."
                
                # åˆ›å»ºæ—¶é—´
                create_time = dataset.get('create_time', '')
                if create_time:
                    try:
                        dt = datetime.fromisoformat(create_time.replace('Z', '+00:00'))
                        time_str = dt.strftime('%Y-%m-%d %H:%M')
                    except:
                        time_str = create_time[:16] if len(create_time) > 16 else create_time
                else:
                    time_str = ""
                
                rows.append([
                    display_name,
                    data_type_cn,
                    size_display,
                    time_str,
                    file_path
                ])
            
            return pd.DataFrame(rows, columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
            
        except Exception as e:
            self.logger.error(f'è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}')
            return pd.DataFrame(columns=["åç§°", "ç±»å‹", "å¤§å°", "åˆ›å»ºæ—¶é—´", "è·¯å¾„"])
    
    def _select_dataset_with_auto_preview(self, evt: gr.SelectData, auto_preview: bool, 
                                        preview_rows: int, enable_truncation: bool, 
                                        max_text_length: int) -> Tuple[str, str, pd.DataFrame, gr.CheckboxGroup, str]:
        """é€‰æ‹©æ•°æ®é›†å¹¶è‡ªåŠ¨é¢„è§ˆï¼ˆæ–°ç‰ˆæœ¬ï¼ŒåŒ…å«å­—æ®µé€‰æ‹©ï¼‰"""
        try:
            # è·å–å½“å‰è¡Œçš„æ‰€æœ‰æ•°æ®
            row_data = evt.row_value
            if not row_data or len(row_data) < 5:
                return "", "*è¯·é€‰æ‹©æœ‰æ•ˆçš„æ•°æ®é›†*", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), ""
            
            # æå–æ•°æ®é›†ä¿¡æ¯
            dataset_name = row_data[0]
            dataset_type = row_data[1]
            dataset_size = row_data[2]
            dataset_time = row_data[3]
            dataset_path = row_data[4]
            
            # å¦‚æœå¯ç”¨è‡ªåŠ¨é¢„è§ˆï¼Œåˆ™æ‰§è¡Œé¢„è§ˆå¹¶åŠ è½½å­—æ®µ
            if auto_preview and dataset_path:
                return self._load_dataset_with_fields(dataset_path, preview_rows, enable_truncation, max_text_length)
            else:
                # æ„å»ºåŸºç¡€æ•°æ®é›†ä¿¡æ¯æ˜¾ç¤º
                info_text = f"""**ğŸ“Š æ•°æ®é›†ä¿¡æ¯**
- **åç§°**: {dataset_name}
- **ç±»å‹**: {dataset_type}
- **å¤§å°**: {dataset_size}
- **åˆ›å»ºæ—¶é—´**: {dataset_time}
- **è·¯å¾„**: `{os.path.basename(dataset_path)}`"""
                
                return dataset_path, info_text, pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), "âœ… æ•°æ®é›†å·²é€‰æ‹©ï¼Œç­‰å¾…é¢„è§ˆ..."
                
        except Exception as e:
            self.logger.error(f'é€‰æ‹©æ•°æ®é›†å¤±è´¥: {e}')
            return "", f"âŒ é€‰æ‹©å¤±è´¥: {str(e)}", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), ""
    
    def _load_dataset_with_fields(self, dataset_path: str, preview_rows: int, 
                                enable_truncation: bool, max_text_length: int) -> Tuple[str, str, pd.DataFrame, gr.CheckboxGroup, str]:
        """åŠ è½½æ•°æ®é›†å¹¶åˆ†æå­—æ®µï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰"""
        try:
            if not dataset_path.strip():
                return "", "*è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†*", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            if not os.path.exists(dataset_path):
                return "", "*æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨*", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ•°æ®é›†ï¼Œå¦‚æœæ˜¯åˆ™æ¸…ç©ºç¼“å­˜
            if self.current_dataset_cache['path'] != dataset_path:
                self.current_dataset_cache = {
                    'path': dataset_path,
                    'data': None,
                    'fields': [],
                    'original_preview': None
                }
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰æ•°æ®ï¼Œåˆ™è¯»å–æ•°æ®é›†
            if self.current_dataset_cache['data'] is None:
                # æ›´æ–°é¢„è§ˆå™¨é…ç½®
                self.dataset_previewer.config.max_rows = preview_rows
                self.dataset_previewer.config.enable_truncation = False  # å…ˆä¸æˆªæ–­ï¼Œä¿å­˜å®Œæ•´æ•°æ®
                self.dataset_previewer.config.max_text_length = max_text_length
                self.dataset_previewer.config.smart_columns = False  # ä¸æ™ºèƒ½éšè—åˆ—ï¼Œæ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
                self.dataset_previewer.config.show_all_columns = True
                
                # ä½¿ç”¨å¢å¼ºé¢„è§ˆå™¨é¢„è§ˆæ•°æ®
                preview_result = self.dataset_previewer.preview_dataset(dataset_path, preview_rows)
                
                if not preview_result.success:
                    return "", "*é¢„è§ˆå¤±è´¥*", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), f"âŒ é¢„è§ˆå¤±è´¥: {preview_result.error_message}"
                
                if not preview_result.data:
                    return "", "*æ•°æ®é›†ä¸ºç©º*", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), "âŒ æ•°æ®é›†ä¸ºç©º"
                
                # ç¼“å­˜æ•°æ®
                self.current_dataset_cache['data'] = preview_result.data
                self.current_dataset_cache['original_preview'] = preview_result
                
                # ä½¿ç”¨universal_field_extractoråˆ†æå­—æ®µï¼ˆä¿®å¤HuggingFaceæ•°æ®é›†é—®é¢˜ï¼‰
                try:
                    field_names = self._get_simple_field_names(dataset_path)
                    if field_names:
                        self.current_dataset_cache['fields'] = field_names
                    else:
                        # å¦‚æœuniversal_field_extractorå¤±è´¥ï¼Œå›é€€åˆ°ä»æ•°æ®ä¸­æå–å­—æ®µ
                        if preview_result.data:
                            available_fields = list(preview_result.data[0].keys())
                            self.current_dataset_cache['fields'] = available_fields
                        else:
                            self.current_dataset_cache['fields'] = []
                except Exception as field_error:
                    self.logger.warning(f'å­—æ®µæå–å¤±è´¥ï¼Œä½¿ç”¨é¢„è§ˆæ•°æ®å­—æ®µ: {field_error}')
                    # å›é€€åˆ°ä»æ•°æ®ä¸­æå–å­—æ®µ
                    if preview_result.data:
                        available_fields = list(preview_result.data[0].keys())
                        self.current_dataset_cache['fields'] = available_fields
                    else:
                        self.current_dataset_cache['fields'] = []
            
            # è·å–å¯ç”¨å­—æ®µ
            available_fields = self.current_dataset_cache['fields']
            
            # è¯†åˆ«å¸¸ç”¨å­—æ®µå¹¶è®¾ä¸ºé»˜è®¤é€‰ä¸­
            common_fields = self._identify_common_fields(available_fields)
            
            # åˆ›å»ºå­—æ®µé€‰æ‹©å™¨
            field_choices = [(field, field) for field in available_fields]
            field_selector = gr.CheckboxGroup(
                label="å¯ç”¨å­—æ®µ",
                choices=field_choices,
                value=common_fields,  # é»˜è®¤é€‰ä¸­å¸¸ç”¨å­—æ®µ
                interactive=True,
                info=f"æ•°æ®é›†åŒ…å« {len(available_fields)} ä¸ªå­—æ®µï¼Œå·²é»˜è®¤é€‰ä¸­å¸¸ç”¨å­—æ®µ"
            )
            
            # ä½¿ç”¨é»˜è®¤é€‰ä¸­çš„å­—æ®µç”Ÿæˆé¢„è§ˆ
            preview_df, status_msg = self._filter_preview_by_fields(common_fields, enable_truncation, max_text_length)
            
            # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
            dataset_info = self._generate_dataset_info_from_cache(dataset_path, preview_df)
            
            return dataset_path, dataset_info, preview_df, field_selector, status_msg
            
        except Exception as e:
            self.logger.error(f'åŠ è½½æ•°æ®é›†å’Œå­—æ®µå¤±è´¥: {e}')
            error_msg = f"âŒ åŠ è½½å¤±è´¥: {str(e)}"
            return "", "*åŠ è½½å¤±è´¥*", pd.DataFrame(), gr.CheckboxGroup(choices=[], value=[]), error_msg
    
    def _identify_common_fields(self, available_fields: List[str]) -> List[str]:
        """è¯†åˆ«å¸¸ç”¨å­—æ®µ"""
        # å¸¸ç”¨å­—æ®µä¼˜å…ˆçº§åˆ—è¡¨ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
        common_patterns = [
            'instruction', 'input', 'output', 'response', 'answer', 'question', 
            'text', 'content', 'prompt', 'completion', 'query', 'reply',
            'conversations', 'messages', 'dialogue', 'chat',
            'title', 'description', 'summary', 'context'
        ]
        
        selected_fields = []
        available_lower = [field.lower() for field in available_fields]
        
        # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©å­—æ®µ
        for pattern in common_patterns:
            for i, field_lower in enumerate(available_lower):
                if pattern in field_lower and available_fields[i] not in selected_fields:
                    selected_fields.append(available_fields[i])
                    break  # æ¯ç§æ¨¡å¼åªé€‰æ‹©ä¸€ä¸ªå­—æ®µ
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å¸¸ç”¨å­—æ®µï¼Œé€‰æ‹©å‰å‡ ä¸ªå­—æ®µ
        if not selected_fields:
            selected_fields = available_fields[:min(3, len(available_fields))]
        
        # é™åˆ¶é€‰ä¸­å­—æ®µæ•°é‡ï¼ˆé¿å…æ˜¾ç¤ºè¿‡å¤šåˆ—ï¼‰
        max_fields = 5
        if len(selected_fields) > max_fields:
            selected_fields = selected_fields[:max_fields]
        
        return selected_fields
    
    def _filter_preview_by_fields(self, selected_fields: List[str], 
                                enable_truncation: bool, max_text_length: int) -> Tuple[pd.DataFrame, str]:
        """æ ¹æ®é€‰ä¸­å­—æ®µè¿‡æ»¤é¢„è§ˆæ•°æ®"""
        try:
            if not self.current_dataset_cache['data']:
                return pd.DataFrame(), "âŒ æ²¡æœ‰ç¼“å­˜çš„æ•°æ®"
            
            if not selected_fields:
                return pd.DataFrame(), "âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå­—æ®µ"
            
            # ä»ç¼“å­˜çš„å®Œæ•´æ•°æ®ä¸­è¿‡æ»¤å­—æ®µ
            filtered_data = []
            for row in self.current_dataset_cache['data']:
                filtered_row = {}
                for field in selected_fields:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—å­—æ®µè·¯å¾„
                    if '.' in field or '[' in field:
                        # ä½¿ç”¨universal_field_extractorçš„æ–¹æ³•æå–åµŒå¥—å€¼
                        try:
                            from universal_field_extractor import _extractor
                            value = _extractor._get_nested_value(row, field)
                        except:
                            value = None
                    else:
                        # ç®€å•å­—æ®µç›´æ¥è·å–
                        value = row.get(field)
                    
                    # å¤„ç†Noneå€¼
                    if value is None:
                        filtered_row[field] = ""
                    else:
                        # åº”ç”¨æ–‡æœ¬æˆªæ–­
                        if enable_truncation and isinstance(value, str) and len(value) > max_text_length:
                            value = value[:max_text_length] + "..."
                        # ç¡®ä¿å€¼å¯ä»¥è¢«pandaså¤„ç†
                        if isinstance(value, (list, dict)):
                            filtered_row[field] = str(value)
                        else:
                            filtered_row[field] = value
                    
                filtered_data.append(filtered_row)
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(filtered_data)
            
            # æ„å»ºçŠ¶æ€ä¿¡æ¯
            preview_result = self.current_dataset_cache['original_preview']
            status_parts = [
                f"âœ… å­—æ®µè¿‡æ»¤é¢„è§ˆæˆåŠŸï¼",
                f"ğŸ“ æ•°æ®é›†: {os.path.basename(self.current_dataset_cache['path'])}",
                f"ğŸ“Š æ€»å­—æ®µæ•°: {len(self.current_dataset_cache['fields'])}",
                f"ğŸ‘ï¸ æ˜¾ç¤ºå­—æ®µæ•°: {len(selected_fields)}",
                f"ğŸ“ˆ æ€»è¡Œæ•°: {preview_result.total_rows:,}",
                f"ğŸ‘€ é¢„è§ˆè¡Œæ•°: {len(filtered_data)}",
                f"ğŸ“‹ é€‰ä¸­å­—æ®µ: {', '.join(selected_fields)}"
            ]
            
            if enable_truncation:
                status_parts.append(f"âœ‚ï¸ æ–‡æœ¬æˆªæ–­: {max_text_length} å­—ç¬¦")
            
            status_msg = "\n".join(status_parts)
            
            return df, status_msg
            
        except Exception as e:
            self.logger.error(f'è¿‡æ»¤é¢„è§ˆæ•°æ®å¤±è´¥: {e}')
            return pd.DataFrame(), f"âŒ è¿‡æ»¤å¤±è´¥: {str(e)}"
    
    def _generate_dataset_info_from_cache(self, dataset_path: str, preview_df: pd.DataFrame) -> str:
        """ä»ç¼“å­˜ä¿¡æ¯ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯"""
        try:
            if not self.current_dataset_cache['original_preview']:
                return "*æš‚æ— æ•°æ®é›†ä¿¡æ¯*"
            
            preview_result = self.current_dataset_cache['original_preview']
            
            info_parts = [
                f"### ğŸ“Š æ•°æ®é›†è¯¦æƒ…",
                f"**åç§°**: {os.path.basename(dataset_path)}",
                f"**æ ¼å¼**: {preview_result.format.value.upper()}",
                f"**æ€»è¡Œæ•°**: {preview_result.total_rows:,}",
                f"**æ€»å­—æ®µæ•°**: {len(self.current_dataset_cache['fields'])}",
                f"**æ–‡ä»¶æ•°**: {preview_result.total_files}"
            ]
            
            # æ–‡ä»¶ä¿¡æ¯
            if preview_result.files:
                file_info = preview_result.files[0]
                info_parts.append(f"**æ–‡ä»¶å¤§å°**: {self._format_size(file_info.size)}")
            
            # é¢„è§ˆä¿¡æ¯
            if not preview_df.empty:
                info_parts.extend([
                    "",
                    f"### ğŸ“ˆ å½“å‰é¢„è§ˆ",
                    f"**é¢„è§ˆè¡Œæ•°**: {len(preview_df)}",
                    f"**æ˜¾ç¤ºå­—æ®µ**: {len(preview_df.columns)}",
                    f"**å­—æ®µåˆ—è¡¨**: {', '.join(preview_df.columns.tolist())}"
                ])
            
            # æ‰€æœ‰å¯ç”¨å­—æ®µ
            all_fields = self.current_dataset_cache['fields']
            if len(all_fields) > len(preview_df.columns):
                hidden_fields = [f for f in all_fields if f not in preview_df.columns]
                info_parts.append(f"**éšè—å­—æ®µ**: {', '.join(hidden_fields[:5])}{'...' if len(hidden_fields) > 5 else ''}")
            
            return "\n".join(info_parts)
            
        except Exception as e:
            self.logger.error(f'ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}')
            return f"*æ•°æ®é›†ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {str(e)}*"
    
    def _update_preview_by_fields(self, selected_fields: List[str], preview_rows: int, 
                                enable_truncation: bool, max_text_length: int) -> Tuple[pd.DataFrame, str]:
        """æ ¹æ®å­—æ®µé€‰æ‹©æ›´æ–°é¢„è§ˆï¼ˆå®æ—¶å“åº”ï¼‰"""
        try:
            if not selected_fields:
                return pd.DataFrame(), "ğŸ’¡ è¯·é€‰æ‹©è¦æ˜¾ç¤ºçš„å­—æ®µ"
            
            # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®è¿›è¡Œå­—æ®µè¿‡æ»¤
            return self._filter_preview_by_fields(selected_fields, enable_truncation, max_text_length)
            
        except Exception as e:
            self.logger.error(f'æ›´æ–°å­—æ®µé¢„è§ˆå¤±è´¥: {e}')
            return pd.DataFrame(), f"âŒ æ›´æ–°å¤±è´¥: {str(e)}"
    
    def _preview_dataset_with_field_filter(self, dataset_path: str, preview_rows: int,
                                         enable_truncation: bool, max_text_length: int,
                                         selected_fields: List[str]) -> Tuple[pd.DataFrame, str]:
        """ä½¿ç”¨å­—æ®µè¿‡æ»¤å™¨é¢„è§ˆæ•°æ®é›†ï¼ˆæ‰‹åŠ¨é¢„è§ˆæŒ‰é’®ï¼‰"""
        try:
            if not dataset_path.strip():
                return pd.DataFrame(), "âŒ è¯·é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†"
            
            # å¦‚æœä¸æ˜¯å½“å‰ç¼“å­˜çš„æ•°æ®é›†ï¼Œé‡æ–°åŠ è½½
            if self.current_dataset_cache['path'] != dataset_path:
                _, _, preview_df, _, status_msg = self._load_dataset_with_fields(
                    dataset_path, preview_rows, enable_truncation, max_text_length
                )
                return preview_df, status_msg
            
            # ä½¿ç”¨é€‰ä¸­çš„å­—æ®µè¿‡æ»¤é¢„è§ˆ
            if selected_fields:
                return self._filter_preview_by_fields(selected_fields, enable_truncation, max_text_length)
            else:
                return pd.DataFrame(), "ğŸ’¡ è¯·é€‰æ‹©è¦æ˜¾ç¤ºçš„å­—æ®µ"
            
        except Exception as e:
            self.logger.error(f'å­—æ®µè¿‡æ»¤é¢„è§ˆå¤±è´¥: {e}')
            return pd.DataFrame(), f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    def _select_all_dataset_fields(self) -> gr.CheckboxGroup:
        """å…¨é€‰æ‰€æœ‰å­—æ®µ"""
        try:
            all_fields = self.current_dataset_cache['fields']
            field_choices = [(field, field) for field in all_fields]
            return gr.CheckboxGroup(
                label="å¯ç”¨å­—æ®µ",
                choices=field_choices,
                value=all_fields,
                interactive=True,
                info=f"å·²é€‰ä¸­æ‰€æœ‰ {len(all_fields)} ä¸ªå­—æ®µ"
            )
        except Exception as e:
            self.logger.error(f'å…¨é€‰å­—æ®µå¤±è´¥: {e}')
            return gr.CheckboxGroup(choices=[], value=[])
    
    def _clear_all_dataset_fields(self) -> gr.CheckboxGroup:
        """æ¸…é™¤æ‰€æœ‰å­—æ®µé€‰æ‹©"""
        try:
            all_fields = self.current_dataset_cache['fields']
            field_choices = [(field, field) for field in all_fields]
            return gr.CheckboxGroup(
                label="å¯ç”¨å­—æ®µ",
                choices=field_choices,
                value=[],
                interactive=True,
                info="å·²æ¸…é™¤æ‰€æœ‰å­—æ®µé€‰æ‹©"
            )
        except Exception as e:
            self.logger.error(f'æ¸…é™¤å­—æ®µé€‰æ‹©å¤±è´¥: {e}')
            return gr.CheckboxGroup(choices=[], value=[])
    
    def _select_common_dataset_fields(self) -> gr.CheckboxGroup:
        """é€‰æ‹©å¸¸ç”¨å­—æ®µ"""
        try:
            all_fields = self.current_dataset_cache['fields']
            common_fields = self._identify_common_fields(all_fields)
            field_choices = [(field, field) for field in all_fields]
            return gr.CheckboxGroup(
                label="å¯ç”¨å­—æ®µ",
                choices=field_choices,
                value=common_fields,
                interactive=True,
                info=f"å·²é€‰ä¸­ {len(common_fields)} ä¸ªå¸¸ç”¨å­—æ®µ: {', '.join(common_fields)}"
            )
        except Exception as e:
            self.logger.error(f'é€‰æ‹©å¸¸ç”¨å­—æ®µå¤±è´¥: {e}')
            return gr.CheckboxGroup(choices=[], value=[])
    
    def _filter_datasets_for_display(self, datasets: list) -> list:
        """
        è¿‡æ»¤æ•°æ®é›†åˆ—è¡¨ï¼ŒæŒ‰æä¾›å•†/æ•°æ®é›†åˆ†å±‚æ˜¾ç¤º
        
        æ˜¾ç¤ºç­–ç•¥ï¼š
        1. æŒ‰æ•°æ®æºæä¾›å•†åˆ†ç»„ï¼ˆhuggingface, modelscopeç­‰ï¼‰
        2. æ¯ä¸ªæä¾›å•†ä¸‹é¢æ˜¾ç¤ºå…·ä½“çš„æ•°æ®é›†
        3. è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„å®é™…ç£ç›˜å¤§å°
        4. è·¯å¾„æŒ‡å‘æ•°æ®é›†ç›®å½•è€Œä¸æ˜¯meta.jsonæ–‡ä»¶
        """
        try:
            # æŒ‰æä¾›å•†å’Œæ•°æ®é›†åˆ†ç»„
            provider_groups = {}
            
            for dataset in datasets:
                file_path = dataset.get('path', '')
                path_norm = (file_path or '').replace('\\', '/')
                
                # æå–æä¾›å•†å’Œæ•°æ®é›†åç§°
                provider = "å…¶ä»–"
                dataset_name = "æœªçŸ¥æ•°æ®é›†"
                dataset_dir = ""
                
                if '/raw/' in path_norm:
                    # è§£æè·¯å¾„ï¼šdata/raw/provider/dataset_name/...
                    parts = path_norm.split('/raw/')
                    if len(parts) > 1:
                        path_parts = parts[1].split('/')
                        if len(path_parts) >= 2:
                            provider = path_parts[0]  # huggingface, modelscopeç­‰
                            dataset_name = path_parts[1]  # å…·ä½“æ•°æ®é›†åç§°
                            # æ„å»ºæ•°æ®é›†ç›®å½•è·¯å¾„
                            dataset_dir = f"{parts[0]}/raw/{provider}/{dataset_name}"
                        elif len(path_parts) == 1:
                            provider = path_parts[0]
                            dataset_name = "æ ¹ç›®å½•æ–‡ä»¶"
                elif '/distilled/' in path_norm:
                    # è’¸é¦æ•°æ®ï¼šè®°å½•è’¸é¦ä¿¡æ¯
                    provider = "è’¸é¦æ•°æ®"  
                    distilled_info = self._extract_distilled_info(file_path)
                    dataset_name = distilled_info['name']
                    # æ„å»ºè’¸é¦æ•°æ®ç›®å½•è·¯å¾„
                    from pathlib import Path
                    dataset_dir = str(Path(file_path).parent)
                elif '/processed/' in path_norm:
                    # å¤„ç†æ•°æ®ï¼šè®°å½•å¤„ç†åŠ¨ä½œå’Œè·¯å¾„
                    provider = "å¤„ç†æ•°æ®"
                    # å°è¯•ä»è·¯å¾„ä¸­æå–å¤„ç†ä¿¡æ¯
                    processed_info = self._extract_processed_info(file_path)
                    dataset_name = processed_info['name']
                    # æ„å»ºå¤„ç†æ•°æ®ç›®å½•è·¯å¾„
                    from pathlib import Path
                    dataset_dir = str(Path(file_path).parent)
                
                # åˆ›å»ºåˆ†ç»„ç»“æ„
                if provider not in provider_groups:
                    provider_groups[provider] = {}
                
                if dataset_name not in provider_groups[provider]:
                    provider_groups[provider][dataset_name] = {
                        'files': [],
                        'dataset_dir': dataset_dir,
                        'total_size': 0,
                        'latest_time': '',
                        'file_count': 0
                    }
                
                group = provider_groups[provider][dataset_name]
                group['files'].append(dataset)
                group['file_count'] += 1
                
                # ç´¯è®¡å¤§å°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if 'size_mb' in dataset:
                    group['total_size'] += dataset.get('size_mb', 0)
                
                # æ›´æ–°æœ€æ–°æ—¶é—´
                file_time = dataset.get('create_time', '')
                if file_time > group['latest_time']:
                    group['latest_time'] = file_time
                
                # å¦‚æœæ²¡æœ‰è®¾ç½®æ•°æ®é›†ç›®å½•ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„çˆ¶ç›®å½•
                if not group['dataset_dir'] and file_path:
                    from pathlib import Path
                    parent_path = str(Path(file_path).parent)
                    parent_norm = parent_path.replace('\\', '/')
                    if '/raw/' in parent_norm:
                        group['dataset_dir'] = parent_path
            
            # æ„å»ºåˆ†å±‚æ˜¾ç¤ºåˆ—è¡¨
            filtered_list = []
            
            for provider, datasets in sorted(provider_groups.items()):
                # æä¾›å•†å±‚çº§æ˜¾ç¤ºè§„åˆ™ï¼š
                # - åŸå§‹æ•°æ®æä¾›å•†ï¼ˆå¦‚ huggingface/modelscopeï¼‰ï¼šå¤šäº1ä¸ªæ•°æ®é›†æˆ–æœ¬å°±åº”åˆ†ç»„
                # - å¤„ç†æ•°æ®/è’¸é¦æ•°æ®ï¼šä¹Ÿæ˜¾ç¤ºé¡¶å±‚åˆ†ç»„ï¼Œä¾¿äºåˆ†ç±»æµè§ˆ
                should_show_provider = (
                    len(datasets) > 1 or provider not in ["å…¶ä»–"] or provider in ["å¤„ç†æ•°æ®", "è’¸é¦æ•°æ®"]
                )
                
                if should_show_provider:
                    provider_total_size = sum(group['total_size'] for group in datasets.values())
                    provider_file_count = sum(group['file_count'] for group in datasets.values())

                    # è®¡ç®—æä¾›å•†åŸºç¡€ç›®å½•
                    if provider in ["å¤„ç†æ•°æ®"]:
                        provider_dir = str(self.root_dir / 'processed')
                    elif provider in ["è’¸é¦æ•°æ®"]:
                        provider_dir = str(self.root_dir / 'distilled')
                    else:
                        # åŸå§‹æ•°æ®çš„å…·ä½“æä¾›å•†ï¼ˆhuggingface/modelscopeç­‰ï¼‰
                        provider_dir = str(self.root_dir / 'raw' / provider)

                    # è®¡ç®—å®é™…å¤§å°
                    provider_actual_size = self._calculate_directory_size(provider_dir)

                    filtered_list.append({
                        'display_name': f"ğŸ“ {provider}",
                        'type': 'æä¾›å•†',
                        'size_mb': provider_actual_size,
                        'create_time': max(group['latest_time'] for group in datasets.values()),
                        'path': provider_dir,
                        'file_count': provider_file_count,
                        'is_provider': True,
                        'provider': provider
                    })
                
                # æ·»åŠ æ•°æ®é›†å±‚çº§
                for dataset_name, group in sorted(datasets.items()):
                    # è®¡ç®—æ•°æ®é›†ç›®å½•çš„å®é™…å¤§å°
                    actual_size = self._calculate_directory_size(group['dataset_dir']) if group['dataset_dir'] else group['total_size']
                    
                    # å¦‚æœæ˜¾ç¤ºäº†æä¾›å•†å±‚çº§ï¼Œæ•°æ®é›†åç§°åŠ ç¼©è¿›
                    display_name = f"  ğŸ“Š {dataset_name}" if should_show_provider else f"ğŸ“Š {dataset_name}"
                    
                    filtered_list.append({
                        'display_name': display_name,
                        'type': 'æ•°æ®é›†',
                        'size_mb': actual_size,
                        'create_time': group['latest_time'],
                        'path': group['dataset_dir'] or (group['files'][0]['path'] if group['files'] else ''),
                        'file_count': group['file_count'],
                        'is_dataset': True,
                        'provider': provider
                    })
            
            return filtered_list
            
        except Exception as e:
            self.logger.error(f'è¿‡æ»¤æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}')
            return datasets  # è¿”å›åŸå§‹åˆ—è¡¨ä½œä¸ºå¤‡ç”¨
    
    def _extract_dataset_name_from_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ•°æ®é›†åç§°"""
        try:
            from pathlib import Path
            path = Path(file_path)
            
            # å»é™¤æ–‡ä»¶åï¼Œè·å–ç›®å½•å
            if path.is_file() or '.' in path.name:
                path = path.parent
            
            # è·å–æœ€æ¥è¿‘çš„æœ‰æ„ä¹‰çš„ç›®å½•å
            parts = path.parts
            for i in reversed(range(len(parts))):
                part = parts[i]
                if part not in ['processed', 'distilled', 'raw', 'data', 'cache', 'organized_files']:
                    return part
            
            return "æœªçŸ¥æ•°æ®é›†"
        except:
            return "æœªçŸ¥æ•°æ®é›†"
    
    def _extract_processed_info(self, file_path: str) -> dict:
        """ä»å¤„ç†æ•°æ®è·¯å¾„ä¸­æå–å¤„ç†ä¿¡æ¯"""
        try:
            from pathlib import Path
            path = Path(file_path)
            
            # è·å–å¤„ç†æ•°æ®çš„ç›®å½•ç»“æ„ä¿¡æ¯
            parts = path.parts
            processed_idx = -1
            for i, part in enumerate(parts):
                if part == 'processed':
                    processed_idx = i
                    break
            
            if processed_idx >= 0 and processed_idx + 1 < len(parts):
                # å¤„ç†ç±»å‹ç›®å½• (å¦‚ cleaned, merged, convertedç­‰)
                process_type = parts[processed_idx + 1]
                
                # æ•°æ®é›†åç§°
                if processed_idx + 2 < len(parts):
                    dataset_name = parts[processed_idx + 2]
                else:
                    dataset_name = process_type
                
                # ç»„åˆæ˜¾ç¤ºåç§°ï¼ŒåŒ…å«å¤„ç†åŠ¨ä½œ
                display_name = f"{dataset_name} ({process_type})"
                
                return {
                    'name': display_name,
                    'process_type': process_type,
                    'dataset_name': dataset_name,
                    'action': self._get_process_action_name(process_type)
                }
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'name': path.stem,
                'process_type': 'æœªçŸ¥å¤„ç†',
                'dataset_name': path.stem,
                'action': 'æ•°æ®å¤„ç†'
            }
            
        except Exception as e:
            return {
                'name': 'æœªçŸ¥å¤„ç†æ•°æ®',
                'process_type': 'æœªçŸ¥',
                'dataset_name': 'æœªçŸ¥',
                'action': 'å¤„ç†'
            }
    
    def _extract_distilled_info(self, file_path: str) -> dict:
        """ä»è’¸é¦æ•°æ®è·¯å¾„ä¸­æå–è’¸é¦ä¿¡æ¯"""
        try:
            from pathlib import Path
            path = Path(file_path)
            
            # è·å–è’¸é¦æ•°æ®çš„ç›®å½•ç»“æ„ä¿¡æ¯
            parts = path.parts
            distilled_idx = -1
            for i, part in enumerate(parts):
                if part == 'distilled':
                    distilled_idx = i
                    break
            
            if distilled_idx >= 0 and distilled_idx + 1 < len(parts):
                # è’¸é¦ä»»åŠ¡ç›®å½•
                task_name = parts[distilled_idx + 1]
                
                # æ•°æ®é›†åç§°
                if distilled_idx + 2 < len(parts):
                    dataset_name = parts[distilled_idx + 2]
                else:
                    dataset_name = task_name
                
                # ç»„åˆæ˜¾ç¤ºåç§°ï¼ŒåŒ…å«è’¸é¦ä¿¡æ¯
                display_name = f"{dataset_name} (è’¸é¦-{task_name})"
                
                return {
                    'name': display_name,
                    'task_name': task_name,
                    'dataset_name': dataset_name,
                    'action': 'æ•°æ®è’¸é¦'
                }
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'name': path.stem,
                'task_name': 'æœªçŸ¥ä»»åŠ¡',
                'dataset_name': path.stem,
                'action': 'æ•°æ®è’¸é¦'
            }
            
        except Exception as e:
            return {
                'name': 'æœªçŸ¥è’¸é¦æ•°æ®',
                'task_name': 'æœªçŸ¥',
                'dataset_name': 'æœªçŸ¥',
                'action': 'è’¸é¦'
            }
    
    def _get_process_action_name(self, process_type: str) -> str:
        """æ ¹æ®å¤„ç†ç±»å‹è·å–å‹å¥½çš„åŠ¨ä½œåç§°"""
        action_map = {
            'cleaned': 'æ•°æ®æ¸…æ´—',
            'merged': 'æ•°æ®åˆå¹¶',
            'converted': 'æ ¼å¼è½¬æ¢',
            'extracted': 'å­—æ®µæå–',
            'filtered': 'æ•°æ®è¿‡æ»¤',
            'augmented': 'æ•°æ®å¢å¼º',
            'normalized': 'æ•°æ®æ ‡å‡†åŒ–',
            'split': 'æ•°æ®åˆ†å‰²'
        }
        return action_map.get(process_type.lower(), f'{process_type}å¤„ç†')
    
    def _calculate_directory_size(self, dir_path: str) -> float:
        """è®¡ç®—ç›®å½•çš„å®é™…ç£ç›˜å¤§å°ï¼ˆMBï¼‰"""
        try:
            from pathlib import Path
            import os
            
            if not dir_path or not os.path.exists(dir_path):
                return 0.0
            
            total_size = 0
            for root, dirs, files in os.walk(dir_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        total_size += os.path.getsize(file_path)
                    except (OSError, IOError):
                        continue  # è·³è¿‡æ— æ³•è®¿é—®çš„æ–‡ä»¶
            
            # è½¬æ¢ä¸ºMB
            return total_size / (1024 * 1024)
        
        except Exception as e:
            self.logger.warning(f'è®¡ç®—ç›®å½•å¤§å°å¤±è´¥ {dir_path}: {e}')
            return 0.0
    
    def _setup_auto_refresh(self, demo):
        """è®¾ç½®è‡ªåŠ¨åˆ·æ–°"""
        def auto_refresh():
            """è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡çŠ¶æ€"""
            while True:
                try:
                    time.sleep(self.update_interval)
                    
                    # åªåœ¨æœ‰ç»„ä»¶å®ä¾‹æ—¶æ›´æ–°
                    if hasattr(self, 'components') and self.components:
                        # è¿™é‡Œå¯ä»¥å®ç°å®šæ—¶æ›´æ–°é€»è¾‘
                        # ç”±äºGradioçš„é™åˆ¶ï¼Œå®é™…çš„è‡ªåŠ¨åˆ·æ–°éœ€è¦åœ¨å‰ç«¯å®ç°
                        pass
                    
                except Exception as e:
                    self.logger.error(f'è‡ªåŠ¨åˆ·æ–°å¤±è´¥: {e}')
                    time.sleep(10)  # å‘ç”Ÿé”™è¯¯æ—¶å»¶é•¿ç­‰å¾…æ—¶é—´
        
        # å¯åŠ¨åå°åˆ·æ–°çº¿ç¨‹
        refresh_thread = threading.Thread(target=auto_refresh, daemon=True)
        refresh_thread.start()


# å…¨å±€UIå¯åŠ¨å™¨å®ä¾‹
ui_launcher = UILauncher()


if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œå…¥å£ï¼Œç”¨äºå¯åŠ¨UIç•Œé¢
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨æ•°æ®è’¸é¦è½¯ä»¶UIå¯åŠ¨å™¨')
    parser.add_argument('--share', action='store_true', help='åˆ›å»ºå…¬å…±é“¾æ¥')
    parser.add_argument('--port', type=int, default=7860, help='æœåŠ¡å™¨ç«¯å£')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨è‡ªåŠ¨æ•°æ®è’¸é¦è½¯ä»¶...")
    print(f"ğŸŒ è®¿é—®åœ°å€: http://localhost:{args.port}")
    
    ui_launcher.launch(share=args.share, server_port=args.port)
