#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆå¹¶æ¨¡å— - data_merger.py

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŒç»“æ„æ•°æ®é›†çš„çºµå‘åˆå¹¶ï¼ˆæ”¯æŒå¤šæ–‡ä»¶åˆå¹¶ä¸ºå•æ–‡ä»¶ï¼‰
2. ä¸¤ç§åˆå¹¶æ¨¡å¼ï¼šæ–°å»ºåˆå¹¶å’Œè¿½åŠ åˆå¹¶
3. åŸºäºæŒ‡å®šå­—æ®µæˆ–å…¨é‡å­—æ®µçš„ç²¾å‡†å»é‡
4. å¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º

è®¾è®¡æ€æƒ³ï¼š
- ä¸“æ³¨äºåŒç»“æ„æ•°æ®é›†åˆå¹¶ï¼Œé¿å…å¤æ‚çš„æ¨ªå‘æ‹¼æ¥
- ä¸¥æ ¼çš„æ ¼å¼ä¸å­—æ®µä¸€è‡´æ€§æ ¡éªŒ
- æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œè¿›åº¦è¿½è¸ª
- å®Œæ•´çš„åˆå¹¶å…ƒæ•°æ®è®°å½•å’Œå¯è¿½æº¯æ€§

ä½œè€…: GitHub Copilot
åˆ›å»ºæ—¶é—´: 2025-08-24
"""

import os
import sys
import json
import pandas as pd
import jsonlines
import logging
import argparse
import time
import uuid
from typing import Dict, List, Union, Optional, TypedDict, Literal, Any, Tuple
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

try:
    from .config_manager import ConfigManager
    from .log_manager import LogManager 
    from .state_manager import StateManager
    from .utils import validate_file, get_file_encoding, ensure_dir
    from .field_extractor import FieldExtractor
except ImportError:
    try:
        # å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆç”¨äºå‘½ä»¤è¡Œæ‰§è¡Œï¼‰
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        
        from config_manager import ConfigManager
        from log_manager import LogManager 
        from state_manager import StateManager
        from utils import validate_file, get_file_encoding, ensure_dir
        from field_extractor import FieldExtractor
    except ImportError as e:
        print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ä¾èµ–æ¨¡å— {e}ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
        ConfigManager = None
        LogManager = None
        StateManager = None
        FieldExtractor = None
        
        # ç®€åŒ–ç‰ˆæœ¬çš„å·¥å…·å‡½æ•°
        def validate_file(file_path: str, formats=None) -> bool:
            return os.path.exists(file_path) and os.path.isfile(file_path)
        
        def get_file_encoding(file_path: str) -> str:
            return 'utf-8'
        
        def ensure_dir(dir_path: str) -> bool:
            try:
                os.makedirs(dir_path, exist_ok=True)
                return True
            except Exception:
                return False

# ç±»å‹å®šä¹‰
class MergeTaskParams(TypedDict):
    """æ•°æ®åˆå¹¶ä»»åŠ¡å‚æ•°ç»“æ„"""
    task_id: str                                    # ä»»åŠ¡å”¯ä¸€æ ‡è¯†
    input_paths: List[str]                          # å¾…åˆå¹¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    merge_mode: Literal["merge", "append"]          # åˆå¹¶æ¨¡å¼ï¼šæ–°å»º/è¿½åŠ 
    target_path: str                                # ç›®æ ‡æ–‡ä»¶è·¯å¾„
    
    # å»é‡é…ç½®
    deduplicate: bool                               # æ˜¯å¦å»é‡
    dedup_field: Optional[str]                      # å»é‡å­—æ®µ(Noneåˆ™å…¨é‡å»é‡)
    dedup_strategy: Literal["keep_first", "keep_last"]  # å»é‡ç­–ç•¥
    
    # ç³»ç»Ÿå‚æ•°
    chunk_size: Optional[int]                       # åˆ†ç‰‡å¤§å°
    encoding: Optional[str]                         # æ–‡ä»¶ç¼–ç 

class MergeMeta(TypedDict):
    """åˆå¹¶å…ƒæ•°æ®ç»“æ„"""
    task_id: str                                    # ä»»åŠ¡ID
    merge_mode: str                                 # åˆå¹¶æ¨¡å¼
    input_files: List[str]                          # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_row_counts: List[int]                     # å„è¾“å…¥æ–‡ä»¶è®°å½•æ•°
    target_path: str                                # ç›®æ ‡æ–‡ä»¶è·¯å¾„
    total_input_rows: int                           # è¾“å…¥æ€»è®°å½•æ•°
    total_output_rows: int                          # åˆå¹¶åè®°å½•æ•°
    duplicate_rows: int                             # å»é‡æ•°é‡
    start_time: str                                 # å¼€å§‹æ—¶é—´
    end_time: str                                   # ç»“æŸæ—¶é—´
    fields: List[str]                               # æ•°æ®é›†å­—æ®µåˆ—è¡¨

class DataMerger:
    """æ•°æ®åˆå¹¶å™¨æ ¸å¿ƒç±»
    
    æä¾›åŒç»“æ„æ•°æ®é›†çš„åˆå¹¶ä¸å»é‡åŠŸèƒ½ã€‚
    æ”¯æŒå¤šç§æ ¼å¼å’Œå¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åˆå¹¶å™¨
        
        è®¾ç½®é»˜è®¤é…ç½®ã€åˆå§‹åŒ–æ—¥å¿—å™¨å’ŒçŠ¶æ€ç®¡ç†å™¨
        åŠ è½½ç³»ç»Ÿé…ç½®å¹¶å‡†å¤‡å·¥ä½œç¯å¢ƒ
        """
        self.config_manager = None
        self.log_manager = None
        self.state_manager = None
        self.field_extractor = None
        self.logger = None
        
        # é»˜è®¤é…ç½®
        self.default_config = {
            'chunk_size': 1000,
            'default_encoding': 'utf-8',
            'supported_formats': ['csv', 'xlsx', 'xls', 'json', 'jsonl'],
            'max_parallel_tasks': 3,
            'temp_dir': './data/temp'
        }
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        self.supported_formats = ['csv', 'xlsx', 'xls', 'json', 'jsonl']
        
    def init_merger(self) -> bool:
        """åˆå§‹åŒ–åˆå¹¶å™¨ç¯å¢ƒ
        
        åŠ è½½é…ç½®ã€æ—¥å¿—ä¸çŠ¶æ€ç®¡ç†å™¨ï¼Œæ£€æŸ¥ä¾èµ–åº“
        
        Returns:
            bool: Trueè¡¨ç¤ºåˆå§‹åŒ–æˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
            
        Raises:
            ImportError: å½“å¿…éœ€çš„ä¾èµ–åº“ç¼ºå¤±æ—¶
            Exception: å½“é…ç½®æ–‡ä»¶æŸåæˆ–å…¶ä»–åˆå§‹åŒ–é”™è¯¯æ—¶
        """
        try:
            # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
            if ConfigManager:
                self.config_manager = ConfigManager()
                config = self.config_manager.get_config('data_merger', self.default_config)
                self.chunk_size = config.get('chunk_size', 1000)
                self.default_encoding = config.get('default_encoding', 'utf-8')
                self.temp_dir = config.get('temp_dir', './data/temp')
            else:
                self.chunk_size = 1000
                self.default_encoding = 'utf-8'
                self.temp_dir = './data/temp'
            
            # åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨
            if LogManager:
                self.log_manager = LogManager()
                self.logger = self.log_manager.get_logger('data_merger')
                self.logger.info("æ•°æ®åˆå¹¶å™¨åˆå§‹åŒ–å¼€å§‹")
            else:
                # åˆ›å»ºç®€å•çš„æ—¥å¿—å™¨
                self.logger = logging.getLogger('data_merger')
                self.logger.setLevel(logging.INFO)
                if not self.logger.handlers:
                    handler = logging.StreamHandler()
                    formatter = logging.Formatter(
                        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                    )
                    handler.setFormatter(formatter)
                    self.logger.addHandler(handler)
            
            # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
            if StateManager:
                self.state_manager = StateManager()
                self.state_manager.init_state('data_merger')
            
            # åˆå§‹åŒ–å­—æ®µæå–å™¨(ç”¨äºå­—æ®µä¸€è‡´æ€§æ ¡éªŒ)
            if FieldExtractor:
                self.field_extractor = FieldExtractor()
                self.field_extractor.init_extractor()
            
            # æ£€æŸ¥å¿…éœ€çš„ä¾èµ–åº“
            missing_deps = []
            try:
                import pandas
            except ImportError:
                missing_deps.append('pandas')
            
            try:
                import jsonlines
            except ImportError:
                missing_deps.append('jsonlines')
            
            if missing_deps:
                error_msg = f"ç¼ºå°‘å¿…éœ€çš„ä¾èµ–åº“: {', '.join(missing_deps)}"
                if self.logger:
                    self.logger.error(error_msg)
                return False
            
            # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
            ensure_dir(self.temp_dir)
            
            if self.logger:
                self.logger.info("æ•°æ®åˆå¹¶å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"æ•°æ®åˆå¹¶å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def validate_merge(self, params: MergeTaskParams) -> Dict[str, Any]:
        """æ ¡éªŒåˆå¹¶å¯è¡Œæ€§
        
        ä¸¥æ ¼æ ¡éªŒæ‰€æœ‰è¾“å…¥æ–‡ä»¶çš„æ ¼å¼ä¸å­—æ®µä¸€è‡´æ€§
        
        Args:
            params (MergeTaskParams): åˆå¹¶ä»»åŠ¡å‚æ•°
            
        Returns:
            Dict[str, Any]: æ ¡éªŒç»“æœå­—å…¸ï¼ŒåŒ…å«validã€reasonã€detailsç­‰ä¿¡æ¯
            
        Raises:
            Exception: å½“æ ¡éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶
        """
        try:
            if self.logger:
                self.logger.info(f"å¼€å§‹æ ¡éªŒåˆå¹¶ä»»åŠ¡: {params['task_id']}")
            
            # åŸºç¡€å‚æ•°æ ¡éªŒ
            if not params.get('input_paths') or len(params['input_paths']) < 1:
                return {
                    'valid': False,
                    'reason': 'è¾“å…¥æ–‡ä»¶åˆ—è¡¨ä¸ºç©ºï¼Œè‡³å°‘éœ€è¦1ä¸ªæ–‡ä»¶',
                    'details': {}
                }
            
            if not params.get('target_path'):
                return {
                    'valid': False,
                    'reason': 'ç›®æ ‡æ–‡ä»¶è·¯å¾„ä¸ºç©º',
                    'details': {}
                }
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å­˜åœ¨æ€§å’Œæ ¼å¼ä¸€è‡´æ€§
            input_paths = params['input_paths']
            base_format = None
            base_fields = None
            
            for i, file_path in enumerate(input_paths):
                # æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
                if not os.path.exists(file_path):
                    return {
                        'valid': False,
                        'reason': f'è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}',
                        'details': {}
                    }
                
                # è·å–æ–‡ä»¶æ ¼å¼
                file_format = self._detect_format(file_path)
                if not file_format:
                    return {
                        'valid': False,
                        'reason': f'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}',
                        'details': {}
                    }
                
                # æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥
                if base_format is None:
                    base_format = file_format
                elif base_format != file_format:
                    return {
                        'valid': False,
                        'reason': f'æ–‡ä»¶æ ¼å¼ä¸ä¸€è‡´: åŸºå‡†æ ¼å¼ä¸º{base_format}ï¼Œæ–‡ä»¶{file_path}ä¸º{file_format}',
                        'details': {'base_format': base_format}
                    }
                
                # å­—æ®µä¸€è‡´æ€§æ£€æŸ¥
                if self.field_extractor:
                    fields = self.field_extractor.get_fields(file_path)
                    if not fields:
                        return {
                            'valid': False,
                            'reason': f'æ— æ³•è·å–æ–‡ä»¶å­—æ®µä¿¡æ¯: {file_path}',
                            'details': {}
                        }
                    
                    field_names = [f['name'] for f in fields]
                else:
                    # ç®€åŒ–ç‰ˆå­—æ®µæå–
                    field_names = self._get_fields_simple(file_path, file_format)
                    if not field_names:
                        return {
                            'valid': False,
                            'reason': f'æ— æ³•è·å–æ–‡ä»¶å­—æ®µä¿¡æ¯: {file_path}',
                            'details': {}
                        }
                
                # å­—æ®µä¸€è‡´æ€§æ£€æŸ¥
                if base_fields is None:
                    base_fields = field_names
                elif set(base_fields) != set(field_names):
                    # è¯¦ç»†çš„å­—æ®µå·®å¼‚åˆ†æ
                    base_set = set(base_fields)
                    current_set = set(field_names)
                    only_in_base = base_set - current_set
                    only_in_current = current_set - base_set
                    
                    error_details = {
                        'base_file': input_paths[0],
                        'current_file': file_path,
                        'base_fields': base_fields,
                        'current_fields': field_names,
                        'only_in_base': list(only_in_base),
                        'only_in_current': list(only_in_current)
                    }
                    
                    error_msg = f"æ–‡ä»¶å­—æ®µä¸ä¸€è‡´!\n\n"
                    error_msg += f"åŸºå‡†æ–‡ä»¶: {os.path.basename(input_paths[0])}\n"
                    error_msg += f"å­—æ®µ: {base_fields}\n\n"
                    error_msg += f"å½“å‰æ–‡ä»¶: {os.path.basename(file_path)}\n"
                    error_msg += f"å­—æ®µ: {field_names}\n\n"
                    
                    if only_in_base:
                        error_msg += f"åŸºå‡†æ–‡ä»¶ç‹¬æœ‰å­—æ®µ: {list(only_in_base)}\n"
                    if only_in_current:
                        error_msg += f"å½“å‰æ–‡ä»¶ç‹¬æœ‰å­—æ®µ: {list(only_in_current)}\n"
                    
                    error_msg += "\nğŸ’¡ æç¤º: åªæœ‰å­—æ®µå®Œå…¨ä¸€è‡´çš„æ–‡ä»¶æ‰èƒ½åˆå¹¶!"
                    
                    return {
                        'valid': False,
                        'reason': error_msg,
                        'details': error_details
                    }
            
            # appendæ¨¡å¼ç‰¹æ®Šæ ¡éªŒ
            if params['merge_mode'] == 'append':
                target_path = params['target_path']
                if not os.path.exists(target_path):
                    return {
                        'valid': False,
                        'reason': f'è¿½åŠ æ¨¡å¼ä¸‹ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {target_path}',
                        'details': {}
                    }
                
                # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ ¼å¼å’Œå­—æ®µ
                target_format = self._detect_format(target_path)
                if target_format != base_format:
                    return {
                        'valid': False,
                        'reason': f'ç›®æ ‡æ–‡ä»¶æ ¼å¼({target_format})ä¸è¾“å…¥æ–‡ä»¶æ ¼å¼({base_format})ä¸ä¸€è‡´',
                        'details': {}
                    }
                
                if self.field_extractor:
                    target_fields_info = self.field_extractor.get_fields(target_path)
                    target_fields = [f['name'] for f in target_fields_info] if target_fields_info else []
                else:
                    target_fields = self._get_fields_simple(target_path, target_format)
                
                if target_fields != base_fields:
                    return {
                        'valid': False,
                        'reason': f'ç›®æ ‡æ–‡ä»¶å­—æ®µ({target_fields})ä¸è¾“å…¥æ–‡ä»¶å­—æ®µ({base_fields})ä¸ä¸€è‡´',
                        'details': {}
                    }
            
            # å»é‡é…ç½®æ ¡éªŒ
            if params.get('deduplicate', False):
                dedup_field = params.get('dedup_field')
                if dedup_field and dedup_field not in base_fields:
                    return {
                        'valid': False,
                        'reason': f'å»é‡å­—æ®µ"{dedup_field}"ä¸åœ¨æ•°æ®é›†å­—æ®µ{base_fields}ä¸­',
                        'details': {}
                    }
            
            if self.logger:
                self.logger.info(f"åˆå¹¶ä»»åŠ¡æ ¡éªŒé€šè¿‡: {len(input_paths)}ä¸ªæ–‡ä»¶ï¼Œæ ¼å¼{base_format}ï¼Œå­—æ®µ{base_fields}")
            
            return {
                'valid': True,
                'reason': 'æ ¡éªŒé€šè¿‡',
                'details': {
                    'base_fields': base_fields,
                    'format': base_format,
                    'target_exists': os.path.exists(params['target_path']) if params['merge_mode'] == 'append' else False
                }
            }
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"åˆå¹¶æ ¡éªŒå¤±è´¥: {str(e)}")
            return {
                'valid': False,
                'reason': f'æ ¡éªŒè¿‡ç¨‹å¼‚å¸¸: {str(e)}',
                'details': {}
            }
    
    def merge_datasets(self, params: MergeTaskParams) -> Optional[str]:
        """æ‰§è¡Œæ•°æ®é›†åˆå¹¶æ“ä½œ
        
        æŒ‰ç…§æŒ‡å®šå‚æ•°æ‰§è¡Œåˆå¹¶ä¸å»é‡ï¼Œæ”¯æŒå¤§æ–‡ä»¶åˆ†ç‰‡å¤„ç†
        
        Args:
            params (MergeTaskParams): åˆå¹¶ä»»åŠ¡å‚æ•°
            
        Returns:
            Optional[str]: æˆåŠŸæ—¶è¿”å›ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
            
        Raises:
            Exception: å½“åˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶
        """
        try:
            task_id = params['task_id']
            if self.logger:
                self.logger.info(f"å¼€å§‹æ•°æ®åˆå¹¶ä»»åŠ¡: {task_id}")
            
            # å‰ç½®æ ¡éªŒ
            validation = self.validate_merge(params)
            if not validation['valid']:
                error_msg = validation['reason']
                if self.logger:
                    self.logger.error(f"åˆå¹¶å‰æ ¡éªŒå¤±è´¥: {error_msg}")
                raise ValueError(f"åˆå¹¶æ ¡éªŒå¤±è´¥: {error_msg}")
            
            # è·å–åŸºç¡€ä¿¡æ¯
            base_fields, file_format, total_rows = self._get_base_info(params['input_paths'])
            
            # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
            if self.state_manager:
                self.state_manager.set_state(f"task.{task_id}", {
                    'status': 'running',
                    'progress': 0,
                    'total_rows': total_rows,
                    'processed_rows': 0,
                    'start_time': datetime.now().isoformat()
                })
            
            # æ‰§è¡Œåˆå¹¶
            target_path = params['target_path']
            merge_mode = params['merge_mode']
            chunk_size = params.get('chunk_size', self.chunk_size)
            encoding = params.get('encoding', self.default_encoding)
            
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            ensure_dir(os.path.dirname(target_path))
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_input_rows = 0
            total_output_rows = 0
            duplicate_rows = 0
            input_row_counts = []
            processed_rows = 0
            
            # ç”¨äºå»é‡çš„é›†åˆ
            seen_values = set() if params.get('deduplicate', False) else None
            
            # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
            if merge_mode == 'merge':
                # æ–°å»ºæ¨¡å¼ï¼šåˆ›å»ºæ–°æ–‡ä»¶
                self._init_output_file(target_path, file_format, base_fields, encoding)
            # appendæ¨¡å¼ï¼šç›´æ¥è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
            
            # åˆ†ç‰‡å¤„ç†æ¯ä¸ªè¾“å…¥æ–‡ä»¶
            for file_idx, input_path in enumerate(params['input_paths']):
                if self.logger:
                    self.logger.info(f"å¤„ç†è¾“å…¥æ–‡ä»¶ {file_idx + 1}/{len(params['input_paths'])}: {input_path}")
                
                file_row_count = 0
                
                # åˆ†ç‰‡è¯»å–å½“å‰æ–‡ä»¶ï¼Œä½¿ç”¨åŠ¨æ€ç¼“å†²åŒºå¤§å°
                write_buffer = []
                # åŠ¨æ€è®¡ç®—ç¼“å†²åŒºå¤§å°ï¼Œç¡®ä¿ä¸è¶…è¿‡å†…å­˜é™åˆ¶
                max_buffer_rows = min(chunk_size * 5, 50000)  # æœ€å¤šç¼“å­˜5ä¸‡è¡Œæˆ–chunk_size*5
                
                for chunk_data in self._read_file_chunks(input_path, file_format, chunk_size, encoding):
                    if not chunk_data:
                        continue
                    
                    file_row_count += len(chunk_data)
                    total_input_rows += len(chunk_data)
                    
                    # æ‰§è¡Œå»é‡
                    if params.get('deduplicate', False):
                        filtered_data, dup_count = self._deduplicate_data(
                            chunk_data, 
                            params.get('dedup_field'),
                            params.get('dedup_strategy', 'keep_first'),
                            seen_values
                        )
                        duplicate_rows += dup_count
                    else:
                        filtered_data = chunk_data
                    
                    # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                    if filtered_data:
                        write_buffer.extend(filtered_data)
                        total_output_rows += len(filtered_data)
                    
                    # å½“ç¼“å†²åŒºè¾¾åˆ°æŒ‡å®šå¤§å°æ—¶æ‰¹é‡å†™å…¥
                    if len(write_buffer) >= max_buffer_rows:
                        self._append_to_file(target_path, file_format, write_buffer, encoding)
                        write_buffer = []
                        
                        # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
                        import gc
                        gc.collect()
                    
                    processed_rows += len(chunk_data)
                    
                    # æ›´æ–°è¿›åº¦
                    if self.state_manager:
                        progress = int(processed_rows / total_rows * 100) if total_rows > 0 else 100
                        self.state_manager.set_state(f"task.{task_id}", {
                            'progress': progress,
                            'processed_rows': processed_rows
                        })
                
                # å†™å…¥å‰©ä½™çš„ç¼“å†²åŒºæ•°æ®
                if write_buffer:
                    self._append_to_file(target_path, file_format, write_buffer, encoding)
                    write_buffer = []
                
                input_row_counts.append(file_row_count)
                
                if self.logger:
                    self.logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {input_path}ï¼Œè®°å½•æ•°: {file_row_count}")
            
            # ç”Ÿæˆåˆå¹¶å…ƒæ•°æ®
            merge_meta = MergeMeta(
                task_id=task_id,
                merge_mode=merge_mode,
                input_files=params['input_paths'],
                input_row_counts=input_row_counts,
                target_path=target_path,
                total_input_rows=total_input_rows,
                total_output_rows=total_output_rows,
                duplicate_rows=duplicate_rows,
                start_time=self.state_manager.get_state(f"task.{task_id}.start_time") if self.state_manager else "",
                end_time=datetime.now().isoformat(),
                fields=base_fields
            )
            
            # æ¸…ç†åˆå¹¶æ–‡ä»¶æœ«å°¾çš„å¤šä½™ç©ºè¡Œ
            self._clean_file_ending(target_path, file_format, encoding)
            
            self._save_merge_meta(target_path, merge_meta)
            
            # åˆ›å»ºåˆå¹¶ä¿¡æ¯æ–‡ä»¶
            output_dir = os.path.dirname(target_path)
            self._create_merge_info_file(output_dir, merge_meta, params)
            
            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            if self.state_manager:
                self.state_manager.set_state(f"task.{task_id}", {
                    'status': 'completed',
                    'progress': 100,
                    'output_path': target_path,
                    'end_time': datetime.now().isoformat()
                })
            
            if self.logger:
                self.logger.info(f"æ•°æ®åˆå¹¶ä»»åŠ¡å®Œæˆ: {target_path}")
                self.logger.info(f"è¾“å…¥è®°å½•æ•°: {total_input_rows}ï¼Œè¾“å‡ºè®°å½•æ•°: {total_output_rows}ï¼Œå»é‡æ•°é‡: {duplicate_rows}")
            
            return target_path
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"æ•°æ®åˆå¹¶ä»»åŠ¡å¤±è´¥: {str(e)}")
            
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            if self.state_manager:
                self.state_manager.set_state(f"task.{task_id}", {
                    'status': 'failed',
                    'error': str(e),
                    'end_time': datetime.now().isoformat()
                })
            
            # æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å› None
            raise Exception(f"æ•°æ®åˆå¹¶æ‰§è¡Œå¤±è´¥: {str(e)}")
    
    def _detect_format(self, file_path: str) -> Optional[str]:
        """æ£€æµ‹æ–‡ä»¶æ ¼å¼
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            
        Returns:
            Optional[str]: æ–‡ä»¶æ ¼å¼å­—ç¬¦ä¸²
        """
        file_ext = Path(file_path).suffix.lower()
        
        format_map = {
            '.csv': 'csv',
            '.xlsx': 'xlsx',
            '.xls': 'xls',
            '.json': 'json',
            '.jsonl': 'jsonl'
        }
        
        return format_map.get(file_ext)
    
    def _get_fields_simple(self, file_path: str, file_format: str) -> List[str]:
        """ç®€åŒ–ç‰ˆå­—æ®µæå–
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            file_format (str): æ–‡ä»¶æ ¼å¼
            
        Returns:
            List[str]: å­—æ®µåç§°åˆ—è¡¨
        """
        try:
            if file_format == 'csv':
                df = pd.read_csv(file_path, nrows=1)
                return list(df.columns)
            elif file_format in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, nrows=1)
                return list(df.columns)
            elif file_format == 'jsonl':
                with jsonlines.open(file_path, 'r') as reader:
                    for item in reader:
                        if isinstance(item, dict):
                            return list(item.keys())
                return []
            elif file_format == 'json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list) and data and isinstance(data[0], dict):
                        return list(data[0].keys())
                    elif isinstance(data, dict):
                        return list(data.keys())
                return []
            else:
                return []
        except Exception as e:
            if self.logger:
                self.logger.warning(f"ç®€åŒ–ç‰ˆå­—æ®µæå–å¤±è´¥: {str(e)}")
            return []
    
    def _get_base_info(self, input_paths: List[str]) -> Tuple[List[str], str, int]:
        """è·å–åŸºç¡€ä¿¡æ¯
        
        Args:
            input_paths (List[str]): è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            Tuple[List[str], str, int]: (åŸºå‡†å­—æ®µåˆ—è¡¨, æ ¼å¼, æ€»è¡Œæ•°)
        """
        # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ä¿¡æ¯ä½œä¸ºåŸºå‡†
        first_file = input_paths[0]
        file_format = self._detect_format(first_file)
        
        if self.field_extractor:
            fields_info = self.field_extractor.get_fields(first_file)
            base_fields = [f['name'] for f in fields_info] if fields_info else []
        else:
            base_fields = self._get_fields_simple(first_file, file_format)
        
        # è®¡ç®—æ€»è¡Œæ•°
        total_rows = 0
        for file_path in input_paths:
            total_rows += self._count_file_rows(file_path, file_format)
        
        return base_fields, file_format, total_rows
    
    def _count_file_rows(self, file_path: str, file_format: str) -> int:
        """è®¡ç®—æ–‡ä»¶è¡Œæ•°
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            file_format (str): æ–‡ä»¶æ ¼å¼
            
        Returns:
            int: æ–‡ä»¶è¡Œæ•°
        """
        try:
            if file_format == 'csv':
                with open(file_path, 'r', encoding=get_file_encoding(file_path)) as f:
                    return sum(1 for _ in f) - 1  # å‡å»è¡¨å¤´è¡Œ
            elif file_format == 'jsonl':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return sum(1 for _ in f)
            elif file_format in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, nrows=1)
                # è¿™é‡Œä½¿ç”¨ç®€å•ä¼°ç®—ï¼Œå®é™…å¯ä»¥é€šè¿‡sheetä¿¡æ¯è·å–
                return 1000  # ç®€åŒ–ä¼°ç®—
            elif file_format == 'json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return len(data)
                    else:
                        return 1
            else:
                return 0
        except Exception:
            return 0
    
    def _read_file_chunks(self, file_path: str, file_format: str, chunk_size: int, encoding: str):
        """åˆ†ç‰‡è¯»å–æ–‡ä»¶æ•°æ®
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            file_format (str): æ–‡ä»¶æ ¼å¼
            chunk_size (int): åˆ†ç‰‡å¤§å°
            encoding (str): æ–‡ä»¶ç¼–ç 
            
        Yields:
            List[Dict]: åˆ†ç‰‡æ•°æ®
        """
        try:
            if file_format == 'csv':
                # CSVåˆ†ç‰‡è¯»å–
                for chunk_df in pd.read_csv(file_path, chunksize=chunk_size, encoding=encoding):
                    yield chunk_df.to_dict('records')
            
            elif file_format in ['xlsx', 'xls']:
                # Excelæ–‡ä»¶è¯»å– - ä½¿ç”¨æ›´å®‰å…¨çš„å†…å­˜ç®¡ç†
                try:
                    # å°è¯•ä½¿ç”¨chunksizeå‚æ•°ï¼ˆpandas 1.2+æ”¯æŒï¼‰
                    for chunk_df in pd.read_excel(file_path, chunksize=chunk_size):
                        yield chunk_df.to_dict('records')
                except TypeError:
                    # å¦‚æœä¸æ”¯æŒchunksizeï¼Œåˆ™åˆ†æ‰¹å¤„ç†
                    df = pd.read_excel(file_path)
                    total_rows = len(df)
                    for i in range(0, total_rows, chunk_size):
                        chunk_df = df.iloc[i:i+chunk_size]
                        yield chunk_df.to_dict('records')
                        # æ˜¾å¼åˆ é™¤chunkä»¥é‡Šæ”¾å†…å­˜
                        del chunk_df
            
            elif file_format == 'jsonl':
                # JSONLåˆ†ç‰‡è¯»å–ï¼Œä½¿ç”¨åŸç”ŸJSONå¤„ç†é¿å…è¡Œç»ˆæ­¢ç¬¦é—®é¢˜
                chunk_data = []
                with open(file_path, 'r', encoding=encoding) as f:
                    for line in f:
                        line = line.strip()
                        if line:  # è·³è¿‡ç©ºè¡Œ
                            try:
                                item = json.loads(line)
                                chunk_data.append(item)
                                if len(chunk_data) >= chunk_size:
                                    yield chunk_data
                                    chunk_data = []
                            except json.JSONDecodeError as e:
                                # å¿½ç•¥æ— æ•ˆçš„JSONè¡Œï¼Œä½†è®°å½•è­¦å‘Š
                                if self.logger:
                                    self.logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ: {line[:100]}... é”™è¯¯: {str(e)}")
                                continue
                    
                    # å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„åˆ†ç‰‡
                    if chunk_data:
                        yield chunk_data
            
            elif file_format == 'json':
                # JSONæ–‡ä»¶è¯»å– - é¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
                with open(file_path, 'r', encoding=encoding) as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        total_items = len(data)
                        for i in range(0, total_items, chunk_size):
                            chunk = data[i:i+chunk_size]
                            yield chunk
                            # åœ¨å¤„ç†å¤§å‹JSONæ—¶å¸®åŠ©å†…å­˜å›æ”¶
                            if total_items > 100000:  # å¯¹äºå¤§äº10ä¸‡æ¡è®°å½•çš„æ–‡ä»¶
                                import gc
                                gc.collect()
                    else:
                        yield [data]
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"è¯»å–æ–‡ä»¶åˆ†ç‰‡å¤±è´¥: {file_path}, {str(e)}")
            yield []
    
    def _init_output_file(self, target_path: str, file_format: str, fields: List[str], encoding: str):
        """åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
        
        Args:
            target_path (str): ç›®æ ‡æ–‡ä»¶è·¯å¾„
            file_format (str): æ–‡ä»¶æ ¼å¼
            fields (List[str]): å­—æ®µåˆ—è¡¨
            encoding (str): æ–‡ä»¶ç¼–ç 
        """
        try:
            if file_format == 'csv':
                # åˆ›å»ºCSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
                df = pd.DataFrame(columns=fields)
                df.to_csv(target_path, index=False, encoding=encoding)
            
            elif file_format in ['xlsx', 'xls']:
                # åˆ›å»ºExcelæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
                df = pd.DataFrame(columns=fields)
                df.to_excel(target_path, index=False)
            
            elif file_format == 'jsonl':
                # åˆ›å»ºç©ºJSONLæ–‡ä»¶
                open(target_path, 'w', encoding=encoding).close()
            
            elif file_format == 'json':
                # åˆ›å»ºJSONæ–‡ä»¶å¹¶å†™å…¥ç©ºæ•°ç»„
                with open(target_path, 'w', encoding=encoding) as f:
                    json.dump([], f, ensure_ascii=False)
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _append_to_file(self, target_path: str, file_format: str, data: List[Dict], encoding: str):
        """è¿½åŠ æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            target_path (str): ç›®æ ‡æ–‡ä»¶è·¯å¾„
            file_format (str): æ–‡ä»¶æ ¼å¼
            data (List[Dict]): å¾…è¿½åŠ çš„æ•°æ®
            encoding (str): æ–‡ä»¶ç¼–ç 
        """
        try:
            if file_format == 'csv':
                # è¿½åŠ åˆ°CSVæ–‡ä»¶
                df = pd.DataFrame(data)
                df.to_csv(target_path, mode='a', header=False, index=False, encoding=encoding)
            
            elif file_format in ['xlsx', 'xls']:
                # Excelè¿½åŠ æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦é‡æ–°å†™å…¥
                try:
                    existing_df = pd.read_excel(target_path)
                    new_df = pd.DataFrame(data)
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    combined_df.to_excel(target_path, index=False)
                except Exception:
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œç›´æ¥å†™å…¥
                    df = pd.DataFrame(data)
                    df.to_excel(target_path, index=False)
            
            elif file_format == 'jsonl':
                # è¿½åŠ åˆ°JSONLæ–‡ä»¶ï¼Œä½¿ç”¨ç»Ÿä¸€çš„è¡Œç»ˆæ­¢ç¬¦
                with open(target_path, 'a', encoding=encoding, newline='\n') as f:
                    for item in data:
                        # ç¡®ä¿ä½¿ç”¨ç»Ÿä¸€çš„æ¢è¡Œç¬¦ï¼Œé¿å…å¼‚å¸¸ç»ˆæ­¢ç¬¦
                        json_line = json.dumps(item, ensure_ascii=False)
                        f.write(json_line + '\n')
            
            elif file_format == 'json':
                # JSONæ–‡ä»¶éœ€è¦é‡æ–°å†™å…¥æ•´ä¸ªæ•°ç»„
                try:
                    with open(target_path, 'r', encoding=encoding) as f:
                        existing_data = json.load(f)
                    
                    if not isinstance(existing_data, list):
                        existing_data = []
                    
                    existing_data.extend(data)
                    
                    with open(target_path, 'w', encoding=encoding) as f:
                        json.dump(existing_data, f, ensure_ascii=False, indent=2)
                except Exception:
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œç›´æ¥å†™å…¥
                    with open(target_path, 'w', encoding=encoding) as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"è¿½åŠ æ•°æ®åˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _deduplicate_data(self, data: List[Dict], dedup_field: Optional[str], strategy: str, seen_values: set) -> Tuple[List[Dict], int]:
        """å¯¹æ•°æ®è¿›è¡Œå»é‡
        
        Args:
            data (List[Dict]): å¾…å»é‡çš„æ•°æ®
            dedup_field (Optional[str]): å»é‡å­—æ®µï¼ŒNoneè¡¨ç¤ºå…¨é‡å»é‡
            strategy (str): å»é‡ç­–ç•¥ keep_first/keep_last
            seen_values (set): å·²è§è¿‡çš„å€¼é›†åˆ
            
        Returns:
            Tuple[List[Dict], int]: (å»é‡åæ•°æ®, å»é‡æ•°é‡)
        """
        if seen_values is None:
            return data, 0
        
        filtered_data = []
        duplicate_count = 0
        
        for item in data:
            # ç”Ÿæˆå»é‡é”®
            if dedup_field is None:
                # å…¨é‡å­—æ®µå»é‡
                dedup_key = tuple(sorted(item.items()))
            else:
                # æŒ‡å®šå­—æ®µå»é‡
                dedup_key = item.get(dedup_field)
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if dedup_key in seen_values:
                duplicate_count += 1
                if strategy == 'keep_last':
                    # keep_lastç­–ç•¥ï¼šç§»é™¤ä¹‹å‰çš„è®°å½•ï¼Œä¿ç•™å½“å‰è®°å½•
                    # ç”±äºè¿™é‡Œæ˜¯åˆ†ç‰‡å¤„ç†ï¼Œæ— æ³•ç®€å•å®ç°keep_last
                    # ç®€åŒ–å¤„ç†ï¼šä»ç„¶è·³è¿‡é‡å¤é¡¹
                    continue
                else:
                    # keep_firstç­–ç•¥ï¼šè·³è¿‡é‡å¤é¡¹
                    continue
            else:
                seen_values.add(dedup_key)
                filtered_data.append(item)
        
        return filtered_data, duplicate_count
    
    def _clean_file_ending(self, file_path: str, file_format: str, encoding: str = 'utf-8'):
        """æ¸…ç†æ–‡ä»¶æœ«å°¾çš„å¤šä½™ç©ºè¡Œ
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„
            file_format (str): æ–‡ä»¶æ ¼å¼
            encoding (str): æ–‡ä»¶ç¼–ç 
        """
        try:
            if file_format == 'jsonl':
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                
                # ç§»é™¤æœ«å°¾çš„å¤šä½™æ¢è¡Œç¬¦ï¼Œä½†ä¿ç•™æœ€åä¸€è¡Œçš„æ¢è¡Œç¬¦
                content = content.rstrip('\n') + '\n'
                
                # é‡å†™æ–‡ä»¶
                with open(file_path, 'w', encoding=encoding, newline='\n') as f:
                    f.write(content)
                
                if self.logger:
                    self.logger.debug(f"å·²æ¸…ç†æ–‡ä»¶æœ«å°¾ç©ºè¡Œ: {file_path}")
                    
        except Exception as e:
            if self.logger:
                self.logger.warning(f"æ¸…ç†æ–‡ä»¶æœ«å°¾å¤±è´¥: {str(e)}")
    
    def _save_merge_meta(self, target_path: str, meta: MergeMeta):
        """ä¿å­˜åˆå¹¶å…ƒæ•°æ®
        
        Args:
            target_path (str): ç›®æ ‡æ–‡ä»¶è·¯å¾„
            meta (MergeMeta): åˆå¹¶å…ƒæ•°æ®
        """
        try:
            # ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            target_dir = os.path.dirname(target_path)
            meta_file = os.path.join(target_dir, 'merge_meta.json')
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
            
            if self.logger:
                self.logger.info(f"åˆå¹¶å…ƒæ•°æ®å·²ä¿å­˜: {meta_file}")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"ä¿å­˜åˆå¹¶å…ƒæ•°æ®å¤±è´¥: {str(e)}")
    
    def _create_merge_info_file(self, output_dir: str, meta: MergeMeta, params: Dict):
        """åˆ›å»ºåˆå¹¶ä¿¡æ¯æ–‡ä»¶
        
        Args:
            output_dir (str): è¾“å‡ºç›®å½•
            meta (MergeMeta): åˆå¹¶å…ƒæ•°æ®
            params (Dict): åˆå¹¶å‚æ•°
        """
        try:
            # åˆ›å»ºä¿¡æ¯æ–‡ä»¶å†…å®¹
            info_content = []
            info_content.append("=" * 60)
            info_content.append("æ•°æ®åˆå¹¶ä»»åŠ¡ä¿¡æ¯")
            info_content.append("=" * 60)
            info_content.append("")
            
            # åŸºæœ¬ä¿¡æ¯
            info_content.append("ã€åŸºæœ¬ä¿¡æ¯ã€‘")
            info_content.append(f"ä»»åŠ¡ID: {meta.task_id}")
            info_content.append(f"åˆå¹¶æ¨¡å¼: {meta.merge_mode}")
            info_content.append(f"å¼€å§‹æ—¶é—´: {meta.start_time}")
            info_content.append(f"ç»“æŸæ—¶é—´: {meta.end_time}")
            info_content.append("")
            
            # è¾“å…¥æ–‡ä»¶ä¿¡æ¯
            info_content.append("ã€è¾“å…¥æ–‡ä»¶ä¿¡æ¯ã€‘")
            for i, (file_path, row_count) in enumerate(zip(meta.input_files, meta.input_row_counts)):
                file_name = os.path.basename(file_path)
                info_content.append(f"{i+1}. {file_name}")
                info_content.append(f"   è·¯å¾„: {file_path}")
                info_content.append(f"   è®°å½•æ•°: {row_count:,}")
            info_content.append("")
            
            # è¾“å‡ºä¿¡æ¯
            info_content.append("ã€è¾“å‡ºä¿¡æ¯ã€‘")
            info_content.append(f"è¾“å‡ºæ–‡ä»¶: {os.path.basename(meta.target_path)}")
            info_content.append(f"è¾“å‡ºè·¯å¾„: {meta.target_path}")
            info_content.append("")
            
            # ç»Ÿè®¡ä¿¡æ¯
            info_content.append("ã€ç»Ÿè®¡ä¿¡æ¯ã€‘")
            info_content.append(f"è¾“å…¥æ€»è®°å½•æ•°: {meta.total_input_rows:,}")
            info_content.append(f"è¾“å‡ºè®°å½•æ•°: {meta.total_output_rows:,}")
            if meta.duplicate_rows > 0:
                info_content.append(f"å»é‡è®°å½•æ•°: {meta.duplicate_rows:,}")
                retention_rate = (meta.total_output_rows / meta.total_input_rows * 100) if meta.total_input_rows > 0 else 0
                info_content.append(f"æ•°æ®ä¿ç•™ç‡: {retention_rate:.2f}%")
            info_content.append("")
            
            # å­—æ®µä¿¡æ¯
            if meta.fields:
                info_content.append("ã€å­—æ®µä¿¡æ¯ã€‘")
                info_content.append(f"å­—æ®µæ€»æ•°: {len(meta.fields)}")
                info_content.append("å­—æ®µåˆ—è¡¨:")
                for i, field in enumerate(meta.fields, 1):
                    info_content.append(f"  {i:2d}. {field}")
                info_content.append("")
            
            # åˆå¹¶å‚æ•°
            info_content.append("ã€åˆå¹¶å‚æ•°ã€‘")
            if params.get('deduplicate'):
                info_content.append(f"å»é‡è®¾ç½®: å¼€å¯")
                info_content.append(f"å»é‡å­—æ®µ: {params.get('dedup_field', 'æ— ')}")
                info_content.append(f"å»é‡ç­–ç•¥: {params.get('dedup_strategy', 'keep_first')}")
            else:
                info_content.append(f"å»é‡è®¾ç½®: å…³é—­")
            info_content.append(f"åˆ†ç‰‡å¤§å°: {params.get('chunk_size', 1000)}")
            info_content.append(f"æ–‡ä»¶ç¼–ç : {params.get('encoding', 'utf-8')}")
            info_content.append("")
            
            info_content.append("=" * 60)
            info_content.append(f"åˆå¹¶ä»»åŠ¡å®Œæˆæ—¶é—´: {meta.end_time}")
            info_content.append("=" * 60)
            
            # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
            info_file = os.path.join(output_dir, "åˆå¹¶ä¿¡æ¯.txt")
            with open(info_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(info_content))
            
            if self.logger:
                self.logger.info(f"åˆå¹¶ä¿¡æ¯æ–‡ä»¶å·²åˆ›å»º: {info_file}")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"åˆ›å»ºåˆå¹¶ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {str(e)}")

def main():
    """å‘½ä»¤è¡Œä¸»å…¥å£å‡½æ•°
    
    è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œç›¸åº”çš„æ•°æ®åˆå¹¶æ“ä½œ
    æ”¯æŒåˆå¹¶æ ¡éªŒå’Œæ•°æ®åˆå¹¶ä¸¤ç§æ“ä½œæ¨¡å¼
    """
    parser = argparse.ArgumentParser(description='æ•°æ®åˆå¹¶æ¨¡å—')
    parser.add_argument('--task_id', required=True, help='ä»»åŠ¡ID')
    parser.add_argument('--input_paths', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„(é€—å·åˆ†éš”)')
    parser.add_argument('--merge_mode', choices=['merge', 'append'], 
                       required=True, help='åˆå¹¶æ¨¡å¼')
    parser.add_argument('--target_path', required=True, help='ç›®æ ‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--deduplicate', type=bool, default=False, help='æ˜¯å¦å»é‡')
    parser.add_argument('--dedup_field', help='å»é‡å­—æ®µ')
    parser.add_argument('--dedup_strategy', choices=['keep_first', 'keep_last'],
                       default='keep_first', help='å»é‡ç­–ç•¥')
    parser.add_argument('--chunk_size', type=int, help='åˆ†ç‰‡å¤§å°')
    parser.add_argument('--encoding', help='æ–‡ä»¶ç¼–ç ')
    parser.add_argument('--validate_only', type=bool, default=False, 
                       help='ä»…æ ¡éªŒä¸æ‰§è¡Œåˆå¹¶')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆå¹¶å™¨
    merger = DataMerger()
    if not merger.init_merger():
        print(json.dumps({
            'status': 'failed',
            'error': 'æ•°æ®åˆå¹¶å™¨åˆå§‹åŒ–å¤±è´¥'
        }, ensure_ascii=False))
        return
    
    try:
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        task_params = MergeTaskParams(
            task_id=args.task_id,
            input_paths=args.input_paths.split(','),
            merge_mode=args.merge_mode,
            target_path=args.target_path,
            deduplicate=args.deduplicate,
            dedup_field=args.dedup_field,
            dedup_strategy=args.dedup_strategy,
            chunk_size=args.chunk_size or merger.chunk_size,
            encoding=args.encoding or merger.default_encoding
        )
        
        if args.validate_only:
            # ä»…æ ¡éªŒæ¨¡å¼
            validation = merger.validate_merge(task_params)
            print(json.dumps({
                'status': 'success' if validation['valid'] else 'failed',
                'validation': validation
            }, ensure_ascii=False))
        else:
            # æ‰§è¡Œåˆå¹¶
            result = merger.merge_datasets(task_params)
            
            if result:
                print(json.dumps({
                    'status': 'success',
                    'task_id': args.task_id,
                    'output': result
                }, ensure_ascii=False))
            else:
                print(json.dumps({
                    'status': 'failed',
                    'task_id': args.task_id,
                    'error': 'æ•°æ®åˆå¹¶å¤±è´¥'
                }, ensure_ascii=False))
                
    except Exception as e:
        print(json.dumps({
            'status': 'failed',
            'error': str(e)
        }, ensure_ascii=False))


# åˆ›å»ºå…¨å±€å®ä¾‹
data_merger = DataMerger()
data_merger.init_merger()  # åˆå§‹åŒ–åˆå¹¶å™¨

# å…¨å±€APIå‡½æ•°
def merge_data(source_paths: List[str], mode: str = "merge", dedup_field: str = None, output_dir: str = None, **kwargs) -> str:
    """åˆå¹¶æ•°æ®çš„å…¨å±€API"""
    
    # ç”Ÿæˆè¾“å‡ºç›®å½•ç»“æ„
    if not output_dir:
        output_dir = "./processed"
    
    # ä¸ºæ¯æ¬¡åˆå¹¶åˆ›å»ºç‹¬ç«‹çš„å­ç›®å½•
    timestamp = int(time.time())
    merge_folder_name = f"merge-{timestamp}"
    merge_output_dir = os.path.join(output_dir, merge_folder_name)
    
    # åˆ›å»ºåˆå¹¶ä¸“ç”¨ç›®å½•
    try:
        os.makedirs(merge_output_dir, exist_ok=True)
    except Exception as e:
        raise ValueError(f"æ— æ³•åˆ›å»ºåˆå¹¶è¾“å‡ºç›®å½• {merge_output_dir}: {str(e)}")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•å†™å…¥æƒé™
    if not os.access(merge_output_dir, os.W_OK):
        raise ValueError(f"åˆå¹¶è¾“å‡ºç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {merge_output_dir}")
    
    # æ ¹æ®æºæ–‡ä»¶åç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶å
    source_names = []
    file_extension = None
    for path in source_paths:
        basename = os.path.splitext(os.path.basename(path))[0]
        source_names.append(basename)
        if file_extension is None:
            file_extension = os.path.splitext(path)[1]
    
    # ç”Ÿæˆåˆå¹¶æ–‡ä»¶åï¼ˆä½¿ç”¨æ›´ç®€æ´çš„å‘½åï¼‰
    if len(source_names) <= 3:
        merged_name = "_".join(source_names)
    else:
        merged_name = f"{source_names[0]}_and_{len(source_names)-1}_others"
    
    output_filename = f"merged_{merged_name}{file_extension}"
    target_path = os.path.join(merge_output_dir, output_filename)
    
    task_params = {
        'task_id': f"merge-{timestamp}-{uuid.uuid4().hex[:6]}",
        'input_paths': source_paths,
        'merge_mode': mode,
        'target_path': target_path,
        'deduplicate': bool(dedup_field),
        'dedup_field': dedup_field,
        'dedup_strategy': kwargs.get('dedup_strategy', 'keep_first'),
        'chunk_size': kwargs.get('chunk_size', 1000),
        'encoding': kwargs.get('encoding', 'utf-8')
    }
    
    return data_merger.merge_datasets(task_params)

if __name__ == '__main__':
    main()
