# 基础支撑层配置和调用指南

> **模块群**: 基础支撑层核心模块  
> **包含模块**: `config_manager.py`、`log_manager.py`、`state_manager.py`、`utils.py`  
> **版本**: v1.0  
> **更新时间**: 2025-08-24

## 目录

1. [概述](#1-概述)
2. [配置管理模块 (config_manager.py)](#2-配置管理模块)
3. [日志管理模块 (log_manager.py)](#3-日志管理模块)
4. [状态管理模块 (state_manager.py)](#4-状态管理模块)
5. [工具函数模块 (utils.py)](#5-工具函数模块)
6. [模块集成与调用关系](#6-模块集成与调用关系)
7. [最佳实践](#7-最佳实践)
8. [常见问题与解决方案](#8-常见问题与解决方案)

---

## 1. 概述

### 1.1 功能定位

基础支撑层是整个自动数据蒸馏软件的核心基础设施，为所有上层业务模块提供统一的配置管理、日志记录、状态跟踪和工具函数支持。该层次的四个模块构成了完整的基础服务体系：

- **配置管理模块**: 统一管理全局配置，支持配置加密和动态更新
- **日志管理模块**: 提供分模块、分级别的日志记录和管理
- **状态管理模块**: 负责任务状态持久化和进度跟踪
- **工具函数模块**: 提供跨模块复用的通用工具函数

### 1.2 设计原则

- **统一性**: 所有模块使用统一的配置、日志和状态管理接口
- **可靠性**: 支持配置加密、状态持久化、异常恢复等可靠性机制
- **高性能**: 优化的文件操作、内存使用和网络处理
- **扩展性**: 模块化设计，便于功能扩展和定制

### 1.3 依赖关系

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  config_manager │    │   log_manager   │    │  state_manager  │    │     utils       │
│                 │    │                 │    │                 │    │                 │
│ • 配置加载保存  │    │ • 日志记录      │    │ • 状态持久化    │    │ • 文件操作      │
│ • 敏感信息加密  │    │ • 分模块日志    │    │ • 任务管理      │    │ • 数据处理      │
│ • 配置验证      │    │ • 日志轮转      │    │ • 进度跟踪      │    │ • 网络工具      │
└─────────────────┘    └─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │                       │
         └───────────────────────┼───────────────────────┼───────────────────────┘
                                 │                       │
                    ┌─────────────────────────────────────────────────┐
                    │              核心功能层                         │
                    │  dataset_downloader, format_converter,          │
                    │  field_extractor, data_merger, etc.            │
                    └─────────────────────────────────────────────────┘
```

---

## 2. 配置管理模块

### 2.1 模块概述

`config_manager.py` 负责统一管理软件的所有配置参数，支持YAML格式配置文件的加载、保存、更新和加密功能。

### 2.2 核心功能

#### 2.2.1 配置加载与保存

```python
from src.config_manager import config_manager

# 加载配置（自动执行）
config = config_manager.load_config()

# 获取配置值（支持嵌套路径）
timeout = config_manager.get_config("download.timeout", 300)
batch_size = config_manager.get_config("distill.batch_size", 10)

# 更新配置
success = config_manager.update_config("download.timeout", 600)
success = config_manager.update_config("distill.batch_size", 20)

# 保存配置
config_manager.save_config()
```

#### 2.2.2 敏感配置加密

```python
# 加密API密钥
result = config_manager.encrypt_config("download.api_keys.huggingface")
print(result)  # {'encrypted': True, 'message': '配置项已加密'}

# 解密获取原始值
api_key = config_manager.decrypt_config("download.api_keys.huggingface")
print(api_key)  # "hf_your_actual_token"

# 更新加密配置
config_manager.update_config("download.api_keys.huggingface", "hf_new_token")
config_manager.encrypt_config("download.api_keys.huggingface")
```

#### 2.2.3 配置验证

```python
# 验证配置完整性
result = config_manager.validate_config()

if not result['valid']:
    print("配置验证失败:")
    for error in result['errors']:
        print(f"  错误: {error}")
    
    for warning in result['warnings']:
        print(f"  警告: {warning}")
```

### 2.3 配置文件结构

默认配置文件 `config.yaml` 包含以下配置节：

```yaml
# 基础配置（所有模块共用）
base:
  root_dir: "./data"              # 主数据目录
  encoding: "utf-8"               # 默认文件编码
  max_workers: 4                  # 最大并发工作线程
  chunk_size: 1000               # 默认数据块大小

# 下载模块配置
download:
  timeout: 300                    # 下载超时时间（秒）
  max_retries: 3                 # 最大重试次数
  retry_delay: 2                 # 重试延迟（秒）
  encrypt_api_key: true          # 是否加密存储API密钥
  api_keys:
    huggingface: ""              # HuggingFace API密钥
    modelscope: ""               # ModelScope API密钥

# 处理模块配置（格式转换/字段提取/合并/清洗）
process:
  excel_engine: "openpyxl"       # Excel读取引擎
  csv_delimiter: ","             # CSV分隔符
  json_indent: 2                 # JSON缩进
  dedup_threshold: 0.95          # 去重相似度阈值
  max_file_size: 1073741824      # 最大文件大小（1GB）

# 模型管理配置
model:
  default_timeout: 600           # 模型调用超时时间
  supported_types: ["vllm", "openai", "sglang", "ollama"]
  max_batch_size: 100           # 最大批量大小
  api_keys: {}                  # 模型API密钥

# 蒸馏生成配置
distill:
  batch_size: 10                # 批量处理大小
  save_interval: 100            # 保存间隔
  max_prompt_length: 2048       # 最大提示词长度
  temperature: 0.7              # 生成温度
  max_tokens: 512              # 最大生成tokens

# 日志配置
log:
  level: "INFO"                 # 日志级别
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  max_file_size: 10485760       # 日志文件最大大小（10MB）
  backup_count: 5               # 备份文件数量
  console_output: true          # 是否输出到控制台

# 状态管理配置
state:
  auto_save: true               # 自动保存状态
  save_interval: 30             # 保存间隔（秒）
  max_history: 1000            # 最大历史记录数
```

### 2.4 命令行工具

```bash
# 查看所有配置
python src/config_manager.py show

# 查看特定配置节
python src/config_manager.py show --section download

# 获取配置值
python src/config_manager.py get download.timeout

# 设置配置值
python src/config_manager.py set download.timeout 600 --type int

# 加密敏感配置
python src/config_manager.py encrypt download.api_keys.huggingface

# 解密查看配置
python src/config_manager.py decrypt download.api_keys.huggingface

# 验证配置
python src/config_manager.py validate

# 重置配置
python src/config_manager.py reset --section download
```

---

## 3. 日志管理模块

### 3.1 模块概述

`log_manager.py` 为所有模块提供统一的日志记录功能，支持按模块分类、分级输出、日志轮转和敏感信息过滤。

### 3.2 核心功能

#### 3.2.1 基本日志记录

```python
from src.log_manager import log_manager

# 获取模块日志记录器
logger = log_manager.get_logger("dataset_downloader")

# 记录不同级别的日志
logger.debug("调试信息", task_id="task_001")
logger.info("数据下载开始", task_id="task_001", dataset_name="squad")
logger.warning("网络连接不稳定", task_id="task_001", retry_count=2)
logger.error("下载失败", task_id="task_001", error_code=404)

# 记录异常日志（包含堆栈信息）
try:
    # 可能出错的代码
    pass
except Exception as e:
    logger.exception("处理过程中发生异常", task_id="task_001")
```

#### 3.2.2 任务关联日志

```python
# 日志自动关联任务ID和上下文信息
logger.info("开始处理数据集", 
           task_id="download-20241001-abc123",
           file_size="1.2GB",
           source="huggingface")

# 输出示例：
# 2024-10-01 10:00:00 - dataset_downloader - INFO - [download-20241001-abc123] 开始处理数据集 | file_size=1.2GB, source=huggingface
```

#### 3.2.3 敏感信息过滤

```python
# 自动过滤敏感信息
logger.info("连接配置", 
           task_id="task_001", 
           api_key="hf_1234567890abcdef1234567890abcdef12")

# 实际输出：
# 2024-10-01 10:00:00 - dataset_downloader - INFO - [task_001] 连接配置 | api_key=***
```

#### 3.2.4 日志管理操作

```python
# 动态设置日志级别
log_manager.set_log_level("DEBUG")

# 添加自定义处理器
import logging
custom_handler = logging.FileHandler("custom.log")
log_manager.add_log_handler(custom_handler, ["dataset_downloader"])

# 获取日志文件列表
log_files = log_manager.get_log_files("dataset_downloader", "20241001")

# 搜索日志内容
results = log_manager.search_logs("错误", "dataset_downloader")
for result in results:
    print(f"{result['file']}:{result['line_number']} - {result['content']}")

# 清理过期日志
log_manager.cleanup_old_logs(7)  # 只保留7天的日志
```

### 3.3 日志文件组织

```
./data/logs/
├── 20241001/                    # 按日期组织
│   ├── dataset_downloader.log   # 数据下载模块日志
│   ├── format_converter.log     # 格式转换模块日志
│   ├── state_manager.log        # 状态管理模块日志
│   └── ...
├── 20241002/
│   └── ...
└── ...
```

### 3.4 命令行工具

```bash
# 设置日志级别
python src/log_manager.py level DEBUG

# 列出日志文件
python src/log_manager.py list --module dataset_downloader --date 20241001

# 搜索日志内容
python src/log_manager.py search "错误" --module dataset_downloader

# 清理过期日志
python src/log_manager.py cleanup --days 7

# 测试日志功能
python src/log_manager.py test --module test_module
```

---

## 4. 状态管理模块

### 4.1 模块概述

`state_manager.py` 负责记录和管理所有任务的状态和进度信息，支持状态持久化、断点续传和实时进度跟踪。

### 4.2 核心功能

#### 4.2.1 任务生命周期管理

```python
from src.state_manager import state_manager, TaskStatus, TaskType

# 创建新任务
task_id = state_manager.add_task(
    task_type=TaskType.DOWNLOAD,
    task_subtype="huggingface",
    params={
        "dataset_name": "squad",
        "save_dir": "./data/raw",
        "api_key": "hf_***"
    }
)
print(f"任务已创建: {task_id}")

# 更新任务状态
state_manager.update_state(task_id, "status", TaskStatus.RUNNING)
state_manager.update_state(task_id, "progress", 25.5)

# 更新统计信息
state_manager.update_state(task_id, "statistics.total_items", 1000)
state_manager.update_state(task_id, "statistics.processed_items", 255)

# 添加元数据
state_manager.update_state(task_id, "metadata.file_size", "1.2GB")
state_manager.update_state(task_id, "metadata.source_url", "https://...")

# 完成任务
state_manager.update_state(task_id, "status", TaskStatus.COMPLETED)
state_manager.update_state(task_id, "progress", 100.0)
state_manager.update_state(task_id, "result", "./data/raw/squad/dataset.jsonl")
```

#### 4.2.2 任务状态查询

```python
# 获取单个任务状态
task_state = state_manager.get_task_state(task_id)
if task_state:
    print(f"任务状态: {task_state['status']}")
    print(f"进度: {task_state['progress']:.1f}%")
    print(f"开始时间: {task_state['start_time']}")

# 列出符合条件的任务
running_tasks = state_manager.list_tasks(status=TaskStatus.RUNNING)
download_tasks = state_manager.list_tasks(task_type=TaskType.DOWNLOAD, limit=10)

for task in running_tasks:
    print(f"{task['task_id']} - {task['progress']:.1f}%")
```

#### 4.2.3 任务控制操作

```python
# 暂停任务
success = state_manager.pause_task(task_id)

# 恢复任务
success = state_manager.resume_task(task_id)

# 取消任务
success = state_manager.cancel_task(task_id)

# 删除已完成的任务
success = state_manager.delete_task(task_id)
```

#### 4.2.4 统计和清理

```python
# 获取任务统计信息
stats = state_manager.get_statistics()
print(f"总任务数: {stats['total_tasks']}")
print(f"活跃任务: {stats['active_tasks']}")
print(f"今日完成: {stats['completed_today']}")

# 清理过期任务
result = state_manager.cleanup_old_tasks(
    days_to_keep=30,
    keep_failed=True  # 保留失败的任务用于调试
)
print(f"清理了 {result['removed_count']} 个过期任务")
```

### 4.3 状态文件格式

状态文件 `./data/states/default.json` 包含所有任务的状态信息：

```json
{
  "tasks": {
    "download-20241001120000-abc123": {
      "task_id": "download-20241001120000-abc123",
      "task_type": "download",
      "task_subtype": "huggingface",
      "status": "completed",
      "progress": 100.0,
      "params": {
        "dataset_name": "squad",
        "save_dir": "./data/raw"
      },
      "created_time": "2024-10-01T12:00:00",
      "start_time": "2024-10-01T12:00:05",
      "end_time": "2024-10-01T12:30:15",
      "last_updated": "2024-10-01T12:30:15",
      "error_message": null,
      "result": "./data/raw/squad/dataset.jsonl",
      "statistics": {
        "total_items": 1000,
        "processed_items": 1000,
        "failed_items": 0,
        "retry_count": 2
      },
      "metadata": {
        "file_size": "1.2GB",
        "source_url": "https://huggingface.co/datasets/squad"
      }
    }
  },
  "last_updated": "2024-10-01T12:30:15",
  "version": "1.0"
}
```

### 4.4 命令行工具

```bash
# 列出所有任务
python src/state_manager.py list

# 按状态过滤任务
python src/state_manager.py list --status running

# 显示任务详情
python src/state_manager.py show download-20241001120000-abc123

# 暂停任务
python src/state_manager.py pause download-20241001120000-abc123

# 恢复任务
python src/state_manager.py resume download-20241001120000-abc123

# 取消任务
python src/state_manager.py cancel download-20241001120000-abc123

# 显示统计信息
python src/state_manager.py stats

# 清理过期任务
python src/state_manager.py cleanup --days 30 --keep-failed
```

---

## 5. 工具函数模块

### 5.1 模块概述

`utils.py` 提供跨模块复用的通用工具函数，包括文件操作、数据处理、网络工具、安全工具等。

### 5.2 文件操作工具

#### 5.2.1 分片读取大文件

```python
from src.utils import FileOperations

# 自动检测格式并分片读取
for chunk in FileOperations.read_file_chunk("large_data.jsonl", chunk_size=1000):
    print(f"处理了 {len(chunk)} 行数据")
    for item in chunk:
        # 处理每行数据
        process_item(item)

# 指定格式读取
for chunk in FileOperations.read_file_chunk("data.csv", 500, "csv"):
    # 处理CSV数据块
    pass
```

#### 5.2.2 文件信息和校验

```python
# 获取文件详细信息
info = FileOperations.get_file_info("dataset.jsonl")
print(f"文件大小: {info['size_mb']} MB")
print(f"行数: {info['line_count']}")
print(f"格式: {info['format']}")

# 计算文件哈希值
hash_md5 = FileOperations.get_file_hash("dataset.jsonl", "md5")
hash_sha256 = FileOperations.get_file_hash("dataset.jsonl", "sha256")

# 自动检测文件格式
format_type = FileOperations.detect_file_format("unknown_file.txt")
```

### 5.3 数据处理工具

#### 5.3.1 字典操作

```python
from src.utils import DataProcessing

# 展平嵌套字典
nested_data = {
    "user": {"name": "Alice", "profile": {"age": 30, "city": "Beijing"}},
    "id": 1
}
flat_data = DataProcessing.flatten_dict(nested_data)
# 结果: {"user.name": "Alice", "user.profile.age": 30, "user.profile.city": "Beijing", "id": 1}
```

#### 5.3.2 数据去重

```python
# 基于字段相似度去重
data = [
    {"text": "Hello world", "id": 1},
    {"text": "Hello world!", "id": 2},  # 与第一条相似
    {"text": "Different content", "id": 3}
]

unique_data, duplicate_data = DataProcessing.dedup_rows(
    data, 
    field="text", 
    threshold=0.9,  # 90%相似度
    keep="first"    # 保留第一个
)

print(f"去重后: {len(unique_data)} 条，重复: {len(duplicate_data)} 条")
```

#### 5.3.3 数据验证

```python
# 验证数据格式
validation_result = DataProcessing.validate_data_format(
    data,
    required_fields=["question", "answer"],
    field_types={"question": str, "answer": str, "id": int}
)

if not validation_result['valid']:
    for error in validation_result['errors']:
        print(f"验证错误: {error}")
```

### 5.4 网络和系统工具

#### 5.4.1 网络连接检查

```python
from src.utils import NetworkUtils

# 检查网络连接
result = NetworkUtils.check_network("https://huggingface.co")
if result['connected']:
    print(f"连接正常，响应时间: {result['response_time']}ms")
else:
    print(f"连接失败: {result['error']}")

# 测试端口连通性
if NetworkUtils.test_port_connectivity("api.openai.com", 443):
    print("API服务可访问")
```

#### 5.4.2 系统信息

```python
from src.utils import SystemUtils

# 获取系统信息
info = SystemUtils.get_system_info()
print(f"系统: {info['platform']}")
print(f"CPU核心数: {info['cpu_count']}")
print(f"内存: {info['memory_available_gb']} GB 可用")

# 检查依赖库
deps = SystemUtils.check_dependencies()
for lib, installed in deps.items():
    status = "已安装" if installed else "未安装"
    print(f"{lib}: {status}")
```

### 5.5 重试和性能工具

#### 5.5.1 函数重试

```python
from src.utils import RetryUtils
import requests

# 使用重试包装函数
def download_with_retry(url):
    return RetryUtils.retry_func(
        requests.get,
        args=(url,),
        kwargs={"timeout": 10},
        max_retry=3,
        delay=2,
        backoff_factor=2,
        exceptions=(requests.RequestException,)
    )

# 使用重试装饰器
@RetryUtils.retry_decorator(max_retry=3, delay=1)
def unreliable_function():
    # 可能失败的函数
    pass
```

#### 5.5.2 性能监控

```python
from src.utils import PerformanceUtils

# 函数执行时间监控
@PerformanceUtils.time_it
def process_large_data():
    # 处理大量数据
    pass

# 内存使用监控
@PerformanceUtils.monitor_memory_usage
def memory_intensive_task():
    # 内存密集型任务
    pass
```

### 5.6 安全工具

```python
from src.utils import SecurityUtils

# 生成安全密钥
secret_key = SecurityUtils.generate_secret_key(32)

# 遮蔽敏感信息
safe_log = SecurityUtils.mask_sensitive_info(
    "连接配置: api_key=hf_1234567890abcdef token=sk_abcd1234"
)
print(safe_log)  # "连接配置: api_key=******** token=********"
```

### 5.7 命令行工具

```bash
# 文件操作测试
python src/utils.py file dataset.jsonl --chunk-size 500

# 计算文件哈希
python src/utils.py hash dataset.jsonl --algorithm sha256

# 网络连接测试
python src/utils.py network --url https://huggingface.co

# 系统信息
python src/utils.py system

# 依赖检查
python src/utils.py deps

# 去重测试
python src/utils.py dedup
```

---

## 6. 模块集成与调用关系

### 6.1 标准集成模式

所有核心功能模块都应按以下模式集成基础支撑层：

```python
#!/usr/bin/env python3
"""
示例核心功能模块集成基础支撑层
"""

# 导入基础支撑层
from src.config_manager import config_manager
from src.log_manager import log_manager
from src.state_manager import state_manager, TaskStatus, TaskType
from src.utils import (
    FileOperations, DataProcessing, NetworkUtils, 
    RetryUtils, SecurityUtils
)

class ExampleProcessor:
    """示例处理器类"""
    
    def __init__(self):
        # 获取配置
        self.config = config_manager.get_config("example")
        self.timeout = config_manager.get_config("example.timeout", 300)
        
        # 获取日志记录器
        self.logger = log_manager.get_logger("example_processor")
        
        self.logger.info("示例处理器初始化完成", 
                        timeout=self.timeout)
    
    def process_data(self, input_file: str, output_dir: str) -> str:
        """处理数据的示例方法"""
        
        # 创建任务状态记录
        task_id = state_manager.add_task(
            task_type=TaskType.CONVERT,
            task_subtype="example_process",
            params={
                "input_file": input_file,
                "output_dir": output_dir
            }
        )
        
        try:
            # 更新任务状态为运行中
            state_manager.update_state(task_id, "status", TaskStatus.RUNNING)
            self.logger.info("开始处理数据", task_id=task_id, 
                           input_file=input_file)
            
            # 获取文件信息
            file_info = FileOperations.get_file_info(input_file)
            total_lines = file_info.get('line_count', 0)
            
            state_manager.update_state(task_id, "statistics.total_items", total_lines)
            
            processed_count = 0
            output_file = Path(output_dir) / "processed_data.jsonl"
            
            # 分片处理文件
            with open(output_file, 'w', encoding='utf-8') as out_f:
                for chunk in FileOperations.read_file_chunk(input_file, 1000):
                    # 处理数据块
                    processed_chunk = self._process_chunk(chunk, task_id)
                    
                    # 写入结果
                    for item in processed_chunk:
                        out_f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    
                    # 更新进度
                    processed_count += len(chunk)
                    progress = (processed_count / total_lines) * 100 if total_lines > 0 else 0
                    
                    state_manager.update_state(task_id, "progress", progress)
                    state_manager.update_state(task_id, "statistics.processed_items", processed_count)
                    
                    self.logger.info(f"已处理 {processed_count}/{total_lines} 行", 
                                   task_id=task_id, progress=f"{progress:.1f}%")
            
            # 任务完成
            state_manager.update_state(task_id, "status", TaskStatus.COMPLETED)
            state_manager.update_state(task_id, "progress", 100.0)
            state_manager.update_state(task_id, "result", str(output_file))
            
            self.logger.info("数据处理完成", task_id=task_id, 
                           output_file=str(output_file))
            
            return str(output_file)
            
        except Exception as e:
            # 处理异常
            state_manager.update_state(task_id, "status", TaskStatus.FAILED)
            state_manager.update_state(task_id, "error_message", str(e))
            
            self.logger.exception("数据处理失败", task_id=task_id)
            raise
    
    def _process_chunk(self, chunk: List[Dict], task_id: str) -> List[Dict]:
        """处理数据块的示例方法"""
        
        # 使用重试机制处理网络请求
        @RetryUtils.retry_decorator(max_retry=3, delay=1)
        def api_call(data):
            # 模拟API调用
            return {"processed": data}
        
        processed_chunk = []
        for item in chunk:
            try:
                # 处理单个数据项
                processed_item = api_call(item)
                processed_chunk.append(processed_item)
                
            except Exception as e:
                self.logger.warning("处理数据项失败", task_id=task_id, 
                                  error=str(e), item_id=item.get('id'))
                # 记录失败统计
                state_manager.update_state(task_id, "statistics.failed_items", 
                                         lambda x: (x or 0) + 1)
        
        return processed_chunk

# 命令行入口
if __name__ == "__main__":
    processor = ExampleProcessor()
    result = processor.process_data("input.jsonl", "./output")
    print(f"处理完成: {result}")
```

### 6.2 配置统一管理

所有模块的配置应统一在 `config.yaml` 中管理：

```yaml
# 为新模块添加配置节
example_processor:
  timeout: 300
  batch_size: 100
  output_format: "jsonl"
  enable_validation: true
```

模块中获取配置：

```python
# 获取模块特定配置
example_config = config_manager.get_config("example_processor")
timeout = config_manager.get_config("example_processor.timeout", 300)

# 获取基础配置
root_dir = config_manager.get_config("base.root_dir", "./data")
max_workers = config_manager.get_config("base.max_workers", 4)
```

### 6.3 日志记录规范

所有模块应遵循统一的日志记录规范：

```python
# 日志记录器命名：使用模块名称
logger = log_manager.get_logger("module_name")

# 日志消息格式：简洁明确的描述
logger.info("开始下载数据集", task_id=task_id, dataset="squad")
logger.warning("网络连接不稳定", task_id=task_id, retry_count=2)
logger.error("下载失败", task_id=task_id, error_code=404, url="https://...")

# 异常日志：使用exception方法自动记录堆栈
try:
    risky_operation()
except Exception as e:
    logger.exception("操作失败", task_id=task_id, operation="download")
```

### 6.4 状态管理规范

所有长时间运行的操作都应创建任务状态记录：

```python
# 创建任务
task_id = state_manager.add_task(
    task_type=TaskType.DOWNLOAD,  # 使用枚举值
    task_subtype="huggingface",   # 具体的子类型
    params={                      # 任务参数
        "dataset_name": "squad",
        "save_dir": "./data/raw"
    }
)

# 状态更新时机
state_manager.update_state(task_id, "status", TaskStatus.RUNNING)    # 开始时
state_manager.update_state(task_id, "progress", 50.0)               # 进度更新
state_manager.update_state(task_id, "statistics.total_items", 1000) # 统计更新
state_manager.update_state(task_id, "status", TaskStatus.COMPLETED) # 完成时
```

---

## 7. 最佳实践

### 7.1 配置管理最佳实践

#### 7.1.1 配置分层设计

```python
# 推荐的配置结构
config_structure = {
    "base": {          # 基础配置，所有模块共用
        "root_dir": "./data",
        "encoding": "utf-8"
    },
    "module_name": {   # 模块特定配置
        "timeout": 300,
        "batch_size": 100
    }
}

# 模块中获取配置的最佳实践
class MyModule:
    def __init__(self):
        # 获取基础配置
        self.root_dir = config_manager.get_config("base.root_dir", "./data")
        
        # 获取模块配置，提供合理默认值
        self.timeout = config_manager.get_config("module_name.timeout", 300)
        self.batch_size = config_manager.get_config("module_name.batch_size", 100)
```

#### 7.1.2 敏感信息处理

```python
# API密钥等敏感信息应当加密存储
def setup_api_key(api_key: str):
    # 存储加密的API密钥
    config_manager.update_config("download.api_keys.huggingface", api_key)
    config_manager.encrypt_config("download.api_keys.huggingface")

def get_api_key() -> str:
    # 获取解密的API密钥
    return config_manager.decrypt_config("download.api_keys.huggingface")
```

### 7.2 日志记录最佳实践

#### 7.2.1 日志级别使用指南

```python
# DEBUG: 详细的调试信息，生产环境关闭
logger.debug("函数参数", task_id=task_id, params=params)

# INFO: 正常业务流程信息
logger.info("开始下载数据集", task_id=task_id, dataset="squad")

# WARNING: 警告信息，程序可以继续运行
logger.warning("网络连接不稳定，正在重试", task_id=task_id, retry_count=2)

# ERROR: 错误信息，影响当前操作
logger.error("下载失败", task_id=task_id, error_code=404)

# CRITICAL: 严重错误，可能导致程序退出
logger.critical("配置文件损坏", config_file="config.yaml")
```

#### 7.2.2 结构化日志信息

```python
# 推荐：使用结构化的日志信息
logger.info("文件处理完成", 
           task_id=task_id,
           input_file="data.jsonl",
           output_file="processed.jsonl",
           processing_time=120.5,
           records_processed=1000)

# 避免：将所有信息放在消息字符串中
logger.info(f"文件处理完成: {input_file} -> {output_file}, 耗时{processing_time}秒")
```

### 7.3 状态管理最佳实践

#### 7.3.1 任务状态生命周期

```python
def process_with_state_management(params):
    # 1. 创建任务
    task_id = state_manager.add_task(TaskType.PROCESS, "example", params)
    
    try:
        # 2. 开始处理
        state_manager.update_state(task_id, "status", TaskStatus.RUNNING)
        
        # 3. 设置总量
        total_items = get_total_items()
        state_manager.update_state(task_id, "statistics.total_items", total_items)
        
        # 4. 处理过程中更新进度
        for i, item in enumerate(items):
            process_item(item)
            
            # 定期更新进度（避免过于频繁）
            if i % 100 == 0:
                progress = (i / total_items) * 100
                state_manager.update_state(task_id, "progress", progress)
                state_manager.update_state(task_id, "statistics.processed_items", i)
        
        # 5. 完成处理
        state_manager.update_state(task_id, "status", TaskStatus.COMPLETED)
        state_manager.update_state(task_id, "progress", 100.0)
        state_manager.update_state(task_id, "result", result_path)
        
    except Exception as e:
        # 6. 处理异常
        state_manager.update_state(task_id, "status", TaskStatus.FAILED)
        state_manager.update_state(task_id, "error_message", str(e))
        raise
    
    return task_id
```

#### 7.3.2 进度更新策略

```python
# 批量更新进度，避免过于频繁的状态写入
class ProgressTracker:
    def __init__(self, task_id: str, total: int, update_interval: int = 100):
        self.task_id = task_id
        self.total = total
        self.processed = 0
        self.update_interval = update_interval
        self.last_update = 0
    
    def update(self, count: int = 1):
        self.processed += count
        
        # 达到更新间隔或处理完成时更新状态
        if (self.processed - self.last_update >= self.update_interval or 
            self.processed >= self.total):
            
            progress = (self.processed / self.total) * 100
            state_manager.update_state(self.task_id, "progress", progress)
            state_manager.update_state(self.task_id, "statistics.processed_items", self.processed)
            
            self.last_update = self.processed
```

### 7.4 工具函数使用最佳实践

#### 7.4.1 大文件处理

```python
# 推荐：使用分片读取处理大文件
def process_large_file(file_path: str):
    total_processed = 0
    
    for chunk in FileOperations.read_file_chunk(file_path, chunk_size=1000):
        # 处理数据块
        processed_chunk = process_chunk(chunk)
        
        # 保存中间结果
        save_chunk_result(processed_chunk)
        
        total_processed += len(chunk)
        print(f"已处理 {total_processed} 行")

# 避免：一次性加载大文件到内存
def process_large_file_bad(file_path: str):
    with open(file_path, 'r') as f:
        all_data = json.load(f)  # 可能导致内存不足
    
    return process_all_data(all_data)
```

#### 7.4.2 网络请求重试

```python
# 推荐：使用重试机制处理不稳定的网络请求
@RetryUtils.retry_decorator(
    max_retry=3, 
    delay=2, 
    backoff_factor=2,
    exceptions=(requests.RequestException, ConnectionError)
)
def download_data(url: str):
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return response.json()

# 或者使用函数包装
def download_with_retry(url: str):
    return RetryUtils.retry_func(
        requests.get,
        args=(url,),
        kwargs={"timeout": 30},
        max_retry=3
    )
```

---

## 8. 常见问题与解决方案

### 8.1 配置管理问题

#### 问题1：配置文件损坏

**症状**：程序启动时报告YAML格式错误

**解决方案**：
```python
# 配置管理器会自动备份损坏的配置文件
# 1. 检查备份文件：config.yaml.backup
# 2. 手动恢复或使用默认配置重新开始

# 重置到默认配置
python src/config_manager.py reset
```

#### 问题2：加密密钥丢失

**症状**：无法解密之前加密的配置项

**解决方案**：
```python
# 1. 删除加密密钥文件，重新生成
rm .encryption_key

# 2. 重新设置敏感配置
python src/config_manager.py set download.api_keys.huggingface "your_new_token"
python src/config_manager.py encrypt download.api_keys.huggingface
```

### 8.2 日志管理问题

#### 问题1：日志文件过多占用磁盘空间

**解决方案**：
```bash
# 定期清理过期日志
python src/log_manager.py cleanup --days 7

# 或在代码中设置自动清理
log_manager.cleanup_old_logs(7)
```

#### 问题2：日志级别设置后不生效

**解决方案**：
```python
# 动态设置日志级别
log_manager.set_log_level("DEBUG")

# 或通过配置文件设置
python src/config_manager.py set log.level DEBUG
```

### 8.3 状态管理问题

#### 问题1：状态文件损坏

**症状**：程序启动时无法加载任务状态

**解决方案**：
```python
# 状态管理器会自动备份损坏文件并重新初始化
# 1. 检查备份文件：default.json.backup
# 2. 手动恢复重要任务状态，或重新开始

# 手动重新初始化状态文件
state_manager.init_state()
```

#### 问题2：任务状态不一致

**症状**：任务显示为运行中但实际已停止

**解决方案**：
```python
# 状态管理器会在加载时自动修复不一致状态
# 也可以手动修复
python src/state_manager.py cancel stuck_task_id
```

### 8.4 工具函数问题

#### 问题1：文件格式检测错误

**解决方案**：
```python
# 手动指定文件格式
for chunk in FileOperations.read_file_chunk("file.txt", 1000, "jsonl"):
    process_chunk(chunk)
```

#### 问题2：依赖库缺失

**解决方案**：
```bash
# 检查缺失的依赖
python src/utils.py deps

# 安装缺失的依赖
pip install pandas jsonlines requests
```

### 8.5 性能优化建议

#### 8.5.1 减少状态更新频率

```python
# 避免：每处理一个项目就更新状态
for item in items:
    process_item(item)
    state_manager.update_state(task_id, "progress", calculate_progress())  # 太频繁

# 推荐：批量更新状态
for i, item in enumerate(items):
    process_item(item)
    if i % 100 == 0:  # 每100个项目更新一次
        state_manager.update_state(task_id, "progress", calculate_progress())
```

#### 8.5.2 优化日志记录

```python
# 避免：在紧密循环中记录DEBUG日志
for item in large_list:
    logger.debug(f"处理项目: {item}")  # 可能产生大量日志

# 推荐：使用合适的日志级别和频率
logger.info(f"开始处理 {len(large_list)} 个项目")
for i, item in enumerate(large_list):
    process_item(item)
    if i % 1000 == 0:
        logger.info(f"已处理 {i}/{len(large_list)} 个项目")
```

---

## 总结

基础支撑层的四个模块为整个自动数据蒸馏软件提供了坚实的基础设施支撑：

1. **config_manager.py** - 统一配置管理，支持加密和动态更新
2. **log_manager.py** - 分模块日志记录，支持任务关联和敏感信息过滤
3. **state_manager.py** - 任务状态管理，支持断点续传和进度跟踪
4. **utils.py** - 通用工具函数，提供文件处理、数据操作、网络工具等

通过遵循本指南中的最佳实践和调用规范，可以确保所有核心功能模块都能充分利用基础支撑层的能力，构建出稳定、可靠、高性能的数据处理系统。

基础支撑层模块已经通过测试验证，可以安全地用于生产环境。在使用过程中如遇到问题，可参考本文档的故障排除章节，或检查模块的详细注释和示例代码。
