# 核心功能_数据合并模块data\_merger.py 模块设计文档

## 一、模块定位与核心职责

### 1.1 模块定位

作为数据处理流水线的**轻量级数据集整合组件**，专注于「同结构数据集合并」与「数据去重」两大核心需求，仅支持字段完全一致的多个数据集合并（纵向拼接），以及基于指定字段或全量数据的去重操作，确保功能稳定、参数简洁，降低使用与维护成本。

### 1.2 核心职责



1.  **同结构合并**：将多个字段完全一致的数据集（如 2 个含 `question`/`answer` 字段的 JSONL 文件）纵向拼接为单数据集；

2.  **双模式支持**：

*   **新建合并**（`merge` 模式）：多文件→新建目标文件；

*   **追加合并**（`append` 模式）：多文件→追加到现有目标文件（仅目标文件字段与源文件一致时支持）；

1.  **精准去重**：合并后支持按指定字段（如 `question`）或全量字段去重，保留第一条 / 最后一条记录；

2.  **工程化保障**：大文件分片处理（避免内存溢出）、合并前格式与字段校验、合并报告生成。

## 二、核心约束（避免复杂问题）



1.  **字段一致性约束**：所有输入文件（含追加模式的目标文件）必须字段完全一致（字段名、数量、顺序均相同），否则终止合并；

2.  **格式一致性约束**：所有输入文件必须为同一格式（如均为 JSONL/CSV/Excel），不支持跨格式合并；

3.  **追加操作约束**：仅支持向目标文件追加**数据记录**（行），不支持追加字段（列）；

4.  **去重范围约束**：去重仅针对合并后的完整数据集（不支持合并过程中逐文件去重）。

## 三、模块依赖关系



| 依赖模块                 | 依赖类型 | 用途描述                                     |
| -------------------- | ---- | ---------------------------------------- |
| `config_manager.py`  | 基础支撑 | 读取默认配置（分片大小 `chunk_size`、默认编码）           |
| `log_manager.py`     | 基础支撑 | 记录合并进度、校验结果、去重统计、错误信息                    |
| `state_manager.py`   | 基础支撑 | 存储任务进度（用于断点续传）、任务状态（运行 / 完成 / 失败）        |
| `utils.py`           | 工具依赖 | 文件格式校验、字段提取、编码检测、分片读写、哈希计算               |
| `field_extractor.py` | 功能依赖 | 调用 `get_fields()` 提取文件字段，用于一致性校验         |
| 第三方库                 | 外部依赖 | `pandas`（表格格式处理）、`jsonlines`（JSONL 流式读写） |

## 四、核心数据结构

### 4.1 合并任务参数字典（MergeTaskParams）

参数精简至核心必要项，避免冗余与歧义：



```
from typing import TypedDict, Literal, Optional

class MergeTaskParams(TypedDict):

&#x20;   task\_id: str  # 任务唯一标识（如 "merge\_20241008\_001"）

&#x20;   input\_paths: list\[str]  # 待合并文件路径列表（如 \["./data/a.jsonl", "./data/b.jsonl"]）

&#x20;   merge\_mode: Literal\["merge", "append"]  # 合并模式：新建(merge)/追加(append)

&#x20;   target\_path: str  # 目标文件路径（merge模式：新建路径；append模式：现有文件路径）

&#x20;   \# 去重配置

&#x20;   deduplicate: bool  # 是否去重（True/False）

&#x20;   dedup\_field: Optional\[str]  # 去重字段（None则全量字段去重）

&#x20;   dedup\_strategy: Literal\["keep\_first", "keep\_last"]  # 去重策略：保留第一条/最后一条

&#x20;   \# 系统参数（可选，默认从配置读取）

&#x20;   chunk\_size: Optional\[int]  # 分片大小（默认 1000 行）

&#x20;   encoding: Optional\[str]  # 文件编码（默认 "utf-8"）
```

### 4.2 合并元数据（MergeMeta）

记录合并关键信息，便于追溯：



```
class MergeMeta(TypedDict):

&#x20;   task\_id: str

&#x20;   merge\_mode: str  # "merge"/"append"

&#x20;   input\_files: list\[str]  # 输入文件路径

&#x20;   input\_row\_counts: list\[int]  # 各输入文件记录数

&#x20;   target\_path: str  # 目标文件路径

&#x20;   total\_input\_rows: int  # 输入总记录数

&#x20;   total\_output\_rows: int  # 合并后记录数（去重后）

&#x20;   duplicate\_rows: int  # 去重数量（deduplicate=True时有效）

&#x20;   start\_time: str  # 开始时间（YYYY-MM-DD HH:MM:SS）

&#x20;   end\_time: str  # 结束时间

&#x20;   fields: list\[str]  # 数据集字段列表（验证一致性用）
```

## 五、核心函数设计

### 5.1 初始化函数

#### `init_merger() -> bool`



*   **功能**：初始化合并器，加载配置、日志与状态管理

*   **流程**：

1.  调用 `config_manager.get_config("data_merger")` 读取默认参数（如 `chunk_size=1000`）；

2.  初始化模块专属日志器（`log_manager.init_logger("data_merger")`）；

3.  检查 `pandas`/`jsonlines` 依赖是否安装；

*   **输出**：`True`（初始化成功）/`False`（失败）。

### 5.2 合并前置校验函数

#### `validate_merge(params: MergeTaskParams) -> dict`



*   **功能**：严格校验合并可行性，避免字段 / 格式不一致导致的错误

*   **输入**：`MergeTaskParams` 实例

*   **输出**：校验结果（含错误原因）



# 核心功能_数据合并模块 `data_merger.py` 设计文档（v2.0）

版本：v2.0（与 `src/data_merger.py` 当前实现对齐）  
最后更新时间：2025-11-15  
所属层级：核心功能层 - 数据合并模块

---

## 一、模块定位与核心职责

### 1.1 功能定位

`data_merger.py` 是数据处理流水线中的**同结构数据集合并组件**，面向已经经过格式标准化与字段整理的数据集（通常来自 `format_converter.py` 和 `field_extractor.py`），提供：

- 对字段结构完全一致的多个数据集进行**纵向合并**；
- 在合并过程中执行**按字段或全量记录级的去重**；
- 针对大体量数据集提供**分片处理**能力，避免内存溢出；
- 生成详细的**合并元数据与说明文件**，便于后续追踪与审计。

模块聚焦于「同结构文件的纵向合并」这一单一场景，不支持横向拼接（按列 join）和复杂 SQL 级多表操作。

### 1.2 核心职责

- **同结构合并**：在保证字段集合完全一致的前提下，将多个同格式文件纵向拼接为单一文件；
- **两种合并模式**：
  - `merge`：新建合并文件；
  - `append`：在已有目标文件基础上追加记录（前提是目标文件格式与字段一致）；
- **可选去重**：支持基于指定字段或全量记录的去重；
- **大文件友好**：通过 `chunk_size` 参数控制分片大小，按片读取和写入；
- **元数据与报告**：输出 `merge_meta.json` 与友好的中文文本报告 `合并信息.txt`；
- **状态集成**：在可用时与 `state_manager` 集成，记录任务进度和最终状态。

### 1.3 模块边界

**负责：**

- 读取 CSV/Excel/JSON/JSONL 文件，进行字段一致性校验；
- 按指定模式执行数据集合并与去重；
- 输出合并后数据文件和配套元数据/报告；
- 通过命令行接口执行一次性合并任务，或通过全局实例提供简单 API。

**不负责：**

- 对字段含义或类型的高级推断与清洗（交由 `data_cleaner.py` 等模块）；
- 对不同结构文件的自动对齐或字段映射；
- 复杂的窗口去重、分组聚合等高级数据处理。

---

## 二、依赖关系与运行环境

### 2.1 内部依赖模块

模块在包内优先通过相对导入使用基础支撑与工具模块：

| 依赖模块              | 角色       | 用途概述                                  |
|-----------------------|------------|-------------------------------------------|
| `ConfigManager`       | 配置管理    | 读取 `data_merger` 相关配置（chunk_size 等） |
| `LogManager`          | 日志管理    | 获取 `data_merger` 专属 `logger`          |
| `StateManager`        | 状态管理    | 记录任务状态和进度（可选，如不可用则退化） |
| `validate_file` 等    | 工具函数    | 基础文件检查、编码检测、目录创建（当前实现仅使用编码和目录工具） |
| `FieldExtractor`      | 字段提取    | 用于精细字段一致性校验（可选，如果导入失败则使用简化字段提取） |

当包内导入失败（直接运行脚本场景）时，会尝试本地导入同名模块；若仍失败，则使用内置简化版本：

- 简化 `ConfigManager/LogManager/StateManager/FieldExtractor`（多数替换为 `None`）；
- 自行实现 `validate_file/get_file_encoding/ensure_dir` 等简化工具函数；
- 通过标准库 `logging` 构造最小可用的日志器。

### 2.2 第三方依赖

- `pandas`：读取/写入 CSV 与 Excel 文件，进行数据帧级合并；
- `jsonlines`：流式读写 JSONL 文件；
- 标准库：`json`、`logging`、`argparse`、`pathlib.Path`、`datetime`、`uuid` 等。

如 `pandas/jsonlines/openpyxl` 等关键依赖缺失，`init_merger()` 会返回 `False`，并记录错误日志。

---

## 三、核心数据结构

### 3.1 合并任务参数 `MergeTaskParams`

在源码中定义为 `TypedDict`：

```python
class MergeTaskParams(TypedDict):
   task_id: str
   input_paths: List[str]
   merge_mode: Literal["merge", "append"]
   target_path: str

   # 去重配置
   deduplicate: bool
   dedup_field: Optional[str]
   dedup_strategy: Literal["keep_first", "keep_last"]

   # 系统参数
   chunk_size: Optional[int]
   encoding: Optional[str]
```

各字段含义：

- `task_id`：任务唯一标识，在 CLI 中由调用方传入，在 API `merge_data` 中自动生成（如 `merge-<timestamp>-<6位随机>`）。
- `input_paths`：待合并的输入文件路径列表，至少包含 1 个文件，要求格式一致且字段集合一致。
- `merge_mode`：合并模式：
  - `"merge"`：生成新文件；
  - `"append"`：向已有 `target_path` 追加记录（需与输入字段完全一致）。
- `target_path`：输出文件路径；在 `merge` 模式下可不存在，在 `append` 模式下必须是已存在的文件。
- `deduplicate`：是否启用去重；`True` 时将对全量合并数据执行去重。
- `dedup_field`：去重字段名；
  - `None`：表示对整行记录做全量去重（以字段键值对集合为 key）；
  - 非空字符串：以该字段值为 key 去重。
- `dedup_strategy`：保留策略，目前实现只真正支持 `keep_first`；当传入 `keep_last` 时，由于分片执行的局限，仍然按“保留首条出现”处理，仅在元数据和信息文件中保留原参数值。
- `chunk_size`：分片大小（行数），用于控制单次加载到内存的记录数；如未提供，则使用配置或默认值 `1000`。
- `encoding`：输入/输出文件编码；如未提供，则使用配置或 `'utf-8'`。

### 3.2 合并元数据 `MergeMeta`

源码中的定义：

```python
class MergeMeta(TypedDict):
   task_id: str
   merge_mode: str
   input_files: List[str]
   input_row_counts: List[int]
   target_path: str
   total_input_rows: int
   total_output_rows: int
   duplicate_rows: int
   start_time: str
   end_time: str
   fields: List[str]
```

字段说明：

- `task_id`：同上，来自任务参数；
- `merge_mode`：`"merge"` 或 `"append"`；
- `input_files`：输入文件路径列表，与 `MergeTaskParams.input_paths` 一致；
- `input_row_counts`：各输入文件的记录数估计值（部分格式使用近似值，如 Excel 默认 1000）；
- `target_path`：合并后输出文件路径；
- `total_input_rows`：合并前所有输入文件记录数总和；
- `total_output_rows`：合并后输出文件记录数（去重后）；
- `duplicate_rows`：去重过滤掉的记录数量；
- `start_time` / `end_time`：任务开始/结束时间（ISO 字符串）；
- `fields`：基准字段列表（以第一个输入文件为基准，确保所有文件字段集合一致）。

在合并结束后，通过 `_save_merge_meta` 写入 `merge_meta.json`，并由 `_create_merge_info_file` 再生成面向用户的中文文本报告。

---

## 四、核心类设计

### 4.1 `DataMerger` 类概览

```python
class DataMerger:
   def __init__(self): ...
   def init_merger(self) -> bool: ...
   def validate_merge(self, params: MergeTaskParams) -> Dict[str, Any]: ...
   def merge_datasets(self, params: MergeTaskParams) -> Optional[str]: ...
   # 以及若干内部工具方法（_detect_format/_read_file_chunks 等）
```

#### 4.1.1 初始化 `__init__`

构造时仅设置默认配置与占位属性，不做重型操作：

- `self.config_manager/log_manager/state_manager/field_extractor/logger` 初始为 `None`；
- `self.default_config`：
  - `chunk_size: 1000`
  - `default_encoding: 'utf-8'`
  - `supported_formats: ['csv', 'xlsx', 'xls', 'json', 'jsonl']`
  - `max_parallel_tasks: 3`（当前未显式使用）
  - `temp_dir: './data/temp'`
- `self.supported_formats`：同样记录可接受的文件格式列表。

#### 4.1.2 环境初始化 `init_merger()`

负责接入基础支撑模块并校验运行条件：

1. 若 `ConfigManager` 可用，则：
  - 实例化 `ConfigManager`，读取 `data_merger` 配置或回落到 `default_config`；
  - 设置 `self.chunk_size/self.default_encoding/self.temp_dir`；
2. 若 `LogManager` 可用，则：
  - 实例化 `LogManager`，通过 `get_logger('data_merger')` 获取模块级日志器；
  - 记录“初始化开始/成功/失败”等信息；
  - 否则创建一个基于标准 `logging` 的 console logger；
3. 若 `StateManager` 可用，则：
  - 实例化 `StateManager`，调用 `init_state('data_merger')` 建立命名空间；
4. 若 `FieldExtractor` 可用，则：
  - 实例化 `FieldExtractor` 并调用 `init_extractor()`，用于 `validate_merge` 中字段一致性校验；
5. 检查 `pandas/jsonlines` 是否可用，不可用则记录错误并返回 `False`；
6. 确保 `temp_dir` 存在（调用 `ensure_dir`）。

返回值：

- `True`：初始化成功，可继续执行合并任务；
- `False`：初始化失败（通常是依赖缺失或配置异常）。

**注意：** 此方法在模块末尾被调用一次，用于初始化全局实例 `data_merger`。CLI 中也会单独实例化并初始化一个局部 `DataMerger`。

---

## 五、核心流程与函数

### 5.1 合并前校验 `validate_merge()`

签名：`validate_merge(self, params: MergeTaskParams) -> Dict[str, Any]`

职责：在执行合并前，对输入参数和文件集合做严格校验，避免运行时结构不一致导致的不可恢复错误。

校验步骤：

1. **基础参数校验：**
  - `input_paths` 非空，且长度 ≥ 1；
  - `target_path` 非空；
2. **文件存在性与格式一致性：**
  - 遍历 `input_paths`，检查每个文件是否存在；
  - 使用 `_detect_format`（基于文件扩展名）确定格式；
  - 若遇到不支持格式或存在多种不同格式，则返回 `valid=False` 并给出错误原因；
3. **字段一致性校验：**
  - 若 `FieldExtractor` 可用，则调用 `self.field_extractor.get_fields(file_path)` 获取字段信息，并取 `name` 列表；
  - 否则使用 `_get_fields_simple`：
    - CSV/Excel：用 `pandas` 读取 1 行，获取列名；
    - JSONL：读取首条记录的键；
    - JSON：根据数据结构（list/dict）获取键集合；
  - 以第一个文件为基准，后续文件的字段集合必须与其集合完全一致（顺序可以不同，使用集合比较）；
  - 不一致时，构造详细差异说明（基准独有字段/当前文件独有字段等），通过 `reason/details` 返回。
4. **append 模式下目标文件校验：**
  - 若 `merge_mode == 'append'`：
    - 要求 `target_path` 必须已存在；
    - 检查目标文件格式必须与输入格式相同；
    - 检查目标文件字段集合必须与基准字段集合完全一致。
5. **去重配置校验：**
  - 若 `deduplicate=True` 且 `dedup_field` 非空，要求 `dedup_field` 必须存在于基准字段中。

返回结构示例：

```python
{
   'valid': True,
   'reason': '校验通过',
   'details': {
      'base_fields': [...],
      'format': 'jsonl',
      'target_exists': False  # append 模式下为 True/False
   }
}
```

在 CLI 和全局 API 中，通常先调用该方法以给出友好的错误提示，而 `merge_datasets` 内部也会再次调用确保安全。

### 5.2 执行合并 `merge_datasets()`

签名：`merge_datasets(self, params: MergeTaskParams) -> Optional[str]`

职责：按照传入参数实际执行数据集合并、可选去重以及元数据/报告生成，并返回输出文件路径。

核心流程：

1. 记录日志“开始数据合并任务”；
2. 调用 `validate_merge(params)`，若 `valid=False`，则记录错误并抛出 `ValueError`；
3. 调用 `_get_base_info(input_paths)` 获取：
  - `base_fields`：基准字段名列表；
  - `file_format`：统一文件格式；
  - `total_rows`：估算总记录数；
4. 若 `state_manager` 可用，则在 `task.{task_id}` 下写入初始状态（running、progress=0、total_rows 等）；
5. 解析运行参数：
  - `target_path` = `params['target_path']`；
  - `merge_mode`、`chunk_size`、`encoding` 等分别从 `params` 或配置中读取；
  - 调用 `ensure_dir(os.path.dirname(target_path))` 确保输出目录存在；
6. 初始化统计变量：
  - `total_input_rows / total_output_rows / duplicate_rows / input_row_counts / processed_rows` 等；
  - 若启用去重，构造 `seen_values = set()`；
7. **准备输出文件：**
  - 若 `merge_mode == 'merge'`，调用 `_init_output_file` 创建空文件：
    - CSV/Excel 创建带表头的空表；
    - JSONL 创建空文件；
    - JSON 写入空数组 `[]`；
  - `append` 模式则直接在已有文件末尾追加；
8. **逐文件分片处理：**
  - 遍历每个 `input_path`：
    - 记录当前文件处理日志；
    - 为当前文件累加 `file_row_count`；
    - 通过 `_read_file_chunks(input_path, file_format, chunk_size, encoding)` 迭代每个分片：
     - 更新 `file_row_count` 与 `total_input_rows`；
     - 若 `deduplicate=True`，调用 `_deduplicate_data(chunk_data, dedup_field, dedup_strategy, seen_values)`；
     - 否则直接作为待写入数据；
     - 将去重后的数据 append 到 `write_buffer`；
     - 当 `len(write_buffer)` 达到动态最大缓存量（`max_buffer_rows = min(chunk_size * 5, 50000)`）时：
      - 调用 `_append_to_file(target_path, file_format, write_buffer, encoding)` 写入；
      - 清空缓冲并强制垃圾回收以释放内存；
     - 增加 `processed_rows` 并通过 `state_manager`（如可用）更新进度百分比；
    - 文件结束后，将剩余 `write_buffer` 一次性写入；
    - 将 `file_row_count` 推入 `input_row_counts`，记录日志“一文件处理完成”。
9. **收尾工作：**
  - 调用 `_clean_file_ending` 清理 JSONL 文件末尾多余空行；
  - 构造 `MergeMeta` 实例并调用 `_save_merge_meta` 写入 `merge_meta.json`；
  - 调用 `_create_merge_info_file` 在输出目录下生成中文报告 `合并信息.txt`；
  - 若 `state_manager` 可用，更新 `task.{task_id}` 状态为 `completed` 并写入 `output_path/end_time` 等；
  - 记录日志“任务完成，输入/输出记录数与去重数量统计”。

错误处理：

- 合并过程中抛出的异常会记录为错误日志；
- 若 `state_manager` 可用，则将 `task.{task_id}.status` 置为 `failed` 并写入 `error` 字段；
- 该方法最终抛出 `Exception("数据合并执行失败: ...")`，上层调用可捕获并用于 CLI/API 的错误返回。

返回值：

- 成功：返回最终 `target_path` 字符串；
- 失败：抛出异常（不返回 `None`）。

### 5.3 CLI 入口 `main()`

命令行接口位于文件末尾：

```bash
python data_merger.py \
  --task_id <任务ID> \
  --input_paths <文件1,文件2,...> \
  --merge_mode merge|append \
  --target_path <输出文件路径> \
  [--deduplicate True|False] \
  [--dedup_field 字段名] \
  [--dedup_strategy keep_first|keep_last] \
  [--chunk_size 分片大小] \
  [--encoding 编码] \
  [--validate_only True|False]
```

CLI 行为：

1. 构造 `DataMerger` 并调用 `init_merger()`，失败则输出 `{"status": "failed", "error": "..."}` 并退出；
2. 将 `--input_paths` 按逗号拆分为列表；
3. 构造 `MergeTaskParams`：
  - `task_id` 使用 CLI 参数传入值；
  - `deduplicate` 为 `bool` 类型；
  - `chunk_size`、`encoding` 参数为空时使用默认值；
4. 若 `--validate_only=True`：
  - 调用 `validate_merge` 并输出包含 `validation` 的 JSON；
5. 否则执行 `merge_datasets` 并输出：
  - 成功：`{"status": "success", "task_id": ..., "output": "<target_path>"}`；
  - 失败：`{"status": "failed", "task_id": ..., "error": "..."}`。

---

## 六、关键内部实现要点

### 6.1 文件格式与字段抽取

- `_detect_format(file_path)`：根据扩展名判断格式：
  - `.csv` → `csv`；`.xlsx` → `xlsx`；`.xls` → `xls`；`.json` → `json`；`.jsonl` → `jsonl`；
- `_get_fields_simple(file_path, file_format)`：在无 `FieldExtractor` 时的兜底字段提取；
- `_get_base_info(input_paths)`：基于第一个文件确定 `base_fields` 和 `file_format`，并通过 `_count_file_rows` 估算总行数。

### 6.2 分片读取 `_read_file_chunks`

根据 `file_format` 采用不同策略：

- CSV：使用 `pandas.read_csv(..., chunksize=chunk_size)` 逐块读取；
- Excel：优先尝试 `pandas.read_excel(..., chunksize=chunk_size)`；如版本不支持，则一次读入再手工分片；
- JSONL：按行读取，逐条 `json.loads`，聚合到分片列表；
- JSON：一次读取 `json.load`，若为 list 则按切片分块，若为 dict 则当作单条记录处理。

### 6.3 输出写入 `_init_output_file` 与 `_append_to_file`

初始化与追加行为严格区分，并兼顾 JSON 和 JSONL 的行终止符规范：

- CSV/Excel：通过 `pandas.DataFrame` 写入；
- JSONL：每条记录序列化为单行 JSON，使用 `newline='\n'` 保持统一行结束符；
- JSON：读取现有数据（如存在），与新增数据合并后整体写回。

### 6.4 去重 `_deduplicate_data`

通过 `seen_values: set` 跨分片维护去重状态：

- 指定字段去重：`dedup_key = item.get(dedup_field)`；
- 全量去重：`dedup_key = tuple(sorted(item.items()))`；
- 若 `dedup_key` 已存在于 `seen_values`，则认为是重复：
  - 当前实现下，无论 `keep_first` 还是 `keep_last`，都会跳过当前记录（更接近 `keep_first` 语义）；
- 否则将 key 加入集合并保留记录。

### 6.5 末尾空行清理 `_clean_file_ending`

仅针对 JSONL：

- 将整个文件读入字符串，使用 `rstrip('\n') + '\n'` 保证文件末尾只有一个换行符，避免下游解析报“多余空行”式错误。

### 6.6 元数据与说明文件

- `_save_merge_meta(target_path, meta)`：在输出目录下生成 `merge_meta.json`；
- `_create_merge_info_file(output_dir, meta, params)`：生成包含任务基本信息、输入/输出统计、字段列表与去重配置的中文说明文件 `合并信息.txt`，方便人工审阅与归档。

---

## 七、全局实例与 API 封装

### 7.1 全局实例

文件末尾创建了一个模块级全局实例：

```python
data_merger = DataMerger()
data_merger.init_merger()
```

可在同一进程内重复复用（避免反复初始化配置与日志）。

### 7.2 高层 API：`merge_data(...)`

函数签名（位于文件末尾）：

```python
def merge_data(source_paths: List[str], mode: str = "merge",
          dedup_field: str = None, output_dir: str = None, **kwargs) -> str:
   ...
```

行为：

1. 自动选择输出目录：
  - 未指定 `output_dir` 时，默认 `"./processed"`；
  - 在其中创建以 `merge-<timestamp>` 命名的子目录；
2. 为输出文件生成易读文件名：
  - 从 `source_paths` 提取各源文件名（不含扩展名）；
  - 少量源文件：拼接前几个名字；
  - 多源文件：使用 `first_and_n_others` 模式；
3. 使用源文件的扩展名作为输出文件扩展名；
4. 自动构造 `MergeTaskParams`（含随机 `task_id`、`deduplicate` 与可选 `dedup_strategy/chunk_size/encoding` 等）；
5. 调用全局实例 `data_merger.merge_datasets(task_params)` 执行实际合并；
6. 返回合并后文件的绝对路径或相对路径字符串。

该 API 面向其他 Python 模块使用（如后续管线脚本），简化了 CLI 层的组装开销。

---

## 八、与其他模块的协作关系

### 8.1 典型调用链

1. 上游通过 `format_converter.py` 将各类原始数据统一转换为 CSV/JSONL 等标准格式；
2. 再通过 `field_extractor.py` 保留且对齐需要的字段，产出字段结构一致的多个数据集文件；
3. 最后调用 `data_merger.py` 的 CLI 或 `merge_data()`，实现数据集的纵向合并与去重；
4. 下游（如蒸馏生成模块）直接使用合并后的单一数据集文件。

### 8.2 可见性与约束

- `DataMerger` 不直接依赖任何上游模块，仅通过文件与字段结构间接协作；
- `FieldExtractor` 的使用是可选增强，缺失时仍能正常工作，但字段差异提示会更简化；
- 模块以文件为唯一输入输出单元，不持有任何跨进程状态，便于在 CLI、UI 和脚本中统一复用。

---

## 九、使用建议与注意事项

- 在使用 `append` 模式前，应先通过 `validate_merge` 或 `--validate_only` 确认目标文件字段与新文件完全一致；
- 对于特别大的 Excel 文件（> 数万行），建议先转换为 CSV 或 JSONL 再使用本模块合并；
- 当 `dedup_field` 为高基数字段（如 `id`）时，`seen_values` 集合可能较大，应合理控制合并规模或分批执行；
- 目前实现未针对 `keep_last` 做特殊优化，如对“保留最后一条”有严格需求，建议在下游进行二次处理或改造该方法。

本设计文档完全基于当前 `src/data_merger.py` 实现撰写。如未来修改代码（新增字段、调整 CLI 参数或更改去重策略），须同步更新本文件，以确保文档与实现保持一致。
