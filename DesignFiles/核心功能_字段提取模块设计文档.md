# 核心功能_字段提取模块 field_extractor.py 设计文档（v2.0，对齐现实现）

**对应源码**：`src/field_extractor.py`

**支持格式**：`csv` / `xlsx` / `xls` / `json` / `jsonl` / `md`

**版本**：0.2 beta
**作者**: leotcw&AI

**最后校对日期**：2025-11-14（基于当前仓库源码全文比对）

---

## 一、模块定位与职责边界

### 1.1 模块定位

`field_extractor.py` 负责两类能力：

- 对单个数据文件进行**字段识别**（字段名、类型、样本值、空值比例）；
- 按用户配置进行**字段提取 + 可选过滤**，输出新的数据文件。

调用方式：

- 作为库函数被其他模块或 UI 调用：
    - 全局实例 `field_extractor`；
    - 全局函数 `get_fields(source_path)` / `extract_fields(source_path, fields, ...)`；
- 作为独立命令行工具，通过 `python src/field_extractor.py --action ...` 调用。

### 1.2 职责范围（基于实际实现）

模块当前实际完成的职责：

- 支持多种输入格式：CSV、Excel（`xlsx`/`xls`）、JSON、JSONL、Markdown 表格（简单形式）；
- 通过采样推断字段类型（字符串 / 数值 / 日期 / 布尔值），统计空值比例，提供少量非空样本值；
- 根据用户指定字段列表和重命名规则，从大文件中分片提取字段；
- 按过滤条件（字符串匹配 / 长度比较 / 数值比较 / 空值判断 / 正则）过滤记录；
- 支持估算总行数并按分片顺序处理，防止一次性载入大文件；
- 支持通过 `StateManager` 记录简单的任务状态和进度（仅在 `state_manager` 可用时生效）。

当前没有实现、但旧文档曾描述的能力：

- 不会调用 `format_converter.convert_format` 做二次格式转换；
- 没有独立的“临时分片文件 + 合并文件”的机制，所有写出直接作用于目标文件；
- 没有自动“chunk_size 减半重试”的内存保护逻辑；
- 没有单独的“extract.log” 日志文件，日志行为依赖 `LogManager` 或标准输出。

文档中不再将上述未实现部分描述为“现有能力，只在扩展章节中作为潜在方向提及”。

---

## 二、模块依赖与整体结构

### 2.1 依赖模块

源码中实际使用的依赖：

- 标准库：`os` / `sys` / `json` / `csv` / `re` / `logging` / `argparse` / `time` / `uuid` / `datetime` / `pathlib.Path`；
- 第三方库：
    - `pandas`：CSV / Excel 的读取与写出；
    - `jsonlines`：JSONL 的读取与写出；
    - `chardet`：编码检测（可选，没有则默认 `utf-8`）。
- 项目内模块（通过相对导入或退化为 None）：
    - `ConfigManager`（`src/config_manager.py`）：用于读取 `field_extractor` 的默认配置；
    - `LogManager`（`src/log_manager.py`）：用于获取模块日志器；
    - `StateManager`（`src/state_manager.py`）：用于存储任务状态（断点续传 / 进度）；
    - `utils`：虽被导入 `validate_file` / `get_file_encoding` / `infer_data_type` / `handle_null` / `read_file_chunk` / `write_file_chunk`，但**当前版本没有实际使用这些工具函数**。

> 若导入失败，模块会打印警告并将 `ConfigManager/LogManager/StateManager` 设为 `None`，此时将使用简化的日志器且不进行状态持久化。

### 2.2 核心类与全局接口

核心类型与类：

- `FieldInfo(TypedDict)`：字段信息结构；
- `FilterCondition(TypedDict)`：过滤规则结构；
- `TaskParams(TypedDict)`：提取任务参数结构；
- `FieldExtractor`：主类，提供字段识别和字段提取功能；
- 命令行入口函数 `main()`；
- 全局单例 `field_extractor = FieldExtractor()` 与全局 API:
    - `get_fields(source_path: str) -> Optional[List[FieldInfo]]`；
    - `extract_fields(source_path: str, fields: List[str], output_dir: str = None, **kwargs) -> str`。

---

## 三、核心数据结构设计

### 3.1 字段信息结构 FieldInfo

与源码一致：

```python
class FieldInfo(TypedDict):
        name: str
        type: Literal["string", "number", "date", "boolean"]
        sample_values: List[Any]
        null_ratio: float
```

- `sample_values`：最多 3 条非空值；
- `null_ratio`：通过采样行中的空值比例估算。

### 3.2 过滤规则结构 FilterCondition

```python
class FilterCondition(TypedDict):
        field: str
        op: Literal[
                ">", "<", ">=", "<=", "==", "!=",
                "contains", "not_contains", "regex",
                "len_gt", "len_lt", "len_eq",
                "is_null", "not_null",
        ]
        value: Union[str, int, float, None]
```

注意：

- `is_null` / `not_null` 不强制要求 `value` 字段，源码中会直接忽略 value；
- 其他操作符要求存在 `value`，否则 `_validate_filter_condition` 返回 False。

### 3.3 任务参数结构 TaskParams

源码中定义：

```python
class TaskParams(TypedDict):
        # 基础标识
        task_id: str
        source_path: str

        # 字段配置
        selected_fields: List[str]
        field_rename: Dict[str, str]

        # 过滤配置
        filter_conditions: List[FilterCondition]
        filter_logic: Literal["and", "or"]

        # 输出配置
        output_dir: str
        target_format: str
        encoding: str

        # 控制参数
        chunk_size: int
        resume: bool
```

在实际调用中（命令行与全局 `extract_fields`），存在以下约定：

- `task_id`：
    - CLI 未显式指定时，自动生成：`extract-<timestamp>-<6位随机>`；
    - 全局函数中同样按该格式生成；
- `output_dir`：
    - 传入值与 `task_id` 组合为真正的输出目录：`os.path.join(output_dir, task_id)`；
- `target_format`：
    - 若为空字符串，则在 `_process_extraction` 中退回到源文件格式；
- `encoding`：
    - 目前仅用于 `meta.json` 的元数据记录，实际写出时 CSV/JSON/JSONL/Excel 固定使用 `utf-8`；
- `resume`：
    - 若为 True 且 `StateManager` 可用，则尝试从状态中恢复 `current_chunk`；否则从 0 开始。

---

## 四、初始化与环境准备

### 4.1 默认配置

在 `FieldExtractor.__init__` 中内置的默认配置：

```python
self.default_config = {
        'sample_rows': 100,
        'chunk_size': 1000,
        'supported_formats': ['csv', 'xlsx', 'xls', 'json', 'jsonl', 'md'],
        'default_encoding': 'utf-8',
        'output_dir': './data/processed',
}
```

### 4.2 `init_extractor()` 行为

```python
def init_extractor(self) -> bool:
        # 1. ConfigManager
        # 2. Logger (LogManager 或简易 logger)
        # 3. StateManager
        # 4. 三方依赖检查
```

具体逻辑：

1. 若 `ConfigManager` 可用：
     - `config = ConfigManager().get_config('field_extractor', self.default_config)`；
     - 设置 `self.sample_rows` / `self.chunk_size` / `self.output_dir`。
     否则使用默认配置。
2. 若 `LogManager` 可用：
     - `self.logger = LogManager().get_logger('field_extractor')`；
     - 记录 “初始化开始/成功/失败”。
     否则创建一个简单的 `logging.Logger('field_extractor')` 输出到 stderr。
3. 若 `StateManager` 可用：
     - `self.state_manager = StateManager()`；
     - `self.state_manager.init_state('field_extractor')`。
4. 检查第三方库：`pandas` / `jsonlines` / `openpyxl` 是否可用：
     - 任一缺失则记录 ERROR 并返回 False；
     - 全部存在则记录 INFO 并返回 True。

> 注意：当前实现中，即使 `init_extractor()` 返回 False，全局实例仍然存在但功能可能不完整。命令行入口 `main()` 遇到初始化失败会直接输出 JSON 错误并退出。

---

## 五、字段识别实现 (`get_fields`)

### 5.1 总体流程

`get_fields(source_path)` 的核心逻辑：

1. 记录开始日志；
2. 调用 `_validate_file(source_path)`：
     - 检查文件是否存在；
     - 扩展名在 `['.csv', '.xlsx', '.xls', '.json', '.jsonl', '.md']` 之内；
3. 调用 `_detect_format(source_path)`：返回 `csv/json/jsonl/xlsx/xls/md` 之一；
4. 按格式分派到：
     - `_parse_json_fields(..., format_type)`；
     - `_parse_csv_fields(...)`；
     - `_parse_excel_fields(...)`；
     - `_parse_markdown_fields(...)`；
5. 记录字段数量并返回 `List[FieldInfo]`；失败返回 `None`。

### 5.2 JSON/JSONL 字段解析

`_parse_json_fields(file_path, format_type)`：

- `jsonl`：
    - 使用 `jsonlines.open(file_path, 'r')`；
    - 逐行读取，最多处理 `self.sample_rows` 行；
    - 仅在行是 `dict` 时统计字段：
        - 初始化 `fields_data[key]` 的 `values/null_count/total_count`；
        - 空字符串或 `None` 计入 `null_count`；
        - 非空值最多保留 3 条在 `values` 中作为样本；
- `json`：
    - 用 `json.load(open(..., 'utf-8'))`；
    - 若顶层是 `list`：遍历前 `self.sample_rows` 个元素，认为每个元素是 `dict`；
    - 若顶层是 `dict`：视为一个样本；
- 失败时记录错误并返回 `[]`。

最终将 `fields_data` 转换为 `FieldInfo`：

- `type`：由 `_infer_field_type(values)` 推断；
- `null_ratio = null_count / max(total_count, 1)`；
- 样本值为 `values[:3]`。

### 5.3 CSV / Excel 字段解析

`_parse_csv_fields(file_path)`：

- 先调用 `_detect_encoding(file_path)` 获取编码：
    - 若安装 `chardet`：读取前 10KB 推断；否则默认为 `utf-8`；
- 使用 `pd.read_csv(..., nrows=self.sample_rows)` 读取采样数据；
- 对每列：
    - 非空样本 `df[column].dropna().tolist()[:3]`；
    - 空值比例 `df[column].isnull().sum() / len(df)`；
    - 类型 `_infer_field_type(sample_values)`。

`_parse_excel_fields(file_path)`：

- 使用 `pd.read_excel(file_path, nrows=self.sample_rows)`，其余逻辑与 CSV 相同。

### 5.4 Markdown 字段解析

`_parse_markdown_fields(file_path)`：

- 使用正则 `table_pattern = r'\|(.+)\|\s*\n\|[-\s|:]+\|\s*\n((?:\|.+\|\s*\n)*)'` 匹配第一个表格；
- 表头解析：`header_line.split('|')` 去空白；
- 数据行解析：对每行 `line.split('|')`，列数匹配表头才保留；
- 只采样前 `self.sample_rows` 行；
- 对每列：
    - 统计空值（空字符串或 `'null'/'none'/'na'`）；
    - 样本值取非空的前三个；
    - 类型 `_infer_field_type`；
    - `null_ratio = null_count / max(len(rows), 1)`。

若未匹配到表格或解析失败则返回空列表。

### 5.5 类型推断 `_infer_field_type`

行为完全按源码：

1. 无样本 → `"string"`；
2. 所有值都能转为 float → `"number"`；
3. 所有值都在布尔集合 `{true,false,1,0,yes,no,y,n}`（忽略大小写） → `"boolean"`；
4. 所有值匹配日期模式之一：
     - `YYYY-MM-DD`、`MM/DD/YYYY`、`YYYY/MM/DD` → `"date"`；
5. 其他情况 → `"string"`。

---

## 六、字段提取实现 (`extract_fields`)

### 6.1 总体流程

`extract_fields(task_params)` 的核心逻辑：

1. 记录开始日志；
2. 调用 `_validate_task_params(task_params)`：
     - 检查必填字段 `task_id/source_path/selected_fields`；
     - 检查源文件存在；
     - 检查过滤条件运算符有效，必要时存在 `value`；
3. 构建输出目录：`output_dir = os.path.join(task_params['output_dir'], task_id)`；
4. 调用 `get_fields(source_path)` 获取源字段信息，核对 `selected_fields` 全部存在；
5. 断点续传：
     - 若 `resume=True` 且 `self.state_manager` 存在：
         - 读取 `state = self.state_manager.get_state(f"task.{task_id}")`；
         - 若 `state['status'] == 'running'`，从 `state['current_chunk']` 继续；
6. 调用 `_process_extraction(task_params, output_dir, start_chunk)`：
     - 完成分片读取、字段提取、过滤和写出；
     - 返回最终输出文件路径；
7. 若成功：
     - 调用 `_generate_metadata(task_params, output_file, output_dir)` 写入 `meta.json` 等；
     - 若有 `StateManager`，设置任务状态为 `completed` 并写入 `output_file` 与 `end_time`；
8. 若失败：
     - 记录错误日志并返回 `None`。

### 6.2 分片处理 `_process_extraction`

函数签名：`_process_extraction(task_params, output_dir, start_chunk=0) -> Optional[str>`。

关键步骤：

1. 解析基础参数：
     - `source_format = _detect_format(source_path)`；
     - `target_format = task_params.get('target_format', source_format)`；
     - `output_file = os.path.join(output_dir, f"extracted.{target_format}")`；
     - `chunk_size = task_params.get('chunk_size', self.chunk_size)`；
2. 估算行数：
     - 调用 `_estimate_total_rows(source_path, source_format)`：
         - CSV：按行数减一（表头）；
         - JSONL：按行数计数；
         - Excel：目前固定返回 1000（占位估算）；
         - 其他：默认 1000。
     - 计算 `total_chunks = (total_rows + chunk_size - 1) // chunk_size`。
3. 初始化状态（若有 `StateManager`）：
     - 写入 `status='running', total_chunks, current_chunk=start_chunk, start_time`。
4. 循环处理每个分片：
     - 调用 `_read_chunk_data(...)` 读取当前分片数据；
     - 调用 `_extract_fields_from_chunk(chunk_data, task_params)` 仅保留所需字段并重命名；
     - 若配置了过滤条件：
         - 调用 `_apply_filters(extracted_data, task_params)` 得到 `filtered_data`；
     - 调用 `_write_chunk_data(filtered_data, output_file, target_format, is_first_chunk)` 将数据写入文件；
     - 累加 `processed_rows` 与 `filtered_rows` 计数；
     - 若有 `StateManager`，更新 `current_chunk` / `progress` / `processed_rows` / `filtered_rows`；
5. 记录完成日志并返回 `output_file`；异常时记录错误并返回 `None`。

### 6.3 分片读取 `_read_chunk_data`

根据 `file_format` 不同采用不同策略：

- CSV：
    - 使用 `skiprows = chunk_idx * chunk_size` + `nrows=chunk_size` 的 `pd.read_csv`；
    - 编码通过 `_detect_encoding(file_path)` 推断；
- JSONL：
    - 用 `jsonlines.open` 逐行读，跳过前 `skip_rows` 行；
    - 收集接下来的 `chunk_size` 条记录；
- Excel：
    - `pd.read_excel(file_path, skiprows=skip_rows, nrows=chunk_size)`；
- JSON：
    - 一次性 `json.load`；
    - 若为列表，则切片 `[start_idx:end_idx]`；
    - 若为 dict，则仅第一个分片返回 `[data]`，后续分片为空列表。

读取异常时记录错误并返回空列表。

### 6.4 字段提取 `_extract_fields_from_chunk`

行为：

- 遍历 `chunk_data` 中的每一行 `row`；
- 对于 `selected_fields` 中的每个字段：
    - 若存在于 `row`：根据 `field_rename` 决定输出字段名（默认为原名）；
- 若结果行非空，则加入输出列表。

### 6.5 过滤实现 `_apply_filters` / `_check_row_filters` / `_evaluate_condition`

过滤流程：

1. `_apply_filters(data, task_params)`：
     - 从 `task_params` 中取出 `filter_conditions` 和 `filter_logic`（默认 `'and'`）；
     - 若条件为空，直接返回原数据；
     - 否则逐行调用 `_check_row_filters(row, conditions, logic)`。

2. `_check_row_filters(row, conditions, logic)`：
     - 对每个条件：
         - 取出 `row[field]` 作为 `field_value`；
         - 调用 `_evaluate_condition(field_value, op, value)`；
     - `logic == 'and'`：要求所有条件为 True；
     - `logic == 'or'`：任一条件为 True 即通过。

3. `_evaluate_condition(field_value, op, compare_value)`：
     - `is_null`：`field_value is None or field_value == ''`；
     - `not_null`：与上相反；
     - 其他操作符在字段值为空时均返回 False；
     - 字符串类：
         - `contains` / `not_contains`：直接用 `in` / `not in`；
         - `regex`：`re.search(compare_value, str(field_value))`；
     - 长度类：
         - `len_gt/len_lt/len_eq`：对 `len(str(field_value))` 与 `int(compare_value)` 比较；
     - 数值类：
         - 先尝试将两边转为 `float` 再比较；
         - 若转换失败，退化为字符串比较，仅支持 `==`/`!=`；
     - 任意异常都记录 warning，返回 False。

### 6.6 输出写入 `_write_chunk_data`

根据 `target_format` 不同采用不同策略：

- CSV：
    - 使用 `pd.DataFrame(data).to_csv`；
    - `mode='w'`（首片）或 `'a'`（追加），`header` 仅首片写入；
    - 编码固定 `utf-8`；
- JSONL：
    - 使用 `jsonlines.open(output_file, mode='w'/'a')`；
    - 每条记录 `writer.write(item)`；
- JSON：
    - 首片：直接写入整个数组；
    - 后续片：先读回已有 JSON 数组，与当前数据 `existing + data` 合并再写回；
- Excel：
    - 首片：`to_excel(index=False)` 覆盖写入；
    - 后续片：读取现有 Excel 为 DataFrame，与新数据 concat 后再覆盖写回。

> 当前实现中没有生成 Markdown 输出；`target_format` 为 `md` 时不会走此函数（调用方也未组合出这种用法），文档中不宣称支持“提取到 Markdown 表格”。

### 6.7 元数据生成 `_generate_metadata`

输出信息包括：

- `task_id/module/action`：标识任务；
- `start_time/end_time/status`：时间与状态；
- `source_info`：
    - `file_path`：源文件路径；
    - `format`：通过 `_detect_format` 得到的格式字符串；
- `extraction_config`：
    - `selected_fields` / `field_rename` / `filter_conditions` / `filter_logic` / `chunk_size`；
- `output_info`：
    - `file_path` / `format` / `file_size` / `encoding`；
- `statistics`：
    - `processed_rows` / `filtered_rows`，但当前代码没有在任务运行时将实际计数写回 `task_params`，因此默认 0；

并额外输出：

- `field_info.json`：
    - 直接复用 `get_fields(source_path)` 的结果；
- `filter_rules.json`（仅当有过滤条件时生成）：
    - 存储最终使用的 `conditions` 与 `logic`。

> 当前实现中没有 `extract.log` 文件，日志由 `logger` 控制输出位置（可能是控制台或集中日志文件）。

---

## 七、命令行接口设计

### 7.1 参数定义

`main()` 使用 `argparse` 定义以下参数：

- `--action`（必选）：`get_fields` 或 `extract`；
- `--source_path`（必选）：源文件路径；
- `--task_id`：任务 ID；
- `--selected_fields`：逗号分隔字段名字符串；
- `--field_rename`：JSON 字符串，形如 `{"old": "new"}`；
- `--filter_conditions`：JSON 字符串，形如 `[{"field":"score","op":">=","value":0.9}]`；
- `--filter_logic`：`and` 或 `or`，默认 `and`；
- `--target_format`：输出格式（如 `csv/jsonl/json/xlsx`），为空则与源格式一致；
- `--output_dir`：输出根目录，默认 `./data/processed`；
- `--chunk_size`：分片大小，默认 1000；
- `--print_fields`：布尔值，是否在控制台打印字段信息（仅 `get_fields` 时有意义）。

### 7.2 行为约定

- 初始化：
    - 创建 `FieldExtractor` 实例并调用 `init_extractor()`；失败则输出 `{"status":"failed","error":"字段提取器初始化失败"}` 并退出。
- `get_fields` 模式：
    - 获取字段列表；
    - 若 `print_fields=True`，按人类可读格式打印；
    - 最后输出 JSON：`{"status":"success","data": fields}` 或 `{"status":"failed","error":"字段识别失败"}`。
- `extract` 模式：
    - 按命令行参数组装 `TaskParams`；
    - 调用 `extract_fields(task_params)`；
    - 成功则输出：`{"status":"success","task_id":...,"output": 输出路径}`；
    - 失败则输出：`{"status":"failed","task_id":...,"error":"字段提取失败"}`。

> 命令行输出都是 JSON 字符串，适合被上级调度器或前端脚本解析。

---

## 八、全局 API 使用方式

模块底部提供了可直接在 Python 代码中调用的两个函数：

```python
def get_fields(source_path: str) -> Optional[List[FieldInfo]]:
        return field_extractor.get_fields(source_path)

def extract_fields(source_path: str, fields: List[str], output_dir: str = None, **kwargs) -> str:
        task_params = {
                'task_id': f"extract-{int(time.time())}-{uuid.uuid4().hex[:6]}",
                'source_path': source_path,
                'selected_fields': fields,
                'field_rename': kwargs.get('field_rename', {}),
                'filter_conditions': kwargs.get('filter_conditions', []),
                'filter_logic': kwargs.get('filter_logic', 'and'),
                'output_dir': output_dir or './data/processed',
                'target_format': kwargs.get('target_format', ''),
                'encoding': kwargs.get('encoding', 'utf-8'),
                'chunk_size': kwargs.get('chunk_size', 1000),
                'resume': True,
        }
        return field_extractor.extract_fields(task_params)
```

推荐用法：

- 在其他模块中直接导入并调用，不必关心 TaskParams 内部细节：

```python
from src.field_extractor import get_fields, extract_fields

fields_info = get_fields('./data/raw/train.jsonl')
output_path = extract_fields(
        './data/raw/train.jsonl',
        fields=['instruction', 'output'],
        output_dir='./data/processed',
        target_format='jsonl',
        field_rename={'output': 'response'},
        filter_conditions=[
                {'field': 'score', 'op': '>=', 'value': 0.9},
        ],
)
```

---

## 九、异常处理与边界情况

结合源码，模块处理异常的方式归纳如下：

- 文件/格式相关：
    - 源文件不存在或扩展名不在支持列表 → `_validate_file` 返回 False，`get_fields`/`extract_fields` 返回 None 并记录错误；
    - 无法识别格式 → `get_fields` 返回 None；
- 解析相关：
    - JSON/JSONL/CSV/Excel/Markdown 解析出错时，记录具体错误并返回空字段列表或空数据分片，整体任务可能继续（按逻辑分支）；
- 参数相关：
    - 缺少必填参数或过滤操作符不合法时 → `_validate_task_params` 返回 False，任务立即失败；
    - 选中字段不在源字段列表中 → 记录错误并返回 None；
- 运行时异常：
    - `_process_extraction` / `_read_chunk_data` / `_write_chunk_data` / `_generate_metadata` 中出现异常时，记录错误日志，返回 None 或忽略该分片。

> 当前实现没有自动调整 `chunk_size` 或重试机制，遇到内存/IO 错误时只会记录日志并结束任务。

---

## 十、扩展与改进方向

基于当前实现，后续可以考虑的改进点：

1. **更精确的行数统计**：
     - 对 Excel/JSON 文件实现真实的总行数估算，而不是固定 1000；
2. **统计数据写回元数据**：
     - 在 `_process_extraction` 中将 `processed_rows` / `filtered_rows` 写入 `task_params`，供 `_generate_metadata` 使用；
3. **更丰富的格式支持**：
     - 在字段识别和提取中支持 Parquet/Arrow 等二进制格式；
4. **更灵活的过滤运算符**：
     - 增加 `in/not_in`、日期区间等运算符；
5. **并行处理**：
     - 对大型文件按分片并行提取和过滤，提升性能。

这些内容当前并未在代码中实现，本节仅作为设计方向参考。

---

> 本设计文档已完全依据 `src/field_extractor.py` 当前源码校对，删除了所有与实现不符的旧描述（例如与 `format_converter` 的紧耦合、临时分片文件合并等），上层模块在使用时应以本说明为准。
