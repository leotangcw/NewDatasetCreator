# 核心功能_格式转换模块 format_converter.py 设计文档（v2.0，对齐现实现）

**模块定位**：核心功能层 · 独立格式转换模块

**对应源码**：`src/format_converter.py`

**支持格式**：`csv` / `json` / `jsonl` / `excel(xlsx/xls)` / `markdown` / `arrow(HF Datasets)`

**版本**：0.2 beta
**作者**: leotcw&AI

**最后校对日期**：2025-11-14（基于当前仓库源码全文比对）

---

## 1. 模块定位与职责边界

### 1.1 模块定位

- 负责**多种文件/数据集格式之间的转换**，是数据获取、字段提取、数据合并等模块之间的“格式枢纽”。
- 面向两类调用方式：
  - 作为 Python 库被其他模块或上层应用调用（首选接口）；
  - 作为独立命令行工具使用（`python src/format_converter.py ...`）。
- 只处理“格式”和“轻量数据清洗”，**不承担业务字段含义、复杂清洗规则或合并逻辑**。

### 1.2 职责范围

当前实现中，模块的职责可以精确归纳为：

- 支持下列源/目标格式：
  - 文件格式：`csv` / `json` / `jsonl` / `excel` / `markdown`；
  - 数据集格式：`arrow`（Hugging Face `Dataset` / `DatasetDict`）。
- 自动识别源文件格式（扩展名 + Arrow 目录结构探测）。
- 自动检测文本编码（若安装 `chardet`），否则回退到 `utf-8`。
- 根据 `TaskParams`：
  - 读取不同格式的数据为列表字典 `List[Dict]`；
  - 执行基础数据预处理（空行过滤、不可见字符清理、基础类型适配）；
  - 写出到目标格式文件或 Arrow 数据集目录。
- 为调用方提供：
  - 单例转换器 `get_converter()`；
  - 简化 API `convert_format()`；
  - 任务管理与进度查询接口。

### 1.3 明确不做的事情

代码中没有实现、且**不应在文档中宣称存在**的能力：

- 不实现复杂“嵌套结构展开/收缩”策略；
- 不实现显式的“转换质量抽样对比/跳错行记录文件”等高级 QA 流程；
- 不使用 `state_manager` 做断点续传，运行期状态仅存在于内存；
- 不通过 `config_manager` / `log_manager` 等其他模块交互，模块内部自带轻量配置和日志器。

---

## 2. 依赖关系与内部结构

### 2.1 外部依赖

源码中的真实依赖如下：

- 标准库：`os` / `sys` / `json` / `csv` / `hashlib` / `argparse` / `logging` / `time` / `threading` / `shutil` / `random` / `string` / `datetime` / `pathlib.Path` / `concurrent.futures.ThreadPoolExecutor`（当前版本未使用并发）等。
- 必须三方库：
  - `pandas`：所有表格类读写的核心；
  - `jsonlines`：只用于 JSONL 写入（读取 JSONL 实际使用 `json.loads` 行解析）。
- 可选三方库：
  - `openpyxl`：Excel 写入引擎；
  - `chardet`：文件编码检测；
  - `datasets` + `pyarrow`：Arrow / Hugging Face Datasets 读写。

> 若缺少必须依赖，模块直接 `print` 错误并 `exit(1)`（当前实现行为），调用方需提前安装依赖。

### 2.2 内部核心类与单例

源码中的核心结构：

- `ConvertError(Exception)`：统一的转换错误异常类型。
- `TaskParams(TypedDict, total=False)`：任务参数定义（详见第 3 章）。
- `ConvertMeta(TypedDict)`：元数据结构（源码中 `_generate_metadata` 构造的字段集合）。
- `ConfigManager`：模块内部的轻量配置类，仅在本模块内使用，不依赖全局配置文件：
  - `config['base']`：`root_dir` / `chunk_size` / `encoding`；
  - `config['process']`：Excel 行列上限、CSV 分隔符等；
  - `config['convert']`：默认输出目录、临时目录、是否显示进度条等。
- `Logger`：基于 `logging` 的简单封装，输出到控制台 + `./data/logs/format_converter_YYYYMMDD.log`。
- `ProgressTracker`：保存任务进度与状态（仅在内存中使用，无持久化）。
- `FormatConverter`：模块的核心类，用于管理任务与执行转换。
- 顶层单例与 API：
  - `_converter_instance: Optional[FormatConverter]`；
  - `get_converter()`：懒加载单例；
  - `convert_format(...)` / `start_convert(...)` / `get_convert_progress(...)` / `list_converts()`。

---

## 3. 核心数据结构设计

### 3.1 任务参数 TaskParams

源码中定义如下（简化为文档形式）：

```python
class TaskParams(TypedDict, total=False):
    # 任务与输入
    task_id: str                 # 任务ID，由 FormatConverter.generate_task_id() 生成
    source_path: str             # 源文件/目录路径（必填）
    target_format: str           # 目标格式：csv/json/jsonl/excel/markdown/arrow（必填）
    output_dir: Optional[str]    # 输出目录，默认 ConfigManager.convert.default_output_dir

    # 处理控制
    chunk_size: Optional[int]    # 暂未在核心执行路径中使用，保留字段
    encoding: Optional[str]      # 源文件编码，None 时自动检测
    output_encoding: Optional[str]   # 输出编码，默认 'utf-8'

    # 格式专用参数
    excel_sheet: Optional[str]       # Excel 写入工作表名，默认 'Sheet1'
    csv_delimiter: Optional[str]     # CSV 分隔符，默认 ConfigManager.process.csv_delimiter（逗号）
    markdown_table_header: Optional[bool]  # Markdown 是否包含表头，默认 True
    json_ensure_ascii: Optional[bool]      # JSON 是否以 ASCII 形式输出，默认 False

    # 数据处理参数
    date_fields: Optional[List[str]]      # 强制按日期处理的字段名列表
    text_fields: Optional[List[str]]      # 强制按文本处理的字段名列表
    nest_depth: Optional[int]            # 当前实现未使用
    keep_nest: Optional[bool]            # 当前实现未使用
    skip_error_rows: Optional[bool]      # 当前实现未使用
    skip_empty_rows: Optional[bool]      # 若为 True，预处理阶段跳过空行
    clean_invisible: Optional[bool]      # 若为 True，清理字符串中的不可见字符
    split_file: Optional[bool]           # Excel 行数超限时拆分，具体逻辑见 `_write_excel`
    preview_rows: Optional[int]          # 当前实现未使用

    # Arrow 专用参数
    arrow_split: Optional[str]           # 从 Arrow 数据集中读取的 split 名称，默认 'train'
    max_rows: Optional[int]              # 读取 Arrow 数据集时的最大行数限制
```

> 注意：`nest_depth` / `keep_nest` / `skip_error_rows` / `preview_rows` 在当前实现中没有实际逻辑，属于“预留字段”，文档不应描述它们有具体行为。

### 3.2 任务进度 ProgressTracker

源码中 `ProgressTracker` 的关键字段：

- `task_id: str`：任务标识。
- `status: str`：`pending` / `running` / `completed` / `failed`。
- `progress: int`：0–100，当前实现中只在 `_execute_convert` 里设置为 `int(len(data)/total_rows*100)` 一次。
- `processed_rows` / `total_rows`：简单行数统计（当前版本中 `total_rows` 默认 0，主要作为占位）。
- `speed` / `eta`：基于 `processed_rows` 与时间差计算，但实现有逻辑瑕疵（`processed_rows` 赋值时机导致速度始终接近 0）。
- `error_msg: str`：失败原因简要信息。
- `output_file: str`：成功时记录输出文件路径。

### 3.3 转换元数据 ConvertMeta

`_generate_metadata()` 返回的结构为：

```python
class ConvertMeta(TypedDict):
    task_id: str
    source_path: str
    source_format: str        # `_detect_source_format` 的返回值
    target_path: str          # 最终输出路径（arrow 时是目录）
    target_format: str
    start_time: str           # 当前实现中 start/end 时间均为生成元数据时刻
    end_time: str
    status: str               # 固定 'success'
    params: TaskParams        # 任务参数完整快照
    data_quality: Dict[str, Any]  # 行数/字段映射/类型信息，占位为基础统计
    source_hash: str          # `md5:<hex>`，失败时为 `md5:unknown`
    error_log_path: Optional[str]  # 当前实现恒为 None
```

其中 `data_quality` 的内容为简单自描述：

- 行数统计：`source/target/skipped/skipped_reason`。
- 字段映射：`original/target/duplicates`（当前直接用 `data[0].keys()` 的列表填充）。
- 数据类型描述：直接来自 `TaskParams.date_fields/text_fields`，`nested_fields` 恒为空列表。

元数据最终写入 `<output_dir>/meta.json`。

---

## 4. 核心类 FormatConverter 设计

### 4.1 初始化与配置

构造函数：

```python
def __init__(self):
    self.config = ConfigManager()
    self.logger = Logger("format_converter")
    self.config.ensure_directories()  # 创建 processed / temp 目录
    self.tasks: Dict[str, Dict[str, Any]] = {}
    self.supported_formats = ['csv', 'json', 'jsonl', 'excel', 'markdown', 'arrow']
    self.excel_limits = {
        'max_rows': self.config.get_config('process.excel_max_rows'),
        'max_cols': self.config.get_config('process.excel_max_cols'),
    }
```

模块级别还创建了一个全局实例 `format_converter = FormatConverter()`，命令行入口和顶层 API 默认使用该实例。

### 4.2 任务生命周期

整体流程：

1. **创建任务**：调用 `add_convert_task(**kwargs)`：
   - 校验 `source_path` 存在、`target_format` 在 `supported_formats` 内；
   - 通过 `generate_task_id()` 生成 ID，如 `conv-20241114-abc123`；
   - 组装 `TaskParams`（应用 `ConfigManager` 默认值）；
   - 构造 `ProgressTracker(task_id)`，保存到 `self.tasks[task_id]`。

2. **启动任务**：调用 `start_task(task_id)`：
   - 若任务存在，创建后台线程执行 `_execute_convert(task_id, params, tracker)`；
   - 线程为 `daemon=True`，不会阻塞进程退出（命令行模式额外轮询等待）。

3. **查询任务**：
   - `get_task_progress(task_id)`：调用对应 `ProgressTracker.get_info()`；
   - `list_tasks()`：返回所有任务的进度与基本信息。

### 4.3 源格式检测 `_detect_source_format`

两个层面：

1. **Arrow 目录检测**：如果 `Path(file_path).is_dir()`，递归检查是否含有：
   - `dataset_info.json` / `state.json` / `dataset_dict.json` / `*.arrow` 任一存在 → 判定为 `arrow`。

2. **扩展名检测**：
   - `.csv` → `csv`
   - `.json` → `json`
   - `.jsonl` → `jsonl`
   - `.xlsx` / `.xls` → `excel`
   - `.md` / `.markdown` → `markdown`
   - `.arrow` → `arrow`

返回上述字符串之一或 `None`。

### 4.4 文件编码检测 `_detect_encoding`

逻辑：

- 若未安装 `chardet`：直接返回 `'utf-8'`；
- 若已安装：读取前 10KB (`10240` bytes) 做探测，`result.get('encoding', 'utf-8')` 作为编码；
- 若发生异常：回退 `'utf-8'`。

当前模块对所有文本读取都遵循：

- 若 `TaskParams.encoding` 明确指定，则优先使用；
- 否则调用 `_detect_encoding()` 自动推断。

### 4.5 源数据读取

所有读取函数最终返回 **列表字典 `List[Dict]`** 形式的行数据。

#### 4.5.1 `_read_csv`

- 使用 `pandas.read_csv(file_path, encoding=encoding, delimiter=csv_delimiter)`；
- 转成 `df.to_dict('records')`；
- 异常统一封装为 `ConvertError("读取CSV文件失败: ...")`。

#### 4.5.2 `_read_json`

- 使用 `json.load(open(file_path, encoding=encoding))`；
- 若顶层是 `list`：直接返回；
- 若顶层是 `dict`：包一层列表返回；
- 其他结构抛 `ConvertError("JSON数据格式不支持")`。

#### 4.5.3 `_read_jsonl`

- 逐行读取文本，跳过：
  - 空行；
  - 以 `#` 或 `//` 开头的注释行；
- 用 `json.loads(line)` 解析，每行失败会抛出含行号的 `ConvertError`：
  - `line contains invalid json: ... (line N)`；
  - 或 `error parsing line N: ...`。
- 若最终 `data` 为空，抛 `ConvertError("文件为空或未包含有效的JSON行")`。

> 注意：当前实现**没有** `skip_error_rows` 行级容错选项，一旦遇到解析错误即终止。

#### 4.5.4 `_read_excel`

- `pandas.read_excel(file_path, sheet_name=params.get('excel_sheet', 'Sheet1'))`；
- 转 `to_dict('records')`；
- 异常封装为 `ConvertError("读取Excel文件失败: ...")`。

#### 4.5.5 `_read_markdown`

- 简易 Markdown 表格解析：
  - 按行查找以 `|...|` 开头和结尾的行作为表格；
  - 第一行视为表头，第二行视为分隔线，后续行按 `|` 切列；
  - 若未找到任何表格行，抛 `ConvertError("未找到Markdown表格")`。

> 仅适用于典型 GitHub 样式简单表格，不支持复杂多表或嵌套情况。

#### 4.5.6 `_read_arrow`

- 要求 `datasets` 与 `pyarrow` 均可用，否则抛 `ConvertError("Arrow格式支持不可用..."`。
- 若 `file_path` 是目录：
  - `datasets.load_from_disk(str(file_path))`；
  - 若返回 `DatasetDict`：优先使用 `params.arrow_split`，找不到则使用第一个 split，并给出 `warning`；
  - 应用 `max_rows` 限制（使用 `select(range(max_rows))`）。
- 若 `file_path` 是 `.arrow` 文件：
  - 使用 `pa.ipc.open_file(...).read_all().to_pandas()` 再转 `to_dict('records')`。

---

## 5. 数据预处理与写出流程

### 5.1 数据预处理 `_preprocess_data`

处理策略：

- 输入：上一步读取得到的 `List[Dict]`；
- 核心逻辑：
  - 若 `skip_empty_rows=True`：对每行检查 `any(str(v).strip() for v in row.values())`，全部为空则跳过；
  - 若 `clean_invisible=True`：调用 `_clean_invisible_chars` 移除换行符之外的不可见字符；
  - 无论是否设置，上游不会自动“跳过错误行”，异常都在读取阶段抛出。

`_clean_invisible_chars`：

- 逐字段处理字符串值：
  - 去掉 `\r` 和 `\ufeff`；
  - 仅保留 ASCII 码 ≥ 32 的字符，外加制表符/换行符；
  - 其他非打印字符过滤掉。

`_adapt_data_types`：

- 针对 `TaskParams.text_fields`：强制转换为字符串；
- 针对 `TaskParams.date_fields`：调用 `_parse_date` 尝试解析为 `YYYY-MM-DD` 字符串；
  - 若值为数值类型：按 Excel 序列号处理：`1900-01-01 + (value - 2) 天`；
  - 其他情况使用 `pandas.to_datetime` 做推断；失败则返回原字符串。

### 5.2 目标写出

所有写出函数以预处理后的 `List[Dict]` 为输入：

#### 5.2.1 `_write_csv`

- 使用 `pandas.DataFrame(data).to_csv(target_path, index=False, sep=delimiter, encoding=encoding)`；
- `delimiter` 来自 `TaskParams.csv_delimiter`，默认逗号；
- 若 `data` 为空，抛 `ConvertError("没有数据可写入")`。

#### 5.2.2 `_write_json`

- 标准 JSON 数组输出：
  - `open(..., newline="\n")` + `json.dump(data, indent=2, ensure_ascii=json_ensure_ascii)`；
  - 额外写入一个结尾换行符，保持文件以 `\n` 终结。

#### 5.2.3 `_write_jsonl`

- 行式输出：
  - `open(..., newline="\n")`；
  - 每个元素 `json.dumps(item, ensure_ascii=False, separators=(',', ':'))` 后追加 `\n`；
  - 不会出现“最后一行没有换行”问题。

#### 5.2.4 `_write_excel`

- 依赖 `openpyxl`：若不可用则抛 `ConvertError` 提示安装；
- 若数据总行数超过 `excel_limits['max_rows']`：
  - 若 `split_file=True`：调用 `_write_excel_split` 拆成多个 `_part{n}.xlsx` 文件；
  - 否则抛 `ConvertError` 提示用户开启拆分或改为 CSV；
- 正常写出使用：`pandas.DataFrame(data).to_excel(target_path, index=False, sheet_name=excel_sheet)`。

#### 5.2.5 `_write_markdown`

- 简单 Markdown 表格：
  - 取首行字段名列表 `headers`；
  - 若 `markdown_table_header=True`：写两行表头 + 分隔线；
  - 再逐行输出 `| value1 | value2 | ... |`；
- 限于单表、平铺输出，不处理嵌套结构。

#### 5.2.6 `_write_arrow`

- 要求 `datasets` 与 `pyarrow` 可用；
- 行为：
  - 在 `target_path.parent / target_path.stem` 下创建目录；
  - 用 `pandas.DataFrame(data)` → `datasets.Dataset.from_pandas(df)`；
  - 调用 `dataset.save_to_disk(str(dataset_dir))`；
  - 日志记录保存路径，并返回目录路径字符串。

> 该行为意味着传入的 `target_path` 主要用于判断 dataset 目录名，而不是具体文件路径。

### 5.3 元数据写出

在 `_execute_convert` 完成写出后：

1. 调用 `_generate_metadata(...)` 获取 `ConvertMeta`；
2. `meta_path = output_dir / 'meta.json'`；
3. `json.dump(meta, indent=2, ensure_ascii=False)` 写出。

元数据与最终输出文件同目录，分布为：

- 若 `use_subdirectory=True`（默认）：`<output_dir>/<task_id>/<data_file>` + `<output_dir>/<task_id>/meta.json`；
- 若调用方显式传入 `use_subdirectory=False`（任务参数中）：输出直接放在指定目录，该参数目前仅由内部约定使用，命令行未暴露。

---

## 6. 命令行接口设计

### 6.1 参数解析

在 `main()` 中定义的 CLI 接口：

- 基本参数：
  - `--source`：源文件/目录路径；
  - `--target`：目标格式，限定选项 `['csv', 'json', 'jsonl', 'excel', 'markdown', 'arrow']`。
- 转换选项：
  - `--output-dir` / `--encoding` / `--output-encoding`；
  - `--excel-sheet` / `--csv-delimiter`；
  - `--date-fields` / `--text-fields`（逗号分隔字符串）；
  - `--skip-empty-rows` / `--clean-invisible` / `--split-file`（布尔 flag）。
- Arrow 特有：
  - `--arrow-split`：默认 `'train'`；
  - `--max-rows`：整数。
- 任务管理：
  - `--list-tasks`：打印当前进程内所有任务的状态；
  - `--progress TASK_ID`：查看指定任务进度。

### 6.2 典型调用流程

1. 命令行解析参数，实例化单例 `converter = get_converter()`；
2. 若 `--list-tasks`：简单打印 `list_tasks()` 结果；
3. 若 `--progress`：打印对应任务进度；
4. 若同时提供 `--source` 与 `--target`：
   - 组装 `kwargs` 并过滤 `None` 值；
   - 调用 `converter.add_convert_task(source_path=args.source, target_format=args.target, **kwargs)`；
   - 打印任务 ID，并立即调用 `converter.start_task(task_id)`；
   - CLI 模式下，使用轮询方式每秒拉取进度，直到状态为 `completed/failed`。

> 命令行工具内部与库接口使用同一个 `FormatConverter` 实例，因此通过 CLI 创建的任务可通过 `--progress` 查询，但只在当前 Python 进程生命周期内有效。

---

## 7. 与其他模块的协同关系

当前实现中，`format_converter.py` 没有直接导入或调用项目中其他模块（`config_manager` / `state_manager` 等），是**逻辑和依赖相对独立的模块**。与其他核心模块的关系更多体现在**文件级协作**：

- 与 `dataset_downloader.py`：
  - 下游：对下载到 `raw/` 目录的 JSON/JSONL/CSV 进行格式转换；
  - 上游：不依赖下载器，可以直接处理用户提供的任何文件路径。
- 与 `field_extractor.py` / `data_merger.py`：
  - 可作为前置/后置步骤，将提取/合并后的结果转为用户需要的格式（如 Excel 或 Arrow）。

因此，对于上层“流程型”模块，推荐的协作方式是：

1. 通过文件路径和格式约定将数据交给 `format_converter`；
2. 通过 `TaskParams` 控制编码和简单预处理行为；
3. 更复杂的业务逻辑（如字段筛选、去重）留在专门模块中实现。

---

## 8. 异常处理与边界行为

### 8.1 统一异常类型

- 模块内部所有可归类为“转换相关错误”的情况都应抛出 `ConvertError`：
  - 不支持的源/目标格式；
  - 文件读取失败；
  - JSON/JSONL 解析错误；
  - Excel 写入/读取异常；
  - Arrow 依赖缺失或读写失败等。
- 上层调用方可以通过 `except ConvertError as e:` 捕获并根据错误信息决定下一步行为。

### 8.2 关键错误场景

已在实现中的主要错误分支：

- **源文件不存在**：
  - `add_convert_task` 中直接 `ValueError("源文件不存在: ...")`。
- **目标格式不受支持**：
  - `ValueError("不支持的目标格式: ...")`。
- **无法识别源格式**：
  - `_execute_convert` 中抛 `ConvertError("无法识别源文件格式")`。
- **JSONL 行解析错误**：
  - 带行号的 `ConvertError("line contains invalid json: ... (line N)")`。
- **Arrow 依赖缺失**：
  - `ConvertError("Arrow格式支持不可用，请安装: pip install datasets pyarrow")`。
- **Excel 行数超限且未允许拆分**：
  - `ConvertError("数据行数(...)超过Excel限制(...)，建议启用split_file或转换为CSV格式")`。

### 8.3 线程中的错误处理

在 `_execute_convert` 中，如果发生任何异常：

- 记录错误日志 `self.logger.error("转换失败: ...", task_id)`；
- 调用 `tracker.fail(str(e))` 将任务标记为 `failed`；
- 不会抛出到主线程，调用方需要通过进度查询获知失败状态。

---

## 9. 扩展性与未来改进方向

结合当前实现，未来可能的扩展点包括：

1. **完善预留参数的实际逻辑**：如 `nest_depth` / `keep_nest` / `skip_error_rows` / `preview_rows`。
2. **更精细的进度统计**：
   - 在读取阶段统计总行数，并在写出过程中分阶段更新 `ProgressTracker`；
   - 修复当前 `speed/eta` 计算中使用 `processed_rows` 的逻辑错误。
3. **更健壮的 Markdown/JSON 结构支持**：
   - 支持多表 Markdown；
   - 支持更复杂的 JSON 嵌套扁平化策略。
4. **并行处理**：
   - 实际利用当前导入的 `ThreadPoolExecutor`，实现按文件或按分片并行转换。

这些扩展在代码层面尚未实现，文档仅在本节作为“潜在改进方向”列出，不应对外宣传为现有能力。

---

> 本设计文档已完全依据 `src/format_converter.py` 当前源码校对，删除了所有与实现不符的旧描述（如复杂质量校验、全局 config/state 依赖等），上层模块在使用时应以本说明为准。
